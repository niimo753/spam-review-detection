{"metadata":{"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":7943411,"datasetId":4579460,"databundleVersionId":8052353}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"* [Libraries](#s1)\n* [Basic Function](#s2)\n* [Initialize](#s3)\n* [CountVectorizer](#s4)\n* [Results](#sn)","metadata":{}},{"cell_type":"markdown","source":"# Libraries <a class=\"anchor\"  id=\"s1\"></a>","metadata":{}},{"cell_type":"code","source":"import shutil\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport os\n\n# text NLP\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords, words\nfrom nltk.stem import PorterStemmer\nimport string\nfrom wordcloud import WordCloud\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom unidecode import unidecode\n\n\n# Preprocessing\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\n\n# model\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Score\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve","metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:52:41.869486Z","iopub.execute_input":"2024-03-26T04:52:41.870350Z","iopub.status.idle":"2024-03-26T04:52:44.578235Z","shell.execute_reply.started":"2024-03-26T04:52:41.870315Z","shell.execute_reply":"2024-03-26T04:52:44.577000Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"nltk.download('all')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:52:37.449822Z","iopub.execute_input":"2024-03-26T04:52:37.450604Z","iopub.status.idle":"2024-03-26T04:52:37.476155Z","shell.execute_reply.started":"2024-03-26T04:52:37.450564Z","shell.execute_reply":"2024-03-26T04:52:37.474987Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Basic Function <a class=\"anchor\"  id=\"s2\"></a>","metadata":{}},{"cell_type":"code","source":"class BasicTextCleaning:\n    def __init__(self):\n        # define some necessary elements\n        self.stopwords = set(stopwords.words('english'))\n        self.words_corpus = set(words.words())\n        self.stemmer = PorterStemmer()\n        self.lemmatizer = WordNetLemmatizer()\n\n        # dictionary of methods can be used\n        self.methods = {'lowercase': str.lower,\n                        'accent_removal': self.accent_removal,\n                        'strip': str.strip,\n                        'nice_display': self.nice_display,\n                        'tokenization': nltk.word_tokenize,\n                        'stemming': self.stemming,\n                        'lemmatization': self.lemmatization,\n                        'punctuation_removal': self.punctuation_removal,\n                        'stopwords_removal': self.stopwords_removal,\n                        'contractions_expand': self.contractions_expand,\n                        'nonsense_removal': self.nonsense_removal,\n                        'number_removal': self.number_removal}\n\n        self.punctuations = '[%s]' % re.escape(string.punctuation)\n\n    def text_cleaning(self, texts, methods=None):\n        if not methods:\n            methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n                       'stopwords_removal', 'lemmatization', 'stemming']\n        if isinstance(texts, str):\n            texts = [texts]\n        cleaned_texts = []\n        for text in texts:\n            for method in methods:\n                if method not in self.methods.keys():\n                    raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n                text = self.methods[method](text)\n            cleaned_texts.append(text)\n        return cleaned_texts\n\n    def strip_text(self, text):\n        return text.strip()\n\n    def lowercase(self, text):\n        return text.lower()\n\n    def contractions_expand(self, text):\n        return contractions.fix(text)\n\n    def number_removal(self, text):\n        text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text\n\n    def nice_display(self, text):\n        text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n\n    def accent_removal(self, text):\n        text = unidecode(text)\n        return text\n\n    def punctuation_removal(self, text):\n        text = re.sub(self.punctuations, ' ', text)\n        text = re.sub(r'\\s+', ' ', text)\n        return text.strip()\n\n    def stopwords_removal(self, text):\n        return \" \".join([word for word in text.split() if word not in self.stopwords])\n\n    def stemming(self, text):\n        return \" \".join([self.stemmer.stem(word) for word in text.split()])\n\n    def lemmatization(self, text):\n        return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n\n    def tokenization(self, text):\n        return nltk.word_tokenize(text)\n\n    def nonsense_removal(self, text):\n        return \" \".join([word for word in text.split() if wordnet.synsets(word)])","metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:51:58.233631Z","iopub.status.idle":"2024-03-26T04:51:58.234006Z","shell.execute_reply.started":"2024-03-26T04:51:58.233824Z","shell.execute_reply":"2024-03-26T04:51:58.233840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n# data_save.to_csv('results.csv', index=False)\ndef save_and_print(data, length_used, feature_extraction, feature_selection, model, accuracy, f1, recall, precision, roc_auc, notes=None):\n    print('Accuracy:', accuracy)\n    print('F1:', f1)\n    print('Recall:', recall)\n    print('Precision:', precision)\n    print('ROC AUC:', roc_auc)\n    data_save = pd.read_csv('/kaggle/working/results.csv')\n    new_row = {'data': data, 'length_used': length_used, \n               'feature_extraction': feature_extraction, \n               'feature_selection': feature_selection, \n               'model': model, 'accuracy': accuracy, \n               'f1': f1, 'recall': recall, \n               'precision': precision, 'roc_auc': roc_auc, \n               'notes': notes}\n    data_save.loc[len(data_save)] = new_row\n    data_save.to_csv('/kaggle/working/results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:52:57.150389Z","iopub.execute_input":"2024-03-26T04:52:57.150939Z","iopub.status.idle":"2024-03-26T04:52:57.159791Z","shell.execute_reply.started":"2024-03-26T04:52:57.150905Z","shell.execute_reply":"2024-03-26T04:52:57.158853Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Initialize <a class=\"anchor\"  id=\"s3\"></a>","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fake-review-dataset/data_input.csv')\ndata['text'] = data['text'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:53:03.551999Z","iopub.execute_input":"2024-03-26T04:53:03.552439Z","iopub.status.idle":"2024-03-26T04:53:03.821903Z","shell.execute_reply.started":"2024-03-26T04:53:03.552405Z","shell.execute_reply":"2024-03-26T04:53:03.820712Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# delete output data in kaggle\nfile_path = \"/kaggle/working/results.csv\"\nif os.path.exists(file_path):\n    os.remove(\"/kaggle/working/results.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-14T14:31:19.205966Z","iopub.execute_input":"2024-03-14T14:31:19.206730Z","iopub.status.idle":"2024-03-14T14:31:19.212867Z","shell.execute_reply.started":"2024-03-14T14:31:19.206688Z","shell.execute_reply":"2024-03-14T14:31:19.212011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# copy results file from input to output in kaggle for updating\nsrc_path = r\"/kaggle/input/fake-review-dataset/results.csv\"\ndst_path = r\"/kaggle/working/\"\nshutil.copy(src_path, dst_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-26T05:05:16.962505Z","iopub.execute_input":"2024-03-26T05:05:16.962910Z","iopub.status.idle":"2024-03-26T05:05:16.978983Z","shell.execute_reply.started":"2024-03-26T05:05:16.962879Z","shell.execute_reply":"2024-03-26T05:05:16.977425Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/results.csv'"},"metadata":{}}]},{"cell_type":"markdown","source":"# CountVectorizer <a class=\"anchor\"  id=\"s4\"></a>","metadata":{}},{"cell_type":"markdown","source":"[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): Convert a collection of text documents to a matrix of token counts.\n- binary: 1 vs 0\n- ngram_range: (1,1), (1,2),(1,3),(2,2),(2,3),(3,3)\n- min_df: 0 or 0.001","metadata":{}},{"cell_type":"code","source":"length_used = None #modify\nif length_used == None:\n    X = data[['text']]\n    y = data[['label']]\nif length_used == 'MinMaxScaler':\n    X = data[['text', 'length_minmax']]\n    y = data['label']\nif length_used == 'StandardScaler':\n    X = data[['text', 'length_std']]\n    y = data['label']","metadata":{"execution":{"iopub.status.busy":"2024-03-14T15:57:24.351338Z","iopub.execute_input":"2024-03-14T15:57:24.351739Z","iopub.status.idle":"2024-03-14T15:57:24.361242Z","shell.execute_reply.started":"2024-03-14T15:57:24.351708Z","shell.execute_reply":"2024-03-14T15:57:24.359867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = CountVectorizer(binary=False, min_df=0, ngram_range=(1,1)) #modify\nf_selection = PCA(n_components=500, random_state=42) #modify","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Logistic","metadata":{}},{"cell_type":"code","source":"# Logistic Regression\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n    \n    # length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    \n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy.append(accuracy_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n    \nsave_and_print(data='fake_reviews_dataset',\n               length_used=None,\n               feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n               feature_selection='PCA(n_components=500, random_state=42)',\n               model=\"LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\",\n               accuracy=np.mean(accuracy).round(5),\n               f1=np.mean(f1).round(5),\n               recall=np.mean(recall).round(5),\n               precision=np.mean(precision).round(5),\n               roc_auc=np.mean(roc_auc).round(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## KNN","metadata":{}},{"cell_type":"code","source":"# KNeighborsClassifier(n_neighbors=1/3/5,metric='euclidean'/ 'manhattan'/ 'minkowski'/'cosine')\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = {}, {}, {}, {}, {}\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n    \n    # length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    for n in [1,3,5]:\n        for metric in ['euclidean', 'manhattan', 'cosine']:\n            model = KNeighborsClassifier(n_neighbors=n, metric=metric)\n            model.fit(X_train, y_train)\n            y_pred = model.predict(X_test)\n\n            if n not in accuracy.keys():\n                accuracy[n] = {}\n                f1[n] = {}\n                recall[n] = {}\n                precision[n] = {}\n                roc_auc[n] = {}\n            if metric not in accuracy[n].keys():\n                accuracy[n][metric] = []\n                f1[n][metric] = []\n                recall[n][metric] = []\n                precision[n][metric] = []\n                roc_auc[n][metric] = []\n\n            accuracy[n][metric].append(accuracy_score(y_test, y_pred))\n            f1[n][metric].append(f1_score(y_test, y_pred))\n            recall[n][metric].append(recall_score(y_test, y_pred))\n            precision[n][metric].append(precision_score(y_test, y_pred))\n            roc_auc[n][metric].append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n            \nfor n in [1,3,5]:\n    for metric in ['euclidean', 'manhattan', 'cosine']:\n        save_and_print(data='fake_reviews_dataset',\n                       length_used=None,\n                       feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n                       feature_selection='PCA(n_components=500, random_state=42)',\n                       model=f\"KNeighborsClassifier(n_neighbors={n}, metric='{metric}')\",\n                       accuracy=np.mean(accuracy[n][metric]).round(5),\n                       f1=np.mean(f1[n][metric]).round(5),\n                       recall=np.mean(recall[n][metric]).round(5),\n                       precision=np.mean(precision[n][metric]).round(5),\n                       roc_auc=np.mean(roc_auc[n][metric]).round(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## SVC","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n    \n    # length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    model = SVC(probability=True, class_weight='balanced', random_state=42)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy.append(accuracy_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n\nsave_and_print(data='fake_reviews_dataset',\n                length_used=None,\n                feature_extraction='CountVectorizer(ngram_range=(1,1))',\n                feature_selection='PCA(n_components=500, random_state=42)',\n                model=\"SVC(probability=True, class_weight='balanced', random_state=42)\",\n                accuracy=np.mean(accuracy).round(5),\n                f1=np.mean(f1).round(5),\n                recall=np.mean(recall).round(5),\n                precision=np.mean(precision).round(5),\n                roc_auc=np.mean(roc_auc).round(5))","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:59:03.144373Z","iopub.status.idle":"2024-03-12T07:59:03.144794Z","shell.execute_reply.started":"2024-03-12T07:59:03.144599Z","shell.execute_reply":"2024-03-12T07:59:03.144615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Gausian NB","metadata":{}},{"cell_type":"code","source":"# Gaussian NB()\n\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n\n# length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    model = GaussianNB()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy.append(accuracy_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n\nsave_and_print(data='fake_reviews_dataset',\n                length_used=None,\n                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n                feature_selection='PCA(n_components=500, random_state=42)',\n                model=\"GaussianNB()\",\n                accuracy=np.mean(accuracy).round(5),\n                f1=np.mean(f1).round(5),\n                recall=np.mean(recall).round(5),\n                precision=np.mean(precision).round(5),\n                roc_auc=np.mean(roc_auc).round(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Multinomial NB","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n    \n    # length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy.append(accuracy_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n\nsave_and_print(data='fake_reviews_dataset',\n                length_used=None,\n                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n                feature_selection='PCA(n_components=500, random_state=42)',\n                model=\"MultinomialNB()\",\n                accuracy=np.mean(accuracy).round(5),\n                f1=np.mean(f1).round(5),\n                recall=np.mean(recall).round(5),\n                precision=np.mean(precision).round(5),\n                roc_auc=np.mean(roc_auc).round(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Bernoulli NB","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\naccuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n\n    # length = None\n    X_train = vectorizer.fit_transform(X_train)\n    X_test = vectorizer.transform(X_test)\n    \n    # length = 'MinMaxScaler' or 'StandardScaler'\n#     X_train_text = vectorizer.fit_transform(X_train['text'])\n#     X_test_text = vectorizer.transform(X_test['text'])\n\n#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n\n    X_train = f_selection.fit_transform(X_train.toarray())\n    X_test = f_selection.transform(X_test.toarray())\n\n    model = BernoulliNB()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n\n    accuracy.append(accuracy_score(y_test, y_pred))\n    f1.append(f1_score(y_test, y_pred))\n    recall.append(recall_score(y_test, y_pred))\n    precision.append(precision_score(y_test, y_pred))\n    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n\nsave_and_print(data='fake_reviews_dataset',\n                length_used=None,\n                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n                feature_selection='PCA(n_components=500, random_state=42)',\n                model=\"BernoulliNB()\",\n                accuracy=np.mean(accuracy).round(5),\n                f1=np.mean(f1).round(5),\n                recall=np.mean(recall).round(5),\n                precision=np.mean(precision).round(5),\n                roc_auc=np.mean(roc_auc).round(5))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Results <a class=\"anchor\"  id=\"sn\"></a>","metadata":{}},{"cell_type":"code","source":"pd.read_csv('/kaggle/working/results.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-26T05:05:24.674014Z","iopub.execute_input":"2024-03-26T05:05:24.674390Z","iopub.status.idle":"2024-03-26T05:05:24.710168Z","shell.execute_reply.started":"2024-03-26T05:05:24.674363Z","shell.execute_reply":"2024-03-26T05:05:24.708965Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                     data length_used  \\\n0    fake_reviews_dataset         NaN   \n1    fake_reviews_dataset         NaN   \n2    fake_reviews_dataset         NaN   \n3    fake_reviews_dataset         NaN   \n4    fake_reviews_dataset         NaN   \n..                    ...         ...   \n270  fake_reviews_dataset         NaN   \n271  fake_reviews_dataset         NaN   \n272  fake_reviews_dataset         NaN   \n273  fake_reviews_dataset         NaN   \n274  fake_reviews_dataset         NaN   \n\n                                    feature_extraction  \\\n0    CountVectorizer(binary=False, min_df=0, ngram_...   \n1    CountVectorizer(binary=False, min_df=0, ngram_...   \n2    CountVectorizer(binary=False, min_df=0, ngram_...   \n3    CountVectorizer(binary=False, min_df=0, ngram_...   \n4    CountVectorizer(binary=False, min_df=0, ngram_...   \n..                                                 ...   \n270  CountVectorizer(binary=True, min_df=0.001, ngr...   \n271  CountVectorizer(binary=True, min_df=0.001, ngr...   \n272  CountVectorizer(binary=True, min_df=0.001, ngr...   \n273  CountVectorizer(binary=True, min_df=0.001, ngr...   \n274  CountVectorizer(binary=True, min_df=0.001, ngr...   \n\n                          feature_selection  \\\n0    PCA(n_components=500, random_state=42)   \n1    PCA(n_components=500, random_state=42)   \n2    PCA(n_components=500, random_state=42)   \n3    PCA(n_components=500, random_state=42)   \n4    PCA(n_components=500, random_state=42)   \n..                                      ...   \n270  PCA(n_components=500, random_state=42)   \n271  PCA(n_components=500, random_state=42)   \n272  PCA(n_components=500, random_state=42)   \n273  PCA(n_components=500, random_state=42)   \n274  PCA(n_components=500, random_state=42)   \n\n                                                 model  accuracy       f1  \\\n0    LogisticRegression(max_iter=1000, class_weight...   0.82855  0.82873   \n1    KNeighborsClassifier(n_neighbors=1, metric='eu...   0.66299  0.69726   \n2    KNeighborsClassifier(n_neighbors=1, metric='ma...   0.65913  0.69170   \n3    KNeighborsClassifier(n_neighbors=1, metric='mi...   0.66299  0.69726   \n4    KNeighborsClassifier(n_neighbors=1, metric='co...   0.67711  0.72248   \n..                                                 ...       ...      ...   \n270  KNeighborsClassifier(n_neighbors=3, metric='ma...   0.71364  0.67571   \n271  KNeighborsClassifier(n_neighbors=3, metric='co...   0.73798  0.71670   \n272  KNeighborsClassifier(n_neighbors=5, metric='eu...   0.73716  0.70838   \n273  KNeighborsClassifier(n_neighbors=5, metric='ma...   0.70499  0.65627   \n274  KNeighborsClassifier(n_neighbors=5, metric='co...   0.70303  0.69182   \n\n      recall  precision  roc_auc  notes  \n0    0.82988    0.82759  0.91569    NaN  \n1    0.77629    0.63289  0.66301    NaN  \n2    0.76482    0.63139  0.65915    NaN  \n3    0.77629    0.63289  0.66301    NaN  \n4    0.84062    0.63349  0.67711    NaN  \n..       ...        ...      ...    ...  \n270  0.59876    0.77727  0.74053    NaN  \n271  0.66286    0.78009  0.75273    NaN  \n272  0.63835    0.79620  0.75573    NaN  \n273  0.56464    0.78494  0.74067    NaN  \n274  0.66802    0.71853  0.75490    NaN  \n\n[275 rows x 11 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>length_used</th>\n      <th>feature_extraction</th>\n      <th>feature_selection</th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>f1</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>roc_auc</th>\n      <th>notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n      <td>0.82855</td>\n      <td>0.82873</td>\n      <td>0.82988</td>\n      <td>0.82759</td>\n      <td>0.91569</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=1, metric='eu...</td>\n      <td>0.66299</td>\n      <td>0.69726</td>\n      <td>0.77629</td>\n      <td>0.63289</td>\n      <td>0.66301</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=1, metric='ma...</td>\n      <td>0.65913</td>\n      <td>0.69170</td>\n      <td>0.76482</td>\n      <td>0.63139</td>\n      <td>0.65915</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=1, metric='mi...</td>\n      <td>0.66299</td>\n      <td>0.69726</td>\n      <td>0.77629</td>\n      <td>0.63289</td>\n      <td>0.66301</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=1, metric='co...</td>\n      <td>0.67711</td>\n      <td>0.72248</td>\n      <td>0.84062</td>\n      <td>0.63349</td>\n      <td>0.67711</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>270</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=3, metric='ma...</td>\n      <td>0.71364</td>\n      <td>0.67571</td>\n      <td>0.59876</td>\n      <td>0.77727</td>\n      <td>0.74053</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>271</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=3, metric='co...</td>\n      <td>0.73798</td>\n      <td>0.71670</td>\n      <td>0.66286</td>\n      <td>0.78009</td>\n      <td>0.75273</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>272</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=5, metric='eu...</td>\n      <td>0.73716</td>\n      <td>0.70838</td>\n      <td>0.63835</td>\n      <td>0.79620</td>\n      <td>0.75573</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>273</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=5, metric='ma...</td>\n      <td>0.70499</td>\n      <td>0.65627</td>\n      <td>0.56464</td>\n      <td>0.78494</td>\n      <td>0.74067</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>274</th>\n      <td>fake_reviews_dataset</td>\n      <td>NaN</td>\n      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n      <td>PCA(n_components=500, random_state=42)</td>\n      <td>KNeighborsClassifier(n_neighbors=5, metric='co...</td>\n      <td>0.70303</td>\n      <td>0.69182</td>\n      <td>0.66802</td>\n      <td>0.71853</td>\n      <td>0.75490</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>275 rows Ã— 11 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# # Drop sth\n# data_save = pd.read_csv('/kaggle/working/results.csv')\n# data_save = data_save.drop(0)\n# data_save.to_csv('/kaggle/working/results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:59:03.148675Z","iopub.status.idle":"2024-03-12T07:59:03.149037Z","shell.execute_reply.started":"2024-03-12T07:59:03.148852Z","shell.execute_reply":"2024-03-12T07:59:03.148867Z"},"trusted":true},"execution_count":null,"outputs":[]}]}