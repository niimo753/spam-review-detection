{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7686600,"sourceType":"datasetVersion","datasetId":4485408},{"sourceId":7929032,"sourceType":"datasetVersion","datasetId":4660271},{"sourceId":7957860,"sourceType":"datasetVersion","datasetId":4680948},{"sourceId":7961391,"sourceType":"datasetVersion","datasetId":4683398}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport time\n\n# text NLP\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport string\nfrom wordcloud import WordCloud\nfrom nltk.corpus import words\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom unidecode import unidecode\n\n# Preprocessing\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler \n\n# model\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\n\n# Score\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import KFold\nfrom scipy.sparse import hstack","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-29T09:44:24.740459Z","iopub.execute_input":"2024-05-29T09:44:24.743023Z","iopub.status.idle":"2024-05-29T09:44:24.752515Z","shell.execute_reply.started":"2024-05-29T09:44:24.742989Z","shell.execute_reply":"2024-05-29T09:44:24.751735Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Cleaning & Preprocessing","metadata":{}},{"cell_type":"code","source":"# import nltk\n# import subprocess\n\n# # Download and unzip wordnet\n# try:\n#     nltk.data.find('wordnet.zip')\n# except:\n#     nltk.download('wordnet', download_dir='/kaggle/working/')\n#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n#     subprocess.run(command.split())\n#     nltk.data.path.append('/kaggle/working/')\n\n# # Now you can import the NLTK resources as usual\n# from nltk.corpus import wordnet","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:43:54.816250Z","iopub.execute_input":"2024-03-24T09:43:54.817111Z","iopub.status.idle":"2024-03-24T09:43:54.821379Z","shell.execute_reply.started":"2024-03-24T09:43:54.817077Z","shell.execute_reply":"2024-03-24T09:43:54.820334Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# class BasicTextCleaning:\n#     def __init__(self):\n#         # define some necessary elements\n#         self.stopwords = set(stopwords.words('english'))\n#         self.words_corpus = set(words.words())\n#         self.stemmer = PorterStemmer()\n#         self.lemmatizer = WordNetLemmatizer()\n\n#         # dictionary of methods can be used\n#         self.methods = {'lowercase': str.lower,\n#                         'accent_removal': self.accent_removal,\n#                         'strip': str.strip,\n#                         'nice_display': self.nice_display,\n#                         'tokenization': nltk.word_tokenize,\n#                         'stemming': self.stemming,\n#                         'lemmatization': self.lemmatization,\n#                         'punctuation_removal': self.punctuation_removal,\n#                         'stopwords_removal': self.stopwords_removal,\n#                         'contractions_expand': self.contractions_expand,\n#                         'nonsense_removal': self.nonsense_removal,\n#                         'number_removal': self.number_removal}\n\n#         self.punctuations = '[%s]' % re.escape(string.punctuation)\n\n#     def text_cleaning(self, texts, methods=None):\n#         if not methods:\n#             methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n#                        'stopwords_removal', 'lemmatization', 'stemming']\n#         if isinstance(texts, str):\n#             texts = [texts]\n#         cleaned_texts = []\n#         for text in texts:\n#             for method in methods:\n#                 if method not in self.methods.keys():\n#                     raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n#                 text = self.methods[method](text)\n#             cleaned_texts.append(text)\n#         return cleaned_texts\n\n#     def strip_text(self, text):\n#         return text.strip()\n\n#     def lowercase(self, text):\n#         return text.lower()\n\n#     def contractions_expand(self, text):\n#         return contractions.fix(text)\n\n#     def number_removal(self, text):\n#         text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text\n\n#     def nice_display(self, text):\n#         text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def accent_removal(self, text):\n#         text = unidecode(text)\n#         return text\n\n#     def punctuation_removal(self, text):\n#         text = re.sub(self.punctuations, ' ', text)\n#         text = re.sub(r'\\s+', ' ', text)\n#         return text.strip()\n\n#     def stopwords_removal(self, text):\n#         return \" \".join([word for word in text.split() if word not in self.stopwords])\n\n#     def stemming(self, text):\n#         return \" \".join([self.stemmer.stem(word) for word in text.split()])\n\n#     def lemmatization(self, text):\n#         return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n\n#     def tokenization(self, text):\n#         return nltk.word_tokenize(text)\n\n#     def nonsense_removal(self, text):\n#         return \" \".join([word for word in text.split() if wordnet.synsets(word)])","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:09.192104Z","iopub.execute_input":"2024-03-24T09:44:09.192486Z","iopub.status.idle":"2024-03-24T09:44:09.199659Z","shell.execute_reply.started":"2024-03-24T09:44:09.192426Z","shell.execute_reply":"2024-03-24T09:44:09.198722Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# cleaning = BasicTextCleaning()\n# data['text'] = cleaning.text_cleaning(data['text_'])\n# data.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-24T09:44:20.842649Z","iopub.execute_input":"2024-03-24T09:44:20.843035Z","iopub.status.idle":"2024-03-24T09:44:20.847196Z","shell.execute_reply.started":"2024-03-24T09:44:20.843005Z","shell.execute_reply":"2024-03-24T09:44:20.846278Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/data-cleaning/datacleaning.csv')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:21:43.249623Z","iopub.execute_input":"2024-05-29T09:21:43.250633Z","iopub.status.idle":"2024-05-29T09:21:43.760074Z","shell.execute_reply.started":"2024-05-29T09:21:43.250593Z","shell.execute_reply":"2024-05-29T09:21:43.759106Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data['label'].replace('CG', 1, inplace=True)\ndata['label'].replace('OR', 0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:21:47.342775Z","iopub.execute_input":"2024-05-29T09:21:47.343142Z","iopub.status.idle":"2024-05-29T09:21:47.383595Z","shell.execute_reply.started":"2024-05-29T09:21:47.343113Z","shell.execute_reply":"2024-05-29T09:21:47.382468Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/384321868.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('CG', 1, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  data['label'].replace('OR', 0, inplace=True)\n/tmp/ipykernel_34/384321868.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  data['label'].replace('OR', 0, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"data = data.fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:21:51.044701Z","iopub.execute_input":"2024-05-29T09:21:51.045600Z","iopub.status.idle":"2024-05-29T09:21:51.078420Z","shell.execute_reply.started":"2024-05-29T09:21:51.045561Z","shell.execute_reply":"2024-05-29T09:21:51.077543Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"data['length'] = data['text'].apply(len)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:23:17.304119Z","iopub.execute_input":"2024-05-29T09:23:17.304923Z","iopub.status.idle":"2024-05-29T09:23:17.333810Z","shell.execute_reply.started":"2024-05-29T09:23:17.304894Z","shell.execute_reply":"2024-05-29T09:23:17.332988Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:23:19.486168Z","iopub.execute_input":"2024-05-29T09:23:19.486847Z","iopub.status.idle":"2024-05-29T09:23:19.503260Z","shell.execute_reply.started":"2024-05-29T09:23:19.486816Z","shell.execute_reply":"2024-05-29T09:23:19.502443Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"             category  rating  label  \\\n0  Home_and_Kitchen_5     5.0      1   \n1  Home_and_Kitchen_5     5.0      1   \n2  Home_and_Kitchen_5     5.0      1   \n3  Home_and_Kitchen_5     1.0      1   \n4  Home_and_Kitchen_5     5.0      1   \n\n                                               text_  \\\n0  Love this!  Well made, sturdy, and very comfor...   \n1  love it, a great upgrade from the original.  I...   \n2  This pillow saved my back. I love the look and...   \n3  Missing information on how to use it, but it i...   \n4  Very nice set. Good quality. We have had the s...   \n\n                                        text  length  \n0  love well made sturdi comfort love pretti      41  \n1   love great upgrad origin mine coupl year      40  \n2     pillow save back love look feel pillow      38  \n3        miss inform use great product price      35  \n4        nice set good qualiti set two month      35  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>category</th>\n      <th>rating</th>\n      <th>label</th>\n      <th>text_</th>\n      <th>text</th>\n      <th>length</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Home_and_Kitchen_5</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>Love this!  Well made, sturdy, and very comfor...</td>\n      <td>love well made sturdi comfort love pretti</td>\n      <td>41</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Home_and_Kitchen_5</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>love it, a great upgrade from the original.  I...</td>\n      <td>love great upgrad origin mine coupl year</td>\n      <td>40</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Home_and_Kitchen_5</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>This pillow saved my back. I love the look and...</td>\n      <td>pillow save back love look feel pillow</td>\n      <td>38</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Home_and_Kitchen_5</td>\n      <td>1.0</td>\n      <td>1</td>\n      <td>Missing information on how to use it, but it i...</td>\n      <td>miss inform use great product price</td>\n      <td>35</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Home_and_Kitchen_5</td>\n      <td>5.0</td>\n      <td>1</td>\n      <td>Very nice set. Good quality. We have had the s...</td>\n      <td>nice set good qualiti set two month</td>\n      <td>35</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Save result","metadata":{}},{"cell_type":"code","source":"# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n# data_save.to_csv('results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T14:21:21.632478Z","iopub.execute_input":"2024-03-22T14:21:21.633096Z","iopub.status.idle":"2024-03-22T14:21:21.642507Z","shell.execute_reply.started":"2024-03-22T14:21:21.633063Z","shell.execute_reply":"2024-03-22T14:21:21.641537Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def save_and_print(data, length_used, feature_extraction, feature_selection, model, scores):\n    data_save = pd.read_csv('/kaggle/working/results.csv')\n    for feature_extraction in feature_extractions:\n        sc = str(feature_extraction)\n        new_row = {'data': data, 'length_used': length_used, \n               'feature_extraction': feature_extraction, \n               'feature_selection': feature_selection, \n               'model': model, 'accuracy': np.mean(scores[sc]['accuracy']),\n               'f1': np.mean(scores[sc]['f1']), 'recall': np.mean(scores[sc]['recall']), \n               'precision': np.mean(scores[sc]['precision']), 'roc_auc': np.mean(scores[sc]['roc_auc']), \n               'notes': notes}\n        data_save.loc[len(data_save)] = new_row\n        data_save.to_csv('/kaggle/working/results.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:29:58.809185Z","iopub.execute_input":"2024-05-29T11:29:58.809541Z","iopub.status.idle":"2024-05-29T11:29:58.818644Z","shell.execute_reply.started":"2024-05-29T11:29:58.809513Z","shell.execute_reply":"2024-05-29T11:29:58.817661Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"X = data[['text', 'length']]\ny = data['label']","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:25:55.872767Z","iopub.execute_input":"2024-05-29T09:25:55.873216Z","iopub.status.idle":"2024-05-29T09:25:55.885046Z","shell.execute_reply.started":"2024-05-29T09:25:55.873187Z","shell.execute_reply":"2024-05-29T09:25:55.884078Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Case 1: Use length","metadata":{}},{"cell_type":"code","source":"X = data[['text', 'length']]\ny = data['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataname = 'Fake Review Dataset'\nlengh_used = 'minmaxscale'\nfeature_selection = None\nnotes = None","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:28:44.113986Z","iopub.execute_input":"2024-05-29T09:28:44.114684Z","iopub.status.idle":"2024-05-29T09:28:44.118790Z","shell.execute_reply.started":"2024-05-29T09:28:44.114643Z","shell.execute_reply":"2024-05-29T09:28:44.117642Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# logistic","metadata":{}},{"cell_type":"code","source":"feature_extractions = [\n    HashingVectorizer(ngram_range=(1, 1)),\n    HashingVectorizer(ngram_range=(1, 2)),\n    HashingVectorizer(ngram_range=(1, 3))\n]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:30:49.371097Z","iopub.execute_input":"2024-05-29T11:30:49.371453Z","iopub.status.idle":"2024-05-29T11:30:49.376376Z","shell.execute_reply.started":"2024-05-29T11:30:49.371425Z","shell.execute_reply":"2024-05-29T11:30:49.375319Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [\n    HashingVectorizer(ngram_range=(1, 1)),\n    HashingVectorizer(ngram_range=(1, 2)),\n    HashingVectorizer(ngram_range=(1, 3))\n]\n\nlength_scaler = MinMaxScaler()\nscores_lr = {}\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_text = X_train['text']\n    X_test_text = X_test['text']\n    X_train_length = X_train[['length']]\n    X_test_length = X_test[['length']]\n    \n    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n    X_test_length_scaled = length_scaler.transform(X_test_length)\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train_text)\n        X_test_hashed = vectorizer.transform(X_test_text)\n        \n        # Combine hashed features with length features\n        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n        \n        model_lr = LogisticRegression(max_iter=1000)\n        model_lr.fit(X_train_combined, y_train)\n        \n        y_test_pred = model_lr.predict(X_test_combined)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_combined)[:, 1])\n        \n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n        \n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T09:45:52.898607Z","iopub.execute_input":"2024-05-29T09:45:52.898990Z","iopub.status.idle":"2024-05-29T09:53:24.300385Z","shell.execute_reply.started":"2024-05-29T09:45:52.898966Z","shell.execute_reply":"2024-05-29T09:53:24.299050Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:31:04.320289Z","iopub.execute_input":"2024-05-29T11:31:04.321115Z","iopub.status.idle":"2024-05-29T11:31:04.347526Z","shell.execute_reply.started":"2024-05-29T11:31:04.321082Z","shell.execute_reply":"2024-05-29T11:31:04.346565Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:31:16.500412Z","iopub.execute_input":"2024-05-29T11:31:16.501385Z","iopub.status.idle":"2024-05-29T11:31:16.506330Z","shell.execute_reply.started":"2024-05-29T11:31:16.501343Z","shell.execute_reply":"2024-05-29T11:31:16.505271Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n\nlength_scaler = MinMaxScaler()\nscores_knn = {}\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_text = X_train['text']\n    X_test_text = X_test['text']\n    X_train_length = X_train[['length']]\n    X_test_length = X_test[['length']]\n    \n    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n    X_test_length_scaled = length_scaler.transform(X_test_length)\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train_text)\n        X_test_hashed = vectorizer.transform(X_test_text)\n        \n        # Combine hashed features with length features\n        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n        \n        model_knn = KNeighborsClassifier()\n        model_knn.fit(X_train_combined, y_train)\n        \n        y_test_pred = model_knn.predict(X_test_combined)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_combined)[:, 1])\n        \n        key = str(feature_extraction)\n        if key not in scores_knn:\n            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n        \n        scores_knn[key]['accuracy'].append(accuracy)\n        scores_knn[key]['f1'].append(f1)\n        scores_knn[key]['recall'].append(recall)\n        scores_knn[key]['precision'].append(precision)\n        scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T10:13:43.697135Z","iopub.execute_input":"2024-05-29T10:13:43.697826Z","iopub.status.idle":"2024-05-29T11:08:53.307186Z","shell.execute_reply.started":"2024-05-29T10:13:43.697792Z","shell.execute_reply":"2024-05-29T11:08:53.306304Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:31:24.526410Z","iopub.execute_input":"2024-05-29T11:31:24.526777Z","iopub.status.idle":"2024-05-29T11:31:24.540313Z","shell.execute_reply.started":"2024-05-29T11:31:24.526746Z","shell.execute_reply":"2024-05-29T11:31:24.539628Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"import catboost as cb\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:11:07.974925Z","iopub.execute_input":"2024-05-29T11:11:07.975626Z","iopub.status.idle":"2024-05-29T11:11:11.760273Z","shell.execute_reply.started":"2024-05-29T11:11:07.975597Z","shell.execute_reply":"2024-05-29T11:11:11.759316Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# LightGBM","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n\nlength_scaler = MinMaxScaler()\nscores_lgbm = {}\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_text = X_train['text']\n    X_test_text = X_test['text']\n    X_train_length = X_train[['length']]\n    X_test_length = X_test[['length']]\n    \n    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n    X_test_length_scaled = length_scaler.transform(X_test_length)\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train_text)\n        X_test_hashed = vectorizer.transform(X_test_text)\n        \n        # Combine hashed features with length features\n        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n        \n        model_lgbm = lgb.LGBMClassifier()\n        model_lgbm.fit(X_train_combined, y_train)\n        \n        y_test_pred = model_lgbm.predict(X_test_combined)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lgbm.predict_proba(X_test_combined)[:, 1])\n        \n        key = str(feature_extraction)\n        if key not in scores_lgbm:\n            scores_lgbm[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n        \n        scores_lgbm[key]['accuracy'].append(accuracy)\n        scores_lgbm[key]['f1'].append(f1)\n        scores_lgbm[key]['recall'].append(recall)\n        scores_lgbm[key]['precision'].append(precision)\n        scores_lgbm[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:11:23.885981Z","iopub.execute_input":"2024-05-29T11:11:23.886399Z","iopub.status.idle":"2024-05-29T11:13:04.657851Z","shell.execute_reply.started":"2024-05-29T11:11:23.886367Z","shell.execute_reply":"2024-05-29T11:13:04.657016Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.320004 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 234120\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7458\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.352111 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 234960\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7467\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.343592 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 232540\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7413\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.397081 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 234960\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7486\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n","output_type":"stream"},{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.489974 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 233714\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7439\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n  _log_warning('Converting data to scipy sparse matrix.')\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lgbm, scores_lgbm)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:31:35.735230Z","iopub.execute_input":"2024-05-29T11:31:35.736183Z","iopub.status.idle":"2024-05-29T11:31:35.749496Z","shell.execute_reply.started":"2024-05-29T11:31:35.736151Z","shell.execute_reply":"2024-05-29T11:31:35.748558Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n\nlength_scaler = MinMaxScaler()\nscores_xg = {}\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_text = X_train['text']\n    X_test_text = X_test['text']\n    X_train_length = X_train[['length']]\n    X_test_length = X_test[['length']]\n    \n    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n    X_test_length_scaled = length_scaler.transform(X_test_length)\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train_text)\n        X_test_hashed = vectorizer.transform(X_test_text)\n        \n        # Combine hashed features with length features\n        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n        \n        model_xg = XGBClassifier()\n        model_xg.fit(X_train_combined, y_train)\n        \n        y_test_pred = model_xg.predict(X_test_combined)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xg.predict_proba(X_test_combined)[:, 1])\n        \n        key = str(feature_extraction)\n        if key not in scores_xg:\n            scores_xg[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n        \n        scores_xg[key]['accuracy'].append(accuracy)\n        scores_xg[key]['f1'].append(f1)\n        scores_xg[key]['recall'].append(recall)\n        scores_xg[key]['precision'].append(precision)\n        scores_xg[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:15:03.658150Z","iopub.execute_input":"2024-05-29T11:15:03.658796Z","iopub.status.idle":"2024-05-29T11:24:58.508565Z","shell.execute_reply.started":"2024-05-29T11:15:03.658763Z","shell.execute_reply":"2024-05-29T11:24:58.507684Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xg, scores_xg)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:31:43.190145Z","iopub.execute_input":"2024-05-29T11:31:43.190483Z","iopub.status.idle":"2024-05-29T11:31:43.205385Z","shell.execute_reply.started":"2024-05-29T11:31:43.190456Z","shell.execute_reply":"2024-05-29T11:31:43.204726Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"# ADABoost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n\nlength_scaler = MinMaxScaler()\nscores_ada = {}\n\nfor train_index, test_index in kf.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    X_train_text = X_train['text']\n    X_test_text = X_test['text']\n    X_train_length = X_train[['length']]\n    X_test_length = X_test[['length']]\n    \n    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n    X_test_length_scaled = length_scaler.transform(X_test_length)\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train_text)\n        X_test_hashed = vectorizer.transform(X_test_text)\n        \n        # Combine hashed features with length features\n        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n        \n        model_ada = AdaBoostClassifier()\n        model_ada.fit(X_train_combined, y_train)\n        \n        y_test_pred = model_ada.predict(X_test_combined)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_combined)[:, 1])\n        \n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n        \n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T11:32:59.093485Z","iopub.execute_input":"2024-05-29T11:32:59.094314Z","iopub.status.idle":"2024-05-29T12:24:06.214114Z","shell.execute_reply.started":"2024-05-29T11:32:59.094280Z","shell.execute_reply":"2024-05-29T12:24:06.213254Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:31:38.347547Z","iopub.execute_input":"2024-05-29T12:31:38.348337Z","iopub.status.idle":"2024-05-29T12:31:38.363559Z","shell.execute_reply.started":"2024-05-29T12:31:38.348306Z","shell.execute_reply":"2024-05-29T12:31:38.362767Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"# CatBoost","metadata":{}},{"cell_type":"markdown","source":"# Case 2\nDon't use feature lengh, feature selection","metadata":{}},{"cell_type":"code","source":"X = data['text']\ny = data['label']","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataname = 'Fake Review Dataset'\nlengh_used = None\nfeature_selection = None\nnotes = None","metadata":{"execution":{"iopub.status.busy":"2024-05-29T00:26:12.339496Z","iopub.execute_input":"2024-05-29T00:26:12.340116Z","iopub.status.idle":"2024-05-29T00:26:12.344616Z","shell.execute_reply.started":"2024-05-29T00:26:12.340081Z","shell.execute_reply":"2024-05-29T00:26:12.343343Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,1)","metadata":{}},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"import lightgbm as lgb","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:02.328490Z","iopub.execute_input":"2024-03-27T22:54:02.328848Z","iopub.status.idle":"2024-03-27T22:54:06.695242Z","shell.execute_reply.started":"2024-03-27T22:54:02.328821Z","shell.execute_reply":"2024-03-27T22:54:06.694436Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:09:23.918631Z","iopub.execute_input":"2024-03-24T11:09:23.919366Z","iopub.status.idle":"2024-03-24T11:11:10.022005Z","shell.execute_reply.started":"2024-03-24T11:09:23.919328Z","shell.execute_reply":"2024-03-24T11:11:10.021053Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 158972\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3270\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 159236\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25498\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 157115\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3251\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12749\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25499\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 158775\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3276\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25497\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 158805\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3296\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:33:36.436013Z","iopub.execute_input":"2024-03-24T11:33:36.436394Z","iopub.status.idle":"2024-03-24T11:33:36.467783Z","shell.execute_reply.started":"2024-03-24T11:33:36.436362Z","shell.execute_reply":"2024-03-24T11:33:36.466902Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"# XG boost","metadata":{}},{"cell_type":"code","source":"from xgboost import XGBClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:11.874686Z","iopub.execute_input":"2024-03-27T22:54:11.875576Z","iopub.status.idle":"2024-03-27T22:54:11.879350Z","shell.execute_reply.started":"2024-03-27T22:54:11.875544Z","shell.execute_reply":"2024-03-27T22:54:11.878441Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:14:48.664840Z","iopub.execute_input":"2024-03-24T11:14:48.665743Z","iopub.status.idle":"2024-03-24T11:24:53.058440Z","shell.execute_reply.started":"2024-03-24T11:14:48.665707Z","shell.execute_reply":"2024-03-24T11:24:53.057464Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T11:34:28.498893Z","iopub.execute_input":"2024-03-24T11:34:28.499635Z","iopub.status.idle":"2024-03-24T11:34:28.539799Z","shell.execute_reply.started":"2024-03-24T11:34:28.499598Z","shell.execute_reply":"2024-03-24T11:34:28.538706Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"# Adaboost","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier","metadata":{"execution":{"iopub.status.busy":"2024-03-28T02:58:05.865792Z","iopub.execute_input":"2024-03-28T02:58:05.866131Z","iopub.status.idle":"2024-03-28T02:58:05.870366Z","shell.execute_reply.started":"2024-03-28T02:58:05.866106Z","shell.execute_reply":"2024-03-28T02:58:05.869417Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:34:37.946831Z","iopub.execute_input":"2024-03-24T15:34:37.947748Z","iopub.status.idle":"2024-03-24T15:44:48.086181Z","shell.execute_reply.started":"2024-03-24T15:34:37.947713Z","shell.execute_reply":"2024-03-24T15:44:48.084958Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:17:28.642738Z","iopub.execute_input":"2024-03-24T16:17:28.643415Z","iopub.status.idle":"2024-03-24T16:17:28.672748Z","shell.execute_reply.started":"2024-03-24T16:17:28.643384Z","shell.execute_reply":"2024-03-24T16:17:28.672022Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"# CATBOOST","metadata":{}},{"cell_type":"code","source":"import catboost as cb","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:23:28.988854Z","iopub.execute_input":"2024-03-28T08:23:28.989646Z","iopub.status.idle":"2024-03-28T08:23:29.191459Z","shell.execute_reply.started":"2024-03-28T08:23:28.989602Z","shell.execute_reply":"2024-03-28T08:23:29.190055Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T15:54:59.959174Z","iopub.execute_input":"2024-03-24T15:54:59.959777Z","iopub.status.idle":"2024-03-24T16:14:34.833742Z","shell.execute_reply.started":"2024-03-24T15:54:59.959746Z","shell.execute_reply":"2024-03-24T16:14:34.832801Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{"execution":{"iopub.status.busy":"2024-03-24T16:16:38.146955Z","iopub.execute_input":"2024-03-24T16:16:38.147849Z","iopub.status.idle":"2024-03-24T16:16:38.176076Z","shell.execute_reply.started":"2024-03-24T16:16:38.147814Z","shell.execute_reply":"2024-03-24T16:16:38.175073Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"# Logistic Regression","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n\nfolds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:10:22.334468Z","iopub.execute_input":"2024-03-15T12:10:22.334843Z","iopub.status.idle":"2024-03-15T12:13:07.614695Z","shell.execute_reply.started":"2024-03-15T12:10:22.334814Z","shell.execute_reply":"2024-03-15T12:13:07.613872Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:08.887605Z","iopub.execute_input":"2024-03-15T12:14:08.887924Z","iopub.status.idle":"2024-03-15T12:14:08.916658Z","shell.execute_reply.started":"2024-03-15T12:14:08.887900Z","shell.execute_reply":"2024-03-15T12:14:08.915827Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"# KNN default with ngram = 1,1\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer()]\nmetrics_param = ['cosine','minkowski']\nneighbors_param = [5, 10, 50, 100]\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n#         for metric in metrics_param:\n#             for neighbor in neighbors_param:\n        model_knn = KNeighborsClassifier()\n        model_knn.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_knn.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_knn:\n            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_knn[key]['accuracy'].append(accuracy)\n        scores_knn[key]['f1'].append(f1)\n        scores_knn[key]['recall'].append(recall)\n        scores_knn[key]['precision'].append(precision)\n        scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T00:30:07.964448Z","iopub.execute_input":"2024-05-29T00:30:07.965099Z","iopub.status.idle":"2024-05-29T00:43:45.114915Z","shell.execute_reply.started":"2024-05-29T00:30:07.965067Z","shell.execute_reply":"2024-05-29T00:43:45.114076Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"a = np.mean(scores_knn['HashingVectorizer()']['accuracy'])\nb = np.mean(scores_knn['HashingVectorizer()']['f1'])\nc = np.mean(scores_knn['HashingVectorizer()']['roc_auc'])\nprint(a,b,c)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T00:48:10.754569Z","iopub.execute_input":"2024-05-29T00:48:10.754965Z","iopub.status.idle":"2024-05-29T00:48:10.760769Z","shell.execute_reply.started":"2024-05-29T00:48:10.754933Z","shell.execute_reply":"2024-05-29T00:48:10.759917Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"0.5694739507509556 0.6939923442843808 0.7362644164722172\n","output_type":"stream"}]},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n                     HashingVectorizer(n_features = 100),HashingVectorizer()]\nmetrics_param = ['cosine','minkowski']\nneighbors_param = [5, 10, 50, 100]\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n        for metric in metrics_param:\n            for neighbor in neighbors_param:\n                model_knn = KNeighborsClassifier(metric = metric, n_neighbors = neighbor)\n                model_knn.fit(X_train_hashed, y_train)\n\n                y_test_pred = model_knn.predict(X_test_hashed)\n                accuracy = accuracy_score(y_test, y_test_pred)\n                f1 = f1_score(y_test, y_test_pred, pos_label=1)\n                recall = recall_score(y_test, y_test_pred)\n                precision = precision_score(y_test, y_test_pred)\n                roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n                key = str(feature_extraction)\n                if key not in scores_knn:\n                    scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n                scores_knn[key]['accuracy'].append(accuracy)\n                scores_knn[key]['f1'].append(f1)\n                scores_knn[key]['recall'].append(recall)\n                scores_knn[key]['precision'].append(precision)\n                scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T12:14:33.554596Z","iopub.execute_input":"2024-03-15T12:14:33.554948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,2)","metadata":{}},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 2)), HashingVectorizer(ngram_range=(1, 3))]\n\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_knn = KNeighborsClassifier()\n        model_knn.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_knn.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_knn:\n            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_knn[key]['accuracy'].append(accuracy)\n        scores_knn[key]['f1'].append(f1)\n        scores_knn[key]['recall'].append(recall)\n        scores_knn[key]['precision'].append(precision)\n        scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T01:02:00.248790Z","iopub.execute_input":"2024-05-29T01:02:00.249689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"a1 = np.mean(scores_knn['HashingVectorizer()']['accuracy'])\nb1 = np.mean(scores_knn['HashingVectorizer()']['f1'])\nc1 = np.mean(scores_knn['HashingVectorizer()']['roc_auc'])\nprint(a1,b1,c1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic ","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:02:25.521468Z","iopub.execute_input":"2024-03-27T15:02:25.522131Z","iopub.status.idle":"2024-03-27T15:05:35.869125Z","shell.execute_reply.started":"2024-03-27T15:02:25.522099Z","shell.execute_reply":"2024-03-27T15:05:35.868275Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:05:58.509988Z","iopub.execute_input":"2024-03-27T15:05:58.510712Z","iopub.status.idle":"2024-03-27T15:05:58.565915Z","shell.execute_reply.started":"2024-03-27T15:05:58.510678Z","shell.execute_reply":"2024-03-27T15:05:58.565175Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:10:52.780632Z","iopub.execute_input":"2024-03-27T15:10:52.781001Z","iopub.status.idle":"2024-03-27T15:13:14.026294Z","shell.execute_reply.started":"2024-03-27T15:10:52.780974Z","shell.execute_reply":"2024-03-27T15:13:14.025116Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 233865\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7457\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 234705\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7466\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 232285\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7412\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 234705\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7485\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 233459\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7438\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:13:46.844075Z","iopub.execute_input":"2024-03-27T15:13:46.844891Z","iopub.status.idle":"2024-03-27T15:13:46.877790Z","shell.execute_reply.started":"2024-03-27T15:13:46.844857Z","shell.execute_reply":"2024-03-27T15:13:46.877077Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# XGboost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:14:15.251179Z","iopub.execute_input":"2024-03-27T15:14:15.251999Z","iopub.status.idle":"2024-03-27T15:24:51.583910Z","shell.execute_reply.started":"2024-03-27T15:14:15.251969Z","shell.execute_reply":"2024-03-27T15:24:51.583082Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:25:30.541664Z","iopub.execute_input":"2024-03-27T15:25:30.542348Z","iopub.status.idle":"2024-03-27T15:25:30.581330Z","shell.execute_reply.started":"2024-03-27T15:25:30.542316Z","shell.execute_reply":"2024-03-27T15:25:30.580289Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"# ADA Boost","metadata":{}},{"cell_type":"code","source":"# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\n# feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n#                        HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n#                      HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n#                        HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T15:26:17.717114Z","iopub.execute_input":"2024-03-27T15:26:17.717489Z","iopub.status.idle":"2024-03-27T17:12:11.435728Z","shell.execute_reply.started":"2024-03-27T15:26:17.717459Z","shell.execute_reply":"2024-03-27T17:12:11.434913Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T17:12:11.995552Z","iopub.execute_input":"2024-03-27T17:12:11.995899Z","iopub.status.idle":"2024-03-27T17:12:12.027455Z","shell.execute_reply.started":"2024-03-27T17:12:11.995872Z","shell.execute_reply":"2024-03-27T17:12:12.026496Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"# CAT boost","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n                       HashingVectorizer(ngram_range=(1, 2))]\n\nfolds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-27T22:54:59.412722Z","iopub.execute_input":"2024-03-27T22:54:59.413103Z","iopub.status.idle":"2024-03-28T00:18:07.200952Z","shell.execute_reply.started":"2024-03-27T22:54:59.413072Z","shell.execute_reply":"2024-03-28T00:18:07.200123Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:18:41.691005Z","iopub.execute_input":"2024-03-28T00:18:41.691373Z","iopub.status.idle":"2024-03-28T00:18:41.748572Z","shell.execute_reply.started":"2024-03-28T00:18:41.691345Z","shell.execute_reply":"2024-03-28T00:18:41.747563Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Ngram = (1,3)","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 3)), \n                       HashingVectorizer(n_features = 50, ngram_range=(1, 3)),\n                     HashingVectorizer(n_features = 100, ngram_range=(1, 3)),\n                       HashingVectorizer(ngram_range=(1, 3))]","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:24:27.277322Z","iopub.execute_input":"2024-03-28T08:24:27.277695Z","iopub.status.idle":"2024-03-28T08:24:27.283725Z","shell.execute_reply.started":"2024-03-28T08:24:27.277663Z","shell.execute_reply":"2024-03-28T08:24:27.282496Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# KNN","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfeature_extractions = [HashingVectorizer(ngram_range=(1, 3))]\n\nfolds = kf.split(X, y)\nscores_knn = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_knn = KNeighborsClassifier()\n        model_knn.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_knn.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_knn[key]['accuracy'].append(accuracy)\n        scores_knn[key]['f1'].append(f1)\n        scores_knn[key]['recall'].append(recall)\n        scores_knn[key]['precision'].append(precision)\n        scores_knn[key]['roc_auc'].append(roc_auc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Logistic","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_lr = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lr = LogisticRegression(max_iter = 1000)\n        model_lr.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lr.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_lr:\n            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_lr[key]['accuracy'].append(accuracy)\n        scores_lr[key]['f1'].append(f1)\n        scores_lr[key]['recall'].append(recall)\n        scores_lr[key]['precision'].append(precision)\n        scores_lr[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:19:38.071913Z","iopub.execute_input":"2024-03-28T00:19:38.072272Z","iopub.status.idle":"2024-03-28T00:23:06.865474Z","shell.execute_reply.started":"2024-03-28T00:19:38.072243Z","shell.execute_reply":"2024-03-28T00:23:06.863961Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:24:41.026131Z","iopub.execute_input":"2024-03-28T00:24:41.026957Z","iopub.status.idle":"2024-03-28T00:24:41.060699Z","shell.execute_reply.started":"2024-03-28T00:24:41.026925Z","shell.execute_reply":"2024-03-28T00:24:41.059839Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"# LGBM","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n        model_lg.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_lg.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores:\n            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores[key]['accuracy'].append(accuracy)\n        scores[key]['f1'].append(f1)\n        scores[key]['recall'].append(recall)\n        scores[key]['precision'].append(precision)\n        scores[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:25:02.807407Z","iopub.execute_input":"2024-03-28T00:25:02.807749Z","iopub.status.idle":"2024-03-28T00:27:59.076020Z","shell.execute_reply.started":"2024-03-28T00:25:02.807723Z","shell.execute_reply":"2024-03-28T00:27:59.074948Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n[LightGBM] [Info] Total Bins 274844\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 8934\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n[LightGBM] [Info] Start training from score 0.003401\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n[LightGBM] [Info] Total Bins 276631\n[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 9020\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n[LightGBM] [Info] Start training from score -0.011316\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n[LightGBM] [Info] Total Bins 273752\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8901\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n[LightGBM] [Info] Start training from score 0.007296\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n[LightGBM] [Info] Total Bins 275954\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 9012\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n[LightGBM] [Info] Start training from score -0.001731\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 2550\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 12750\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 25500\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n[LightGBM] [Info] Total Bins 274746\n[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8942\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n[LightGBM] [Info] Start training from score 0.002350\n","output_type":"stream"}]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:31:24.843273Z","iopub.execute_input":"2024-03-28T00:31:24.844046Z","iopub.status.idle":"2024-03-28T00:31:24.881137Z","shell.execute_reply.started":"2024-03-28T00:31:24.844014Z","shell.execute_reply":"2024-03-28T00:31:24.880334Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"# XG Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_xgb = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_xgb = XGBClassifier()\n        model_xgb.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_xgb.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_xgb:\n            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_xgb[key]['accuracy'].append(accuracy)\n        scores_xgb[key]['f1'].append(f1)\n        scores_xgb[key]['recall'].append(recall)\n        scores_xgb[key]['precision'].append(precision)\n        scores_xgb[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:31:44.656830Z","iopub.execute_input":"2024-03-28T00:31:44.657812Z","iopub.status.idle":"2024-03-28T00:44:10.297036Z","shell.execute_reply.started":"2024-03-28T00:31:44.657778Z","shell.execute_reply":"2024-03-28T00:44:10.296094Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T00:45:30.829263Z","iopub.execute_input":"2024-03-28T00:45:30.830059Z","iopub.status.idle":"2024-03-28T00:45:30.876509Z","shell.execute_reply.started":"2024-03-28T00:45:30.830028Z","shell.execute_reply":"2024-03-28T00:45:30.875455Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# ADA Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_ada = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n        model_ada.fit(X_train_hashed, y_train)\n\n        y_test_pred = model_ada.predict(X_test_hashed)\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_ada:\n            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_ada[key]['accuracy'].append(accuracy)\n        scores_ada[key]['f1'].append(f1)\n        scores_ada[key]['recall'].append(recall)\n        scores_ada[key]['precision'].append(precision)\n        scores_ada[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T03:00:18.029746Z","iopub.execute_input":"2024-03-28T03:00:18.030466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CAT Boost","metadata":{}},{"cell_type":"code","source":"folds = kf.split(X, y)\nscores_cat = {}\nfor train_index, test_index in folds:\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n    for feature_extraction in feature_extractions:\n        vectorizer = feature_extraction\n        X_train_hashed = vectorizer.fit_transform(X_train)\n        X_test_hashed = vectorizer.transform(X_test)\n\n        model_cat = cb.CatBoostClassifier(\n            random_state=42,\n            task_type='GPU',\n            border_count=2 * len(feature_extractions)\n        )\n\n        model_cat.fit(\n            X_train_hashed,\n            y_train,\n            verbose=False,\n            plot=False,\n            early_stopping_rounds=50,\n            use_best_model=True,\n            eval_set=(X_test_hashed, y_test)\n        )\n\n        y_test_pred = model_cat.predict(X_test_hashed)\n\n        accuracy = accuracy_score(y_test, y_test_pred)\n        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n        recall = recall_score(y_test, y_test_pred)\n        precision = precision_score(y_test, y_test_pred)\n        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n\n        key = str(feature_extraction)\n        if key not in scores_cat:\n            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n\n        scores_cat[key]['accuracy'].append(accuracy)\n        scores_cat[key]['f1'].append(f1)\n        scores_cat[key]['recall'].append(recall)\n        scores_cat[key]['precision'].append(precision)\n        scores_cat[key]['roc_auc'].append(roc_auc)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T08:24:38.987635Z","iopub.execute_input":"2024-03-28T08:24:38.988338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Result","metadata":{}},{"cell_type":"code","source":"result = pd.read_csv('/kaggle/working/results.csv')\n# result = result.sort_values(by='accuracy', ascending=False)\nresult.tail(10)","metadata":{"execution":{"iopub.status.busy":"2024-05-29T12:32:03.771692Z","iopub.execute_input":"2024-05-29T12:32:03.772040Z","iopub.status.idle":"2024-05-29T12:32:03.793920Z","shell.execute_reply.started":"2024-05-29T12:32:03.772013Z","shell.execute_reply":"2024-05-29T12:32:03.793061Z"},"trusted":true},"execution_count":46,"outputs":[{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                   data  length_used  \\\n77  Fake Review Dataset          NaN   \n78  Fake Review Dataset          NaN   \n79  Fake Review Dataset  minmaxscale   \n80  Fake Review Dataset  minmaxscale   \n81  Fake Review Dataset  minmaxscale   \n82  Fake Review Dataset  minmaxscale   \n83  Fake Review Dataset  minmaxscale   \n84  Fake Review Dataset  minmaxscale   \n85  Fake Review Dataset  minmaxscale   \n86  Fake Review Dataset  minmaxscale   \n\n                                   feature_extraction  feature_selection  \\\n77  HashingVectorizer(n_features=100, ngram_range=...                NaN   \n78              HashingVectorizer(ngram_range=(1, 3))                NaN   \n79              HashingVectorizer(ngram_range=(1, 2))                NaN   \n80                                HashingVectorizer()                NaN   \n81              HashingVectorizer(ngram_range=(1, 2))                NaN   \n82              HashingVectorizer(ngram_range=(1, 3))                NaN   \n83              HashingVectorizer(ngram_range=(1, 2))                NaN   \n84              HashingVectorizer(ngram_range=(1, 2))                NaN   \n85              HashingVectorizer(ngram_range=(1, 2))                NaN   \n86              HashingVectorizer(ngram_range=(1, 2))                NaN   \n\n                                                model  accuracy        f1  \\\n77               AdaBoostClassifier(n_estimators=100)  0.684865  0.686397   \n78               AdaBoostClassifier(n_estimators=100)  0.797329  0.792084   \n79                  LogisticRegression(max_iter=1000)  0.875767  0.874141   \n80                  LogisticRegression(max_iter=1000)  0.853408  0.853843   \n81                  LogisticRegression(max_iter=1000)  0.875767  0.874141   \n82                  LogisticRegression(max_iter=1000)  0.873516  0.871270   \n83                             KNeighborsClassifier()  0.570809  0.696239   \n84                                   LGBMClassifier()  0.862510  0.860396   \n85  XGBClassifier(base_score=None, booster=None, c...  0.854719  0.851104   \n86                               AdaBoostClassifier()  0.755169  0.753621   \n\n      recall  precision   roc_auc  notes  \n77  0.684461   0.688559  0.748474    NaN  \n78  0.777390   0.807344  0.889228    NaN  \n79  0.863011   0.885578  0.947730    NaN  \n80  0.856464   0.851254  0.931868    NaN  \n81  0.863011   0.885578  0.947730    NaN  \n82  0.856277   0.886818  0.945084    NaN  \n83  0.983356   0.539114  0.747059    NaN  \n84  0.847538   0.873678  0.942933    NaN  \n85  0.830525   0.872743  0.937047    NaN  \n86  0.749054   0.758277  0.847452    NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data</th>\n      <th>length_used</th>\n      <th>feature_extraction</th>\n      <th>feature_selection</th>\n      <th>model</th>\n      <th>accuracy</th>\n      <th>f1</th>\n      <th>recall</th>\n      <th>precision</th>\n      <th>roc_auc</th>\n      <th>notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>77</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(n_features=100, ngram_range=...</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.684865</td>\n      <td>0.686397</td>\n      <td>0.684461</td>\n      <td>0.688559</td>\n      <td>0.748474</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>78</th>\n      <td>Fake Review Dataset</td>\n      <td>NaN</td>\n      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier(n_estimators=100)</td>\n      <td>0.797329</td>\n      <td>0.792084</td>\n      <td>0.777390</td>\n      <td>0.807344</td>\n      <td>0.889228</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.875767</td>\n      <td>0.874141</td>\n      <td>0.863011</td>\n      <td>0.885578</td>\n      <td>0.947730</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>80</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer()</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.853408</td>\n      <td>0.853843</td>\n      <td>0.856464</td>\n      <td>0.851254</td>\n      <td>0.931868</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>81</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.875767</td>\n      <td>0.874141</td>\n      <td>0.863011</td>\n      <td>0.885578</td>\n      <td>0.947730</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n      <td>NaN</td>\n      <td>LogisticRegression(max_iter=1000)</td>\n      <td>0.873516</td>\n      <td>0.871270</td>\n      <td>0.856277</td>\n      <td>0.886818</td>\n      <td>0.945084</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>KNeighborsClassifier()</td>\n      <td>0.570809</td>\n      <td>0.696239</td>\n      <td>0.983356</td>\n      <td>0.539114</td>\n      <td>0.747059</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>LGBMClassifier()</td>\n      <td>0.862510</td>\n      <td>0.860396</td>\n      <td>0.847538</td>\n      <td>0.873678</td>\n      <td>0.942933</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>85</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n      <td>0.854719</td>\n      <td>0.851104</td>\n      <td>0.830525</td>\n      <td>0.872743</td>\n      <td>0.937047</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>Fake Review Dataset</td>\n      <td>minmaxscale</td>\n      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n      <td>NaN</td>\n      <td>AdaBoostClassifier()</td>\n      <td>0.755169</td>\n      <td>0.753621</td>\n      <td>0.749054</td>\n      <td>0.758277</td>\n      <td>0.847452</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]}]}