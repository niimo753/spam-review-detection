{"cells":[{"cell_type":"code","execution_count":15,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-05-29T09:44:24.743023Z","iopub.status.busy":"2024-05-29T09:44:24.740459Z","iopub.status.idle":"2024-05-29T09:44:24.752515Z","shell.execute_reply":"2024-05-29T09:44:24.751735Z","shell.execute_reply.started":"2024-05-29T09:44:24.742989Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import time\n","\n","# text NLP\n","import nltk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","import string\n","from wordcloud import WordCloud\n","from nltk.corpus import words, wordnet\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from unidecode import unidecode\n","\n","# Preprocessing\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler \n","\n","# model\n","from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n","from sklearn.svm import SVC\n","from sklearn.decomposition import PCA\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","\n","# Score\n","from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import roc_curve\n","from sklearn.model_selection import KFold\n","from scipy.sparse import hstack"]},{"cell_type":"markdown","metadata":{},"source":["# Cleaning & Preprocessing"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T09:43:54.817111Z","iopub.status.busy":"2024-03-24T09:43:54.816250Z","iopub.status.idle":"2024-03-24T09:43:54.821379Z","shell.execute_reply":"2024-03-24T09:43:54.820334Z","shell.execute_reply.started":"2024-03-24T09:43:54.817077Z"},"trusted":true},"outputs":[],"source":["# import nltk\n","# import subprocess\n","\n","# # Download and unzip wordnet\n","# try:\n","#     nltk.data.find('wordnet.zip')\n","# except:\n","#     nltk.download('wordnet', download_dir='/kaggle/working/')\n","#     command = \"unzip /kaggle/working/corpora/wordnet.zip -d /kaggle/working/corpora\"\n","#     subprocess.run(command.split())\n","#     nltk.data.path.append('/kaggle/working/')\n","\n","# # Now you can import the NLTK resources as usual\n","# from nltk.corpus import wordnet"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T09:44:09.192486Z","iopub.status.busy":"2024-03-24T09:44:09.192104Z","iopub.status.idle":"2024-03-24T09:44:09.199659Z","shell.execute_reply":"2024-03-24T09:44:09.198722Z","shell.execute_reply.started":"2024-03-24T09:44:09.192426Z"},"trusted":true},"outputs":[],"source":["# class BasicTextCleaning:\n","#     def __init__(self):\n","#         # define some necessary elements\n","#         self.stopwords = set(stopwords.words('english'))\n","#         self.words_corpus = set(words.words())\n","#         self.stemmer = PorterStemmer()\n","#         self.lemmatizer = WordNetLemmatizer()\n","\n","#         # dictionary of methods can be used\n","#         self.methods = {'lowercase': str.lower,\n","#                         'accent_removal': self.accent_removal,\n","#                         'strip': str.strip,\n","#                         'nice_display': self.nice_display,\n","#                         'tokenization': nltk.word_tokenize,\n","#                         'stemming': self.stemming,\n","#                         'lemmatization': self.lemmatization,\n","#                         'punctuation_removal': self.punctuation_removal,\n","#                         'stopwords_removal': self.stopwords_removal,\n","#                         'contractions_expand': self.contractions_expand,\n","#                         'nonsense_removal': self.nonsense_removal,\n","#                         'number_removal': self.number_removal}\n","\n","#         self.punctuations = '[%s]' % re.escape(string.punctuation)\n","\n","#     def text_cleaning(self, texts, methods=None):\n","#         if not methods:\n","#             methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n","#                        'stopwords_removal', 'lemmatization', 'stemming']\n","#         if isinstance(texts, str):\n","#             texts = [texts]\n","#         cleaned_texts = []\n","#         for text in texts:\n","#             for method in methods:\n","#                 if method not in self.methods.keys():\n","#                     raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n","#                 text = self.methods[method](text)\n","#             cleaned_texts.append(text)\n","#         return cleaned_texts\n","\n","#     def strip_text(self, text):\n","#         return text.strip()\n","\n","#     def lowercase(self, text):\n","#         return text.lower()\n","\n","#     def contractions_expand(self, text):\n","#         return contractions.fix(text)\n","\n","#     def number_removal(self, text):\n","#         text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n","#         text = re.sub(r'\\s+', ' ', text)\n","#         return text\n","\n","#     def nice_display(self, text):\n","#         text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n","#         text = re.sub(r'\\s+', ' ', text)\n","#         return text.strip()\n","\n","#     def accent_removal(self, text):\n","#         text = unidecode(text)\n","#         return text\n","\n","#     def punctuation_removal(self, text):\n","#         text = re.sub(self.punctuations, ' ', text)\n","#         text = re.sub(r'\\s+', ' ', text)\n","#         return text.strip()\n","\n","#     def stopwords_removal(self, text):\n","#         return \" \".join([word for word in text.split() if word not in self.stopwords])\n","\n","#     def stemming(self, text):\n","#         return \" \".join([self.stemmer.stem(word) for word in text.split()])\n","\n","#     def lemmatization(self, text):\n","#         return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n","\n","#     def tokenization(self, text):\n","#         return nltk.word_tokenize(text)\n","\n","#     def nonsense_removal(self, text):\n","#         return \" \".join([word for word in text.split() if wordnet.synsets(word)])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T09:44:20.843035Z","iopub.status.busy":"2024-03-24T09:44:20.842649Z","iopub.status.idle":"2024-03-24T09:44:20.847196Z","shell.execute_reply":"2024-03-24T09:44:20.846278Z","shell.execute_reply.started":"2024-03-24T09:44:20.843005Z"},"trusted":true},"outputs":[],"source":["# cleaning = BasicTextCleaning()\n","# data['text'] = cleaning.text_cleaning(data['text_'])\n","# data.head()"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:21:43.250633Z","iopub.status.busy":"2024-05-29T09:21:43.249623Z","iopub.status.idle":"2024-05-29T09:21:43.760074Z","shell.execute_reply":"2024-05-29T09:21:43.759106Z","shell.execute_reply.started":"2024-05-29T09:21:43.250593Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/data-cleaning/datacleaning.csv')\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:21:47.343142Z","iopub.status.busy":"2024-05-29T09:21:47.342775Z","iopub.status.idle":"2024-05-29T09:21:47.383595Z","shell.execute_reply":"2024-05-29T09:21:47.382468Z","shell.execute_reply.started":"2024-05-29T09:21:47.343113Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/384321868.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  data['label'].replace('CG', 1, inplace=True)\n","/tmp/ipykernel_34/384321868.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n","The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n","\n","For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n","\n","\n","  data['label'].replace('OR', 0, inplace=True)\n","/tmp/ipykernel_34/384321868.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  data['label'].replace('OR', 0, inplace=True)\n"]}],"source":["data['label'].replace('CG', 1, inplace=True)\n","data['label'].replace('OR', 0, inplace=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:21:51.045600Z","iopub.status.busy":"2024-05-29T09:21:51.044701Z","iopub.status.idle":"2024-05-29T09:21:51.078420Z","shell.execute_reply":"2024-05-29T09:21:51.077543Z","shell.execute_reply.started":"2024-05-29T09:21:51.045561Z"},"trusted":true},"outputs":[],"source":["data = data.fillna('')"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:23:17.304923Z","iopub.status.busy":"2024-05-29T09:23:17.304119Z","iopub.status.idle":"2024-05-29T09:23:17.333810Z","shell.execute_reply":"2024-05-29T09:23:17.332988Z","shell.execute_reply.started":"2024-05-29T09:23:17.304894Z"},"trusted":true},"outputs":[],"source":["data['length'] = data['text'].apply(len)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:23:19.486847Z","iopub.status.busy":"2024-05-29T09:23:19.486168Z","iopub.status.idle":"2024-05-29T09:23:19.503260Z","shell.execute_reply":"2024-05-29T09:23:19.502443Z","shell.execute_reply.started":"2024-05-29T09:23:19.486816Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>category</th>\n","      <th>rating</th>\n","      <th>label</th>\n","      <th>text_</th>\n","      <th>text</th>\n","      <th>length</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Home_and_Kitchen_5</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>Love this!  Well made, sturdy, and very comfor...</td>\n","      <td>love well made sturdi comfort love pretti</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Home_and_Kitchen_5</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>love it, a great upgrade from the original.  I...</td>\n","      <td>love great upgrad origin mine coupl year</td>\n","      <td>40</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Home_and_Kitchen_5</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>This pillow saved my back. I love the look and...</td>\n","      <td>pillow save back love look feel pillow</td>\n","      <td>38</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Home_and_Kitchen_5</td>\n","      <td>1.0</td>\n","      <td>1</td>\n","      <td>Missing information on how to use it, but it i...</td>\n","      <td>miss inform use great product price</td>\n","      <td>35</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Home_and_Kitchen_5</td>\n","      <td>5.0</td>\n","      <td>1</td>\n","      <td>Very nice set. Good quality. We have had the s...</td>\n","      <td>nice set good qualiti set two month</td>\n","      <td>35</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             category  rating  label  \\\n","0  Home_and_Kitchen_5     5.0      1   \n","1  Home_and_Kitchen_5     5.0      1   \n","2  Home_and_Kitchen_5     5.0      1   \n","3  Home_and_Kitchen_5     1.0      1   \n","4  Home_and_Kitchen_5     5.0      1   \n","\n","                                               text_  \\\n","0  Love this!  Well made, sturdy, and very comfor...   \n","1  love it, a great upgrade from the original.  I...   \n","2  This pillow saved my back. I love the look and...   \n","3  Missing information on how to use it, but it i...   \n","4  Very nice set. Good quality. We have had the s...   \n","\n","                                        text  length  \n","0  love well made sturdi comfort love pretti      41  \n","1   love great upgrad origin mine coupl year      40  \n","2     pillow save back love look feel pillow      38  \n","3        miss inform use great product price      35  \n","4        nice set good qualiti set two month      35  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["data.head()"]},{"cell_type":"markdown","metadata":{},"source":["# Save result"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T14:21:21.633096Z","iopub.status.busy":"2024-03-22T14:21:21.632478Z","iopub.status.idle":"2024-03-22T14:21:21.642507Z","shell.execute_reply":"2024-03-22T14:21:21.641537Z","shell.execute_reply.started":"2024-03-22T14:21:21.633063Z"},"trusted":true},"outputs":[],"source":["# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n","# data_save.to_csv('results.csv', index=False)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:29:58.809541Z","iopub.status.busy":"2024-05-29T11:29:58.809185Z","iopub.status.idle":"2024-05-29T11:29:58.818644Z","shell.execute_reply":"2024-05-29T11:29:58.817661Z","shell.execute_reply.started":"2024-05-29T11:29:58.809513Z"},"trusted":true},"outputs":[],"source":["def save_and_print(data, length_used, feature_extraction, feature_selection, model, scores):\n","    data_save = pd.read_csv('/kaggle/working/results.csv')\n","    for feature_extraction in feature_extractions:\n","        sc = str(feature_extraction)\n","        new_row = {'data': data, 'length_used': length_used, \n","               'feature_extraction': feature_extraction, \n","               'feature_selection': feature_selection, \n","               'model': model, 'accuracy': np.mean(scores[sc]['accuracy']),\n","               'f1': np.mean(scores[sc]['f1']), 'recall': np.mean(scores[sc]['recall']), \n","               'precision': np.mean(scores[sc]['precision']), 'roc_auc': np.mean(scores[sc]['roc_auc']), \n","               'notes': notes}\n","        data_save.loc[len(data_save)] = new_row\n","        data_save.to_csv('/kaggle/working/results.csv', index=False)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:25:55.873216Z","iopub.status.busy":"2024-05-29T09:25:55.872767Z","iopub.status.idle":"2024-05-29T09:25:55.885046Z","shell.execute_reply":"2024-05-29T09:25:55.884078Z","shell.execute_reply.started":"2024-05-29T09:25:55.873187Z"},"trusted":true},"outputs":[],"source":["X = data[['text', 'length']]\n","y = data['label']"]},{"cell_type":"markdown","metadata":{},"source":["# Case 1: Use length"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = data[['text', 'length']]\n","y = data['label']"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:28:44.114684Z","iopub.status.busy":"2024-05-29T09:28:44.113986Z","iopub.status.idle":"2024-05-29T09:28:44.118790Z","shell.execute_reply":"2024-05-29T09:28:44.117642Z","shell.execute_reply.started":"2024-05-29T09:28:44.114643Z"},"trusted":true},"outputs":[],"source":["dataname = 'Fake Review Dataset'\n","lengh_used = 'minmaxscale'\n","feature_selection = None\n","notes = None"]},{"cell_type":"markdown","metadata":{},"source":["# logistic"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:30:49.371453Z","iopub.status.busy":"2024-05-29T11:30:49.371097Z","iopub.status.idle":"2024-05-29T11:30:49.376376Z","shell.execute_reply":"2024-05-29T11:30:49.375319Z","shell.execute_reply.started":"2024-05-29T11:30:49.371425Z"},"trusted":true},"outputs":[],"source":["feature_extractions = [\n","    HashingVectorizer(ngram_range=(1, 1)),\n","    HashingVectorizer(ngram_range=(1, 2)),\n","    HashingVectorizer(ngram_range=(1, 3))\n","]"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T09:45:52.898990Z","iopub.status.busy":"2024-05-29T09:45:52.898607Z","iopub.status.idle":"2024-05-29T09:53:24.300385Z","shell.execute_reply":"2024-05-29T09:53:24.299050Z","shell.execute_reply.started":"2024-05-29T09:45:52.898966Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [\n","    HashingVectorizer(ngram_range=(1, 1)),\n","    HashingVectorizer(ngram_range=(1, 2)),\n","    HashingVectorizer(ngram_range=(1, 3))\n","]\n","\n","length_scaler = MinMaxScaler()\n","scores_lr = {}\n","\n","for train_index, test_index in kf.split(X, y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    X_train_text = X_train['text']\n","    X_test_text = X_test['text']\n","    X_train_length = X_train[['length']]\n","    X_test_length = X_test[['length']]\n","    \n","    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n","    X_test_length_scaled = length_scaler.transform(X_test_length)\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train_text)\n","        X_test_hashed = vectorizer.transform(X_test_text)\n","        \n","        # Combine hashed features with length features\n","        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n","        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n","        \n","        model_lr = LogisticRegression(max_iter=1000)\n","        model_lr.fit(X_train_combined, y_train)\n","        \n","        y_test_pred = model_lr.predict(X_test_combined)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_combined)[:, 1])\n","        \n","        key = str(feature_extraction)\n","        if key not in scores_lr:\n","            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","        \n","        scores_lr[key]['accuracy'].append(accuracy)\n","        scores_lr[key]['f1'].append(f1)\n","        scores_lr[key]['recall'].append(recall)\n","        scores_lr[key]['precision'].append(precision)\n","        scores_lr[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:31:04.321115Z","iopub.status.busy":"2024-05-29T11:31:04.320289Z","iopub.status.idle":"2024-05-29T11:31:04.347526Z","shell.execute_reply":"2024-05-29T11:31:04.346565Z","shell.execute_reply.started":"2024-05-29T11:31:04.321082Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)"]},{"cell_type":"markdown","metadata":{},"source":["# KNN"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:31:16.501385Z","iopub.status.busy":"2024-05-29T11:31:16.500412Z","iopub.status.idle":"2024-05-29T11:31:16.506330Z","shell.execute_reply":"2024-05-29T11:31:16.505271Z","shell.execute_reply.started":"2024-05-29T11:31:16.501343Z"},"trusted":true},"outputs":[],"source":["feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T10:13:43.697826Z","iopub.status.busy":"2024-05-29T10:13:43.697135Z","iopub.status.idle":"2024-05-29T11:08:53.307186Z","shell.execute_reply":"2024-05-29T11:08:53.306304Z","shell.execute_reply.started":"2024-05-29T10:13:43.697792Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n","\n","length_scaler = MinMaxScaler()\n","scores_knn = {}\n","\n","for train_index, test_index in kf.split(X, y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    X_train_text = X_train['text']\n","    X_test_text = X_test['text']\n","    X_train_length = X_train[['length']]\n","    X_test_length = X_test[['length']]\n","    \n","    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n","    X_test_length_scaled = length_scaler.transform(X_test_length)\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train_text)\n","        X_test_hashed = vectorizer.transform(X_test_text)\n","        \n","        # Combine hashed features with length features\n","        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n","        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n","        \n","        model_knn = KNeighborsClassifier()\n","        model_knn.fit(X_train_combined, y_train)\n","        \n","        y_test_pred = model_knn.predict(X_test_combined)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_combined)[:, 1])\n","        \n","        key = str(feature_extraction)\n","        if key not in scores_knn:\n","            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","        \n","        scores_knn[key]['accuracy'].append(accuracy)\n","        scores_knn[key]['f1'].append(f1)\n","        scores_knn[key]['recall'].append(recall)\n","        scores_knn[key]['precision'].append(precision)\n","        scores_knn[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:31:24.526777Z","iopub.status.busy":"2024-05-29T11:31:24.526410Z","iopub.status.idle":"2024-05-29T11:31:24.540313Z","shell.execute_reply":"2024-05-29T11:31:24.539628Z","shell.execute_reply.started":"2024-05-29T11:31:24.526746Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:11:07.975626Z","iopub.status.busy":"2024-05-29T11:11:07.974925Z","iopub.status.idle":"2024-05-29T11:11:11.760273Z","shell.execute_reply":"2024-05-29T11:11:11.759316Z","shell.execute_reply.started":"2024-05-29T11:11:07.975597Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n","  warnings.warn(\n"]}],"source":["import catboost as cb\n","from sklearn.ensemble import AdaBoostClassifier\n","from xgboost import XGBClassifier\n","import lightgbm as lgb"]},{"cell_type":"markdown","metadata":{},"source":["# LightGBM"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:11:23.886399Z","iopub.status.busy":"2024-05-29T11:11:23.885981Z","iopub.status.idle":"2024-05-29T11:13:04.657851Z","shell.execute_reply":"2024-05-29T11:13:04.657016Z","shell.execute_reply.started":"2024-05-29T11:11:23.886367Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.320004 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 234120\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7458\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n","/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.352111 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 234960\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7467\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n","/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.343592 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 232540\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7413\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n","/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.397081 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 234960\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7486\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n","/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.489974 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 233714\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7439\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n","/opt/conda/lib/python3.10/site-packages/lightgbm/basic.py:1192: UserWarning: Converting data to scipy sparse matrix.\n","  _log_warning('Converting data to scipy sparse matrix.')\n"]}],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n","\n","length_scaler = MinMaxScaler()\n","scores_lgbm = {}\n","\n","for train_index, test_index in kf.split(X, y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    X_train_text = X_train['text']\n","    X_test_text = X_test['text']\n","    X_train_length = X_train[['length']]\n","    X_test_length = X_test[['length']]\n","    \n","    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n","    X_test_length_scaled = length_scaler.transform(X_test_length)\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train_text)\n","        X_test_hashed = vectorizer.transform(X_test_text)\n","        \n","        # Combine hashed features with length features\n","        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n","        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n","        \n","        model_lgbm = lgb.LGBMClassifier()\n","        model_lgbm.fit(X_train_combined, y_train)\n","        \n","        y_test_pred = model_lgbm.predict(X_test_combined)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lgbm.predict_proba(X_test_combined)[:, 1])\n","        \n","        key = str(feature_extraction)\n","        if key not in scores_lgbm:\n","            scores_lgbm[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","        \n","        scores_lgbm[key]['accuracy'].append(accuracy)\n","        scores_lgbm[key]['f1'].append(f1)\n","        scores_lgbm[key]['recall'].append(recall)\n","        scores_lgbm[key]['precision'].append(precision)\n","        scores_lgbm[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:31:35.736183Z","iopub.status.busy":"2024-05-29T11:31:35.735230Z","iopub.status.idle":"2024-05-29T11:31:35.749496Z","shell.execute_reply":"2024-05-29T11:31:35.748558Z","shell.execute_reply.started":"2024-05-29T11:31:35.736151Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lgbm, scores_lgbm)"]},{"cell_type":"markdown","metadata":{},"source":["# XGBoost"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:15:03.658796Z","iopub.status.busy":"2024-05-29T11:15:03.658150Z","iopub.status.idle":"2024-05-29T11:24:58.508565Z","shell.execute_reply":"2024-05-29T11:24:58.507684Z","shell.execute_reply.started":"2024-05-29T11:15:03.658763Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n","\n","length_scaler = MinMaxScaler()\n","scores_xg = {}\n","\n","for train_index, test_index in kf.split(X, y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    X_train_text = X_train['text']\n","    X_test_text = X_test['text']\n","    X_train_length = X_train[['length']]\n","    X_test_length = X_test[['length']]\n","    \n","    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n","    X_test_length_scaled = length_scaler.transform(X_test_length)\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train_text)\n","        X_test_hashed = vectorizer.transform(X_test_text)\n","        \n","        # Combine hashed features with length features\n","        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n","        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n","        \n","        model_xg = XGBClassifier()\n","        model_xg.fit(X_train_combined, y_train)\n","        \n","        y_test_pred = model_xg.predict(X_test_combined)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_xg.predict_proba(X_test_combined)[:, 1])\n","        \n","        key = str(feature_extraction)\n","        if key not in scores_xg:\n","            scores_xg[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","        \n","        scores_xg[key]['accuracy'].append(accuracy)\n","        scores_xg[key]['f1'].append(f1)\n","        scores_xg[key]['recall'].append(recall)\n","        scores_xg[key]['precision'].append(precision)\n","        scores_xg[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:31:43.190483Z","iopub.status.busy":"2024-05-29T11:31:43.190145Z","iopub.status.idle":"2024-05-29T11:31:43.205385Z","shell.execute_reply":"2024-05-29T11:31:43.204726Z","shell.execute_reply.started":"2024-05-29T11:31:43.190456Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xg, scores_xg)"]},{"cell_type":"markdown","metadata":{},"source":["# ADABoost"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T11:32:59.094314Z","iopub.status.busy":"2024-05-29T11:32:59.093485Z","iopub.status.idle":"2024-05-29T12:24:06.214114Z","shell.execute_reply":"2024-05-29T12:24:06.213254Z","shell.execute_reply.started":"2024-05-29T11:32:59.094280Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 2))]\n","\n","length_scaler = MinMaxScaler()\n","scores_ada = {}\n","\n","for train_index, test_index in kf.split(X, y):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    X_train_text = X_train['text']\n","    X_test_text = X_test['text']\n","    X_train_length = X_train[['length']]\n","    X_test_length = X_test[['length']]\n","    \n","    X_train_length_scaled = length_scaler.fit_transform(X_train_length)\n","    X_test_length_scaled = length_scaler.transform(X_test_length)\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train_text)\n","        X_test_hashed = vectorizer.transform(X_test_text)\n","        \n","        # Combine hashed features with length features\n","        X_train_combined = hstack((X_train_hashed, X_train_length_scaled))\n","        X_test_combined = hstack((X_test_hashed, X_test_length_scaled))\n","        \n","        model_ada = AdaBoostClassifier()\n","        model_ada.fit(X_train_combined, y_train)\n","        \n","        y_test_pred = model_ada.predict(X_test_combined)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_combined)[:, 1])\n","        \n","        key = str(feature_extraction)\n","        if key not in scores_ada:\n","            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","        \n","        scores_ada[key]['accuracy'].append(accuracy)\n","        scores_ada[key]['f1'].append(f1)\n","        scores_ada[key]['recall'].append(recall)\n","        scores_ada[key]['precision'].append(precision)\n","        scores_ada[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T12:31:38.348337Z","iopub.status.busy":"2024-05-29T12:31:38.347547Z","iopub.status.idle":"2024-05-29T12:31:38.363559Z","shell.execute_reply":"2024-05-29T12:31:38.362767Z","shell.execute_reply.started":"2024-05-29T12:31:38.348306Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)"]},{"cell_type":"markdown","metadata":{},"source":["# CatBoost"]},{"cell_type":"markdown","metadata":{},"source":["# Case 2\n","Don't use feature lengh, feature selection"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X = data['text']\n","y = data['label']"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T00:26:12.340116Z","iopub.status.busy":"2024-05-29T00:26:12.339496Z","iopub.status.idle":"2024-05-29T00:26:12.344616Z","shell.execute_reply":"2024-05-29T00:26:12.343343Z","shell.execute_reply.started":"2024-05-29T00:26:12.340081Z"},"trusted":true},"outputs":[],"source":["dataname = 'Fake Review Dataset'\n","lengh_used = None\n","feature_selection = None\n","notes = None"]},{"cell_type":"markdown","metadata":{},"source":["# Ngram = (1,1)"]},{"cell_type":"markdown","metadata":{},"source":["# LGBM"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:54:02.328848Z","iopub.status.busy":"2024-03-27T22:54:02.328490Z","iopub.status.idle":"2024-03-27T22:54:06.695242Z","shell.execute_reply":"2024-03-27T22:54:06.694436Z","shell.execute_reply.started":"2024-03-27T22:54:02.328821Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/dask/dataframe/_pyarrow_compat.py:23: UserWarning: You are using pyarrow version 11.0.0 which is known to be insecure. See https://www.cve.org/CVERecord?id=CVE-2023-47248 for further details. Please upgrade to pyarrow>=14.0.1 or install pyarrow-hotfix to patch your current version.\n","  warnings.warn(\n"]}],"source":["import lightgbm as lgb"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T11:09:23.919366Z","iopub.status.busy":"2024-03-24T11:09:23.918631Z","iopub.status.idle":"2024-03-24T11:11:10.022005Z","shell.execute_reply":"2024-03-24T11:11:10.021053Z","shell.execute_reply.started":"2024-03-24T11:09:23.919328Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 25499\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 158972\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3270\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 25498\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 159236\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 3276\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 25498\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 157115\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3251\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 12749\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 25499\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 158775\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3276\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 25497\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 158805\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 3296\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n"]}],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n","                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n","\n","folds = kf.split(X, y)\n","scores = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n","        model_lg.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lg.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores:\n","            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores[key]['accuracy'].append(accuracy)\n","        scores[key]['f1'].append(f1)\n","        scores[key]['recall'].append(recall)\n","        scores[key]['precision'].append(precision)\n","        scores[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T11:33:36.436394Z","iopub.status.busy":"2024-03-24T11:33:36.436013Z","iopub.status.idle":"2024-03-24T11:33:36.467783Z","shell.execute_reply":"2024-03-24T11:33:36.466902Z","shell.execute_reply.started":"2024-03-24T11:33:36.436362Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)"]},{"cell_type":"markdown","metadata":{},"source":["# XG boost"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:54:11.875576Z","iopub.status.busy":"2024-03-27T22:54:11.874686Z","iopub.status.idle":"2024-03-27T22:54:11.879350Z","shell.execute_reply":"2024-03-27T22:54:11.878441Z","shell.execute_reply.started":"2024-03-27T22:54:11.875544Z"},"trusted":true},"outputs":[],"source":["from xgboost import XGBClassifier"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T11:14:48.665743Z","iopub.status.busy":"2024-03-24T11:14:48.664840Z","iopub.status.idle":"2024-03-24T11:24:53.058440Z","shell.execute_reply":"2024-03-24T11:24:53.057464Z","shell.execute_reply.started":"2024-03-24T11:14:48.665707Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n","                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n","\n","folds = kf.split(X, y)\n","scores_xgb = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_xgb = XGBClassifier()\n","        model_xgb.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_xgb.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_xgb:\n","            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_xgb[key]['accuracy'].append(accuracy)\n","        scores_xgb[key]['f1'].append(f1)\n","        scores_xgb[key]['recall'].append(recall)\n","        scores_xgb[key]['precision'].append(precision)\n","        scores_xgb[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T11:34:28.499635Z","iopub.status.busy":"2024-03-24T11:34:28.498893Z","iopub.status.idle":"2024-03-24T11:34:28.539799Z","shell.execute_reply":"2024-03-24T11:34:28.538706Z","shell.execute_reply.started":"2024-03-24T11:34:28.499598Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)"]},{"cell_type":"markdown","metadata":{},"source":["# Adaboost"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T02:58:05.866131Z","iopub.status.busy":"2024-03-28T02:58:05.865792Z","iopub.status.idle":"2024-03-28T02:58:05.870366Z","shell.execute_reply":"2024-03-28T02:58:05.869417Z","shell.execute_reply.started":"2024-03-28T02:58:05.866106Z"},"trusted":true},"outputs":[],"source":["from sklearn.ensemble import AdaBoostClassifier"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:34:37.947748Z","iopub.status.busy":"2024-03-24T15:34:37.946831Z","iopub.status.idle":"2024-03-24T15:44:48.086181Z","shell.execute_reply":"2024-03-24T15:44:48.084958Z","shell.execute_reply.started":"2024-03-24T15:34:37.947713Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n","                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n","\n","folds = kf.split(X, y)\n","scores_ada = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n","        model_ada.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_ada.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_ada:\n","            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_ada[key]['accuracy'].append(accuracy)\n","        scores_ada[key]['f1'].append(f1)\n","        scores_ada[key]['recall'].append(recall)\n","        scores_ada[key]['precision'].append(precision)\n","        scores_ada[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:17:28.643415Z","iopub.status.busy":"2024-03-24T16:17:28.642738Z","iopub.status.idle":"2024-03-24T16:17:28.672748Z","shell.execute_reply":"2024-03-24T16:17:28.672022Z","shell.execute_reply.started":"2024-03-24T16:17:28.643384Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)"]},{"cell_type":"markdown","metadata":{},"source":["# CATBOOST"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T08:23:28.989646Z","iopub.status.busy":"2024-03-28T08:23:28.988854Z","iopub.status.idle":"2024-03-28T08:23:29.191459Z","shell.execute_reply":"2024-03-28T08:23:29.190055Z","shell.execute_reply.started":"2024-03-28T08:23:28.989602Z"},"trusted":true},"outputs":[],"source":["import catboost as cb"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T15:54:59.959777Z","iopub.status.busy":"2024-03-24T15:54:59.959174Z","iopub.status.idle":"2024-03-24T16:14:34.833742Z","shell.execute_reply":"2024-03-24T16:14:34.832801Z","shell.execute_reply.started":"2024-03-24T15:54:59.959746Z"},"trusted":true},"outputs":[],"source":["folds = kf.split(X, y)\n","scores_cat = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_cat = cb.CatBoostClassifier(\n","            random_state=42,\n","            task_type='GPU',\n","            border_count=2 * len(feature_extractions)\n","        )\n","\n","        model_cat.fit(\n","            X_train_hashed,\n","            y_train,\n","            verbose=False,\n","            plot=False,\n","            early_stopping_rounds=50,\n","            use_best_model=True,\n","            eval_set=(X_test_hashed, y_test)\n","        )\n","\n","        y_test_pred = model_cat.predict(X_test_hashed)\n","\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_cat:\n","            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_cat[key]['accuracy'].append(accuracy)\n","        scores_cat[key]['f1'].append(f1)\n","        scores_cat[key]['recall'].append(recall)\n","        scores_cat[key]['precision'].append(precision)\n","        scores_cat[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-03-24T16:16:38.147849Z","iopub.status.busy":"2024-03-24T16:16:38.146955Z","iopub.status.idle":"2024-03-24T16:16:38.176076Z","shell.execute_reply":"2024-03-24T16:16:38.175073Z","shell.execute_reply.started":"2024-03-24T16:16:38.147814Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic Regression"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T12:10:22.334843Z","iopub.status.busy":"2024-03-15T12:10:22.334468Z","iopub.status.idle":"2024-03-15T12:13:07.614695Z","shell.execute_reply":"2024-03-15T12:13:07.613872Z","shell.execute_reply.started":"2024-03-15T12:10:22.334814Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n","                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n","\n","folds = kf.split(X, y)\n","scores_lr = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lr = LogisticRegression(max_iter = 1000)\n","        model_lr.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lr.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_lr:\n","            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_lr[key]['accuracy'].append(accuracy)\n","        scores_lr[key]['f1'].append(f1)\n","        scores_lr[key]['recall'].append(recall)\n","        scores_lr[key]['precision'].append(precision)\n","        scores_lr[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T12:14:08.887924Z","iopub.status.busy":"2024-03-15T12:14:08.887605Z","iopub.status.idle":"2024-03-15T12:14:08.916658Z","shell.execute_reply":"2024-03-15T12:14:08.915827Z","shell.execute_reply.started":"2024-03-15T12:14:08.887900Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)"]},{"cell_type":"markdown","metadata":{},"source":["# KNN"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T00:30:07.965099Z","iopub.status.busy":"2024-05-29T00:30:07.964448Z","iopub.status.idle":"2024-05-29T00:43:45.114915Z","shell.execute_reply":"2024-05-29T00:43:45.114076Z","shell.execute_reply.started":"2024-05-29T00:30:07.965067Z"},"trusted":true},"outputs":[],"source":["# KNN default with ngram = 1,1\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer()]\n","metrics_param = ['cosine','minkowski']\n","neighbors_param = [5, 10, 50, 100]\n","folds = kf.split(X, y)\n","scores_knn = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","#         for metric in metrics_param:\n","#             for neighbor in neighbors_param:\n","        model_knn = KNeighborsClassifier()\n","        model_knn.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_knn.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_knn:\n","            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_knn[key]['accuracy'].append(accuracy)\n","        scores_knn[key]['f1'].append(f1)\n","        scores_knn[key]['recall'].append(recall)\n","        scores_knn[key]['precision'].append(precision)\n","        scores_knn[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T00:48:10.754965Z","iopub.status.busy":"2024-05-29T00:48:10.754569Z","iopub.status.idle":"2024-05-29T00:48:10.760769Z","shell.execute_reply":"2024-05-29T00:48:10.759917Z","shell.execute_reply.started":"2024-05-29T00:48:10.754933Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0.5694739507509556 0.6939923442843808 0.7362644164722172\n"]}],"source":["a = np.mean(scores_knn['HashingVectorizer()']['accuracy'])\n","b = np.mean(scores_knn['HashingVectorizer()']['f1'])\n","c = np.mean(scores_knn['HashingVectorizer()']['roc_auc'])\n","print(a,b,c)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-15T12:14:33.554948Z","iopub.status.busy":"2024-03-15T12:14:33.554596Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10), HashingVectorizer(n_features = 50),\n","                     HashingVectorizer(n_features = 100),HashingVectorizer()]\n","metrics_param = ['cosine','minkowski']\n","neighbors_param = [5, 10, 50, 100]\n","folds = kf.split(X, y)\n","scores_knn = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","        for metric in metrics_param:\n","            for neighbor in neighbors_param:\n","                model_knn = KNeighborsClassifier(metric = metric, n_neighbors = neighbor)\n","                model_knn.fit(X_train_hashed, y_train)\n","\n","                y_test_pred = model_knn.predict(X_test_hashed)\n","                accuracy = accuracy_score(y_test, y_test_pred)\n","                f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","                recall = recall_score(y_test, y_test_pred)\n","                precision = precision_score(y_test, y_test_pred)\n","                roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n","\n","                key = str(feature_extraction)\n","                if key not in scores_knn:\n","                    scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","                scores_knn[key]['accuracy'].append(accuracy)\n","                scores_knn[key]['f1'].append(f1)\n","                scores_knn[key]['recall'].append(recall)\n","                scores_knn[key]['precision'].append(precision)\n","                scores_knn[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_knn, scores_knn)"]},{"cell_type":"markdown","metadata":{},"source":["# Ngram = (1,2)"]},{"cell_type":"markdown","metadata":{},"source":["# KNN"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T01:02:00.249689Z","iopub.status.busy":"2024-05-29T01:02:00.248790Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 2)), HashingVectorizer(ngram_range=(1, 3))]\n","\n","folds = kf.split(X, y)\n","scores_knn = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_knn = KNeighborsClassifier()\n","        model_knn.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_knn.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_knn:\n","            scores_knn[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_knn[key]['accuracy'].append(accuracy)\n","        scores_knn[key]['f1'].append(f1)\n","        scores_knn[key]['recall'].append(recall)\n","        scores_knn[key]['precision'].append(precision)\n","        scores_knn[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["a1 = np.mean(scores_knn['HashingVectorizer()']['accuracy'])\n","b1 = np.mean(scores_knn['HashingVectorizer()']['f1'])\n","c1 = np.mean(scores_knn['HashingVectorizer()']['roc_auc'])\n","print(a1,b1,c1)"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic "]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:02:25.522131Z","iopub.status.busy":"2024-03-27T15:02:25.521468Z","iopub.status.idle":"2024-03-27T15:05:35.869125Z","shell.execute_reply":"2024-03-27T15:05:35.868275Z","shell.execute_reply.started":"2024-03-27T15:02:25.522099Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n","                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n","                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n","                       HashingVectorizer(ngram_range=(1, 2))]\n","\n","folds = kf.split(X, y)\n","scores_lr = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lr = LogisticRegression(max_iter = 1000)\n","        model_lr.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lr.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_lr:\n","            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_lr[key]['accuracy'].append(accuracy)\n","        scores_lr[key]['f1'].append(f1)\n","        scores_lr[key]['recall'].append(recall)\n","        scores_lr[key]['precision'].append(precision)\n","        scores_lr[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:05:58.510712Z","iopub.status.busy":"2024-03-27T15:05:58.509988Z","iopub.status.idle":"2024-03-27T15:05:58.565915Z","shell.execute_reply":"2024-03-27T15:05:58.565175Z","shell.execute_reply.started":"2024-03-27T15:05:58.510678Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)"]},{"cell_type":"markdown","metadata":{},"source":["# LGBM"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:10:52.781001Z","iopub.status.busy":"2024-03-27T15:10:52.780632Z","iopub.status.idle":"2024-03-27T15:13:14.026294Z","shell.execute_reply":"2024-03-27T15:13:14.025116Z","shell.execute_reply.started":"2024-03-27T15:10:52.780974Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 233865\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7457\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 234705\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 7466\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 232285\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7412\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 234705\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7485\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 233459\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 7438\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n"]}],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n","                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n","                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n","                       HashingVectorizer(ngram_range=(1, 2))]\n","\n","folds = kf.split(X, y)\n","scores = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n","        model_lg.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lg.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores:\n","            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores[key]['accuracy'].append(accuracy)\n","        scores[key]['f1'].append(f1)\n","        scores[key]['recall'].append(recall)\n","        scores[key]['precision'].append(precision)\n","        scores[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:13:46.844891Z","iopub.status.busy":"2024-03-27T15:13:46.844075Z","iopub.status.idle":"2024-03-27T15:13:46.877790Z","shell.execute_reply":"2024-03-27T15:13:46.877077Z","shell.execute_reply.started":"2024-03-27T15:13:46.844857Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)"]},{"cell_type":"markdown","metadata":{},"source":["# XGboost"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:14:15.251999Z","iopub.status.busy":"2024-03-27T15:14:15.251179Z","iopub.status.idle":"2024-03-27T15:24:51.583910Z","shell.execute_reply":"2024-03-27T15:24:51.583082Z","shell.execute_reply.started":"2024-03-27T15:14:15.251969Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n","                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n","                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n","                       HashingVectorizer(ngram_range=(1, 2))]\n","\n","folds = kf.split(X, y)\n","scores_xgb = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_xgb = XGBClassifier()\n","        model_xgb.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_xgb.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_xgb:\n","            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_xgb[key]['accuracy'].append(accuracy)\n","        scores_xgb[key]['f1'].append(f1)\n","        scores_xgb[key]['recall'].append(recall)\n","        scores_xgb[key]['precision'].append(precision)\n","        scores_xgb[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:25:30.542348Z","iopub.status.busy":"2024-03-27T15:25:30.541664Z","iopub.status.idle":"2024-03-27T15:25:30.581330Z","shell.execute_reply":"2024-03-27T15:25:30.580289Z","shell.execute_reply.started":"2024-03-27T15:25:30.542316Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)"]},{"cell_type":"markdown","metadata":{},"source":["# ADA Boost"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T15:26:17.717489Z","iopub.status.busy":"2024-03-27T15:26:17.717114Z","iopub.status.idle":"2024-03-27T17:12:11.435728Z","shell.execute_reply":"2024-03-27T17:12:11.434913Z","shell.execute_reply.started":"2024-03-27T15:26:17.717459Z"},"trusted":true},"outputs":[],"source":["# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n","#                        HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n","#                      HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n","#                        HashingVectorizer(ngram_range=(1, 2))]\n","\n","folds = kf.split(X, y)\n","scores_ada = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n","        model_ada.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_ada.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_ada:\n","            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_ada[key]['accuracy'].append(accuracy)\n","        scores_ada[key]['f1'].append(f1)\n","        scores_ada[key]['recall'].append(recall)\n","        scores_ada[key]['precision'].append(precision)\n","        scores_ada[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T17:12:11.995899Z","iopub.status.busy":"2024-03-27T17:12:11.995552Z","iopub.status.idle":"2024-03-27T17:12:12.027455Z","shell.execute_reply":"2024-03-27T17:12:12.026496Z","shell.execute_reply.started":"2024-03-27T17:12:11.995872Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)"]},{"cell_type":"markdown","metadata":{},"source":["# CAT boost"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-03-27T22:54:59.413103Z","iopub.status.busy":"2024-03-27T22:54:59.412722Z","iopub.status.idle":"2024-03-28T00:18:07.200952Z","shell.execute_reply":"2024-03-28T00:18:07.200123Z","shell.execute_reply.started":"2024-03-27T22:54:59.413072Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 2)), \n","                       HashingVectorizer(n_features = 50, ngram_range=(1, 2)),\n","                     HashingVectorizer(n_features = 100, ngram_range=(1, 2)),\n","                       HashingVectorizer(ngram_range=(1, 2))]\n","\n","folds = kf.split(X, y)\n","scores_cat = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_cat = cb.CatBoostClassifier(\n","            random_state=42,\n","            task_type='GPU',\n","            border_count=2 * len(feature_extractions)\n","        )\n","\n","        model_cat.fit(\n","            X_train_hashed,\n","            y_train,\n","            verbose=False,\n","            plot=False,\n","            early_stopping_rounds=50,\n","            use_best_model=True,\n","            eval_set=(X_test_hashed, y_test)\n","        )\n","\n","        y_test_pred = model_cat.predict(X_test_hashed)\n","\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_cat:\n","            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_cat[key]['accuracy'].append(accuracy)\n","        scores_cat[key]['f1'].append(f1)\n","        scores_cat[key]['recall'].append(recall)\n","        scores_cat[key]['precision'].append(precision)\n","        scores_cat[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:18:41.691373Z","iopub.status.busy":"2024-03-28T00:18:41.691005Z","iopub.status.idle":"2024-03-28T00:18:41.748572Z","shell.execute_reply":"2024-03-28T00:18:41.747563Z","shell.execute_reply.started":"2024-03-28T00:18:41.691345Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)"]},{"cell_type":"markdown","metadata":{},"source":["# Ngram = (1,3)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T08:24:27.277695Z","iopub.status.busy":"2024-03-28T08:24:27.277322Z","iopub.status.idle":"2024-03-28T08:24:27.283725Z","shell.execute_reply":"2024-03-28T08:24:27.282496Z","shell.execute_reply.started":"2024-03-28T08:24:27.277663Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(n_features = 10, ngram_range=(1, 3)), \n","                       HashingVectorizer(n_features = 50, ngram_range=(1, 3)),\n","                     HashingVectorizer(n_features = 100, ngram_range=(1, 3)),\n","                       HashingVectorizer(ngram_range=(1, 3))]"]},{"cell_type":"markdown","metadata":{},"source":["# KNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","feature_extractions = [HashingVectorizer(ngram_range=(1, 3))]\n","\n","folds = kf.split(X, y)\n","scores_knn = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_knn = KNeighborsClassifier()\n","        model_knn.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_knn.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_knn.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores:\n","            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_knn[key]['accuracy'].append(accuracy)\n","        scores_knn[key]['f1'].append(f1)\n","        scores_knn[key]['recall'].append(recall)\n","        scores_knn[key]['precision'].append(precision)\n","        scores_knn[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"markdown","metadata":{},"source":["# Logistic"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:19:38.072272Z","iopub.status.busy":"2024-03-28T00:19:38.071913Z","iopub.status.idle":"2024-03-28T00:23:06.865474Z","shell.execute_reply":"2024-03-28T00:23:06.863961Z","shell.execute_reply.started":"2024-03-28T00:19:38.072243Z"},"trusted":true},"outputs":[],"source":["folds = kf.split(X, y)\n","scores_lr = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lr = LogisticRegression(max_iter = 1000)\n","        model_lr.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lr.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lr.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_lr:\n","            scores_lr[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_lr[key]['accuracy'].append(accuracy)\n","        scores_lr[key]['f1'].append(f1)\n","        scores_lr[key]['recall'].append(recall)\n","        scores_lr[key]['precision'].append(precision)\n","        scores_lr[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:24:41.026957Z","iopub.status.busy":"2024-03-28T00:24:41.026131Z","iopub.status.idle":"2024-03-28T00:24:41.060699Z","shell.execute_reply":"2024-03-28T00:24:41.059839Z","shell.execute_reply.started":"2024-03-28T00:24:41.026925Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lr, scores_lr)"]},{"cell_type":"markdown","metadata":{},"source":["# LGBM"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:25:02.807749Z","iopub.status.busy":"2024-03-28T00:25:02.807407Z","iopub.status.idle":"2024-03-28T00:27:59.076020Z","shell.execute_reply":"2024-03-28T00:27:59.074948Z","shell.execute_reply.started":"2024-03-28T00:25:02.807723Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16200, number of negative: 16145\n","[LightGBM] [Info] Total Bins 274844\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 8934\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500850 -> initscore=0.003401\n","[LightGBM] [Info] Start training from score 0.003401\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16081, number of negative: 16264\n","[LightGBM] [Info] Total Bins 276631\n","[LightGBM] [Info] Number of data points in the train set: 32345, number of used features: 9020\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.497171 -> initscore=-0.011316\n","[LightGBM] [Info] Start training from score -0.011316\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16232, number of negative: 16114\n","[LightGBM] [Info] Total Bins 273752\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8901\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501824 -> initscore=0.007296\n","[LightGBM] [Info] Start training from score 0.007296\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16159, number of negative: 16187\n","[LightGBM] [Info] Total Bins 275954\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 9012\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499567 -> initscore=-0.001731\n","[LightGBM] [Info] Start training from score -0.001731\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 2550\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 10\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 12750\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 50\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 25500\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 100\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n","[LightGBM] [Info] Number of positive: 16192, number of negative: 16154\n","[LightGBM] [Info] Total Bins 274746\n","[LightGBM] [Info] Number of data points in the train set: 32346, number of used features: 8942\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500587 -> initscore=0.002350\n","[LightGBM] [Info] Start training from score 0.002350\n"]}],"source":["folds = kf.split(X, y)\n","scores = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_lg = lgb.LGBMClassifier(force_row_wise=True)\n","        model_lg.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_lg.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_lg.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores:\n","            scores[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores[key]['accuracy'].append(accuracy)\n","        scores[key]['f1'].append(f1)\n","        scores[key]['recall'].append(recall)\n","        scores[key]['precision'].append(precision)\n","        scores[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:31:24.844046Z","iopub.status.busy":"2024-03-28T00:31:24.843273Z","iopub.status.idle":"2024-03-28T00:31:24.881137Z","shell.execute_reply":"2024-03-28T00:31:24.880334Z","shell.execute_reply.started":"2024-03-28T00:31:24.844014Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_lg, scores)"]},{"cell_type":"markdown","metadata":{},"source":["# XG Boost"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:31:44.657812Z","iopub.status.busy":"2024-03-28T00:31:44.656830Z","iopub.status.idle":"2024-03-28T00:44:10.297036Z","shell.execute_reply":"2024-03-28T00:44:10.296094Z","shell.execute_reply.started":"2024-03-28T00:31:44.657778Z"},"trusted":true},"outputs":[],"source":["folds = kf.split(X, y)\n","scores_xgb = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_xgb = XGBClassifier()\n","        model_xgb.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_xgb.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_xgb.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_xgb:\n","            scores_xgb[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_xgb[key]['accuracy'].append(accuracy)\n","        scores_xgb[key]['f1'].append(f1)\n","        scores_xgb[key]['recall'].append(recall)\n","        scores_xgb[key]['precision'].append(precision)\n","        scores_xgb[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T00:45:30.830059Z","iopub.status.busy":"2024-03-28T00:45:30.829263Z","iopub.status.idle":"2024-03-28T00:45:30.876509Z","shell.execute_reply":"2024-03-28T00:45:30.875455Z","shell.execute_reply.started":"2024-03-28T00:45:30.830028Z"},"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_xgb, scores_xgb)"]},{"cell_type":"markdown","metadata":{},"source":["# ADA Boost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T03:00:18.030466Z","iopub.status.busy":"2024-03-28T03:00:18.029746Z"},"trusted":true},"outputs":[],"source":["folds = kf.split(X, y)\n","scores_ada = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","    \n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_ada = AdaBoostClassifier(n_estimators=100, learning_rate=1.0)\n","        model_ada.fit(X_train_hashed, y_train)\n","\n","        y_test_pred = model_ada.predict(X_test_hashed)\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_ada.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_ada:\n","            scores_ada[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_ada[key]['accuracy'].append(accuracy)\n","        scores_ada[key]['f1'].append(f1)\n","        scores_ada[key]['recall'].append(recall)\n","        scores_ada[key]['precision'].append(precision)\n","        scores_ada[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_ada, scores_ada)"]},{"cell_type":"markdown","metadata":{},"source":["# CAT Boost"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-28T08:24:38.988338Z","iopub.status.busy":"2024-03-28T08:24:38.987635Z"},"trusted":true},"outputs":[],"source":["folds = kf.split(X, y)\n","scores_cat = {}\n","for train_index, test_index in folds:\n","    X_train, X_test = X[train_index], X[test_index]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    for feature_extraction in feature_extractions:\n","        vectorizer = feature_extraction\n","        X_train_hashed = vectorizer.fit_transform(X_train)\n","        X_test_hashed = vectorizer.transform(X_test)\n","\n","        model_cat = cb.CatBoostClassifier(\n","            random_state=42,\n","            task_type='GPU',\n","            border_count=2 * len(feature_extractions)\n","        )\n","\n","        model_cat.fit(\n","            X_train_hashed,\n","            y_train,\n","            verbose=False,\n","            plot=False,\n","            early_stopping_rounds=50,\n","            use_best_model=True,\n","            eval_set=(X_test_hashed, y_test)\n","        )\n","\n","        y_test_pred = model_cat.predict(X_test_hashed)\n","\n","        accuracy = accuracy_score(y_test, y_test_pred)\n","        f1 = f1_score(y_test, y_test_pred, pos_label=1)\n","        recall = recall_score(y_test, y_test_pred)\n","        precision = precision_score(y_test, y_test_pred)\n","        roc_auc = roc_auc_score(y_test, model_cat.predict_proba(X_test_hashed)[:, 1])\n","\n","        key = str(feature_extraction)\n","        if key not in scores_cat:\n","            scores_cat[key] = {'accuracy': [], 'f1': [], 'recall': [], 'precision': [], 'roc_auc': []}\n","\n","        scores_cat[key]['accuracy'].append(accuracy)\n","        scores_cat[key]['f1'].append(f1)\n","        scores_cat[key]['recall'].append(recall)\n","        scores_cat[key]['precision'].append(precision)\n","        scores_cat[key]['roc_auc'].append(roc_auc)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["save_and_print(dataname, lengh_used, feature_extractions, feature_selection, model_cat, scores_cat)"]},{"cell_type":"markdown","metadata":{},"source":["# Result"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2024-05-29T12:32:03.772040Z","iopub.status.busy":"2024-05-29T12:32:03.771692Z","iopub.status.idle":"2024-05-29T12:32:03.793920Z","shell.execute_reply":"2024-05-29T12:32:03.793061Z","shell.execute_reply.started":"2024-05-29T12:32:03.772013Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data</th>\n","      <th>length_used</th>\n","      <th>feature_extraction</th>\n","      <th>feature_selection</th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>roc_auc</th>\n","      <th>notes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>77</th>\n","      <td>Fake Review Dataset</td>\n","      <td>NaN</td>\n","      <td>HashingVectorizer(n_features=100, ngram_range=...</td>\n","      <td>NaN</td>\n","      <td>AdaBoostClassifier(n_estimators=100)</td>\n","      <td>0.684865</td>\n","      <td>0.686397</td>\n","      <td>0.684461</td>\n","      <td>0.688559</td>\n","      <td>0.748474</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>Fake Review Dataset</td>\n","      <td>NaN</td>\n","      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n","      <td>NaN</td>\n","      <td>AdaBoostClassifier(n_estimators=100)</td>\n","      <td>0.797329</td>\n","      <td>0.792084</td>\n","      <td>0.777390</td>\n","      <td>0.807344</td>\n","      <td>0.889228</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>79</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>LogisticRegression(max_iter=1000)</td>\n","      <td>0.875767</td>\n","      <td>0.874141</td>\n","      <td>0.863011</td>\n","      <td>0.885578</td>\n","      <td>0.947730</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>80</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer()</td>\n","      <td>NaN</td>\n","      <td>LogisticRegression(max_iter=1000)</td>\n","      <td>0.853408</td>\n","      <td>0.853843</td>\n","      <td>0.856464</td>\n","      <td>0.851254</td>\n","      <td>0.931868</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>81</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>LogisticRegression(max_iter=1000)</td>\n","      <td>0.875767</td>\n","      <td>0.874141</td>\n","      <td>0.863011</td>\n","      <td>0.885578</td>\n","      <td>0.947730</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>82</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 3))</td>\n","      <td>NaN</td>\n","      <td>LogisticRegression(max_iter=1000)</td>\n","      <td>0.873516</td>\n","      <td>0.871270</td>\n","      <td>0.856277</td>\n","      <td>0.886818</td>\n","      <td>0.945084</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>KNeighborsClassifier()</td>\n","      <td>0.570809</td>\n","      <td>0.696239</td>\n","      <td>0.983356</td>\n","      <td>0.539114</td>\n","      <td>0.747059</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>84</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>LGBMClassifier()</td>\n","      <td>0.862510</td>\n","      <td>0.860396</td>\n","      <td>0.847538</td>\n","      <td>0.873678</td>\n","      <td>0.942933</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>85</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>XGBClassifier(base_score=None, booster=None, c...</td>\n","      <td>0.854719</td>\n","      <td>0.851104</td>\n","      <td>0.830525</td>\n","      <td>0.872743</td>\n","      <td>0.937047</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>Fake Review Dataset</td>\n","      <td>minmaxscale</td>\n","      <td>HashingVectorizer(ngram_range=(1, 2))</td>\n","      <td>NaN</td>\n","      <td>AdaBoostClassifier()</td>\n","      <td>0.755169</td>\n","      <td>0.753621</td>\n","      <td>0.749054</td>\n","      <td>0.758277</td>\n","      <td>0.847452</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                   data  length_used  \\\n","77  Fake Review Dataset          NaN   \n","78  Fake Review Dataset          NaN   \n","79  Fake Review Dataset  minmaxscale   \n","80  Fake Review Dataset  minmaxscale   \n","81  Fake Review Dataset  minmaxscale   \n","82  Fake Review Dataset  minmaxscale   \n","83  Fake Review Dataset  minmaxscale   \n","84  Fake Review Dataset  minmaxscale   \n","85  Fake Review Dataset  minmaxscale   \n","86  Fake Review Dataset  minmaxscale   \n","\n","                                   feature_extraction  feature_selection  \\\n","77  HashingVectorizer(n_features=100, ngram_range=...                NaN   \n","78              HashingVectorizer(ngram_range=(1, 3))                NaN   \n","79              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","80                                HashingVectorizer()                NaN   \n","81              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","82              HashingVectorizer(ngram_range=(1, 3))                NaN   \n","83              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","84              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","85              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","86              HashingVectorizer(ngram_range=(1, 2))                NaN   \n","\n","                                                model  accuracy        f1  \\\n","77               AdaBoostClassifier(n_estimators=100)  0.684865  0.686397   \n","78               AdaBoostClassifier(n_estimators=100)  0.797329  0.792084   \n","79                  LogisticRegression(max_iter=1000)  0.875767  0.874141   \n","80                  LogisticRegression(max_iter=1000)  0.853408  0.853843   \n","81                  LogisticRegression(max_iter=1000)  0.875767  0.874141   \n","82                  LogisticRegression(max_iter=1000)  0.873516  0.871270   \n","83                             KNeighborsClassifier()  0.570809  0.696239   \n","84                                   LGBMClassifier()  0.862510  0.860396   \n","85  XGBClassifier(base_score=None, booster=None, c...  0.854719  0.851104   \n","86                               AdaBoostClassifier()  0.755169  0.753621   \n","\n","      recall  precision   roc_auc  notes  \n","77  0.684461   0.688559  0.748474    NaN  \n","78  0.777390   0.807344  0.889228    NaN  \n","79  0.863011   0.885578  0.947730    NaN  \n","80  0.856464   0.851254  0.931868    NaN  \n","81  0.863011   0.885578  0.947730    NaN  \n","82  0.856277   0.886818  0.945084    NaN  \n","83  0.983356   0.539114  0.747059    NaN  \n","84  0.847538   0.873678  0.942933    NaN  \n","85  0.830525   0.872743  0.937047    NaN  \n","86  0.749054   0.758277  0.847452    NaN  "]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["result = pd.read_csv('/kaggle/working/results.csv')\n","# result = result.sort_values(by='accuracy', ascending=False)\n","result.tail(10)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4485408,"sourceId":7686600,"sourceType":"datasetVersion"},{"datasetId":4660271,"sourceId":7929032,"sourceType":"datasetVersion"},{"datasetId":4680948,"sourceId":7957860,"sourceType":"datasetVersion"},{"datasetId":4683398,"sourceId":7961391,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
