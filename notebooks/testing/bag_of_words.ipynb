{"cells":[{"cell_type":"markdown","metadata":{},"source":["* [Libraries](#s1)\n","* [Basic Function](#s2)\n","* [Initialize](#s3)\n","* [CountVectorizer](#s4)\n","* [Results](#sn)"]},{"cell_type":"markdown","metadata":{},"source":["# Libraries <a class=\"anchor\"  id=\"s1\"></a>"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T04:52:41.870350Z","iopub.status.busy":"2024-03-26T04:52:41.869486Z","iopub.status.idle":"2024-03-26T04:52:44.578235Z","shell.execute_reply":"2024-03-26T04:52:44.577000Z","shell.execute_reply.started":"2024-03-26T04:52:41.870315Z"},"trusted":true},"outputs":[],"source":["import shutil\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import re\n","import os\n","\n","# text NLP\n","import nltk\n","from nltk.util import ngrams\n","from nltk.corpus import stopwords, words\n","from nltk.stem import PorterStemmer\n","import string\n","from wordcloud import WordCloud\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from unidecode import unidecode\n","import contractions\n","\n","\n","# Preprocessing\n","from sklearn.decomposition import PCA\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split\n","from sklearn.pipeline import Pipeline\n","from sklearn.model_selection import KFold\n","\n","# model\n","from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n","from sklearn.svm import SVC\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n","from xgboost import XGBClassifier\n","\n","# Score\n","from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from sklearn.metrics import classification_report\n","from sklearn.model_selection import cross_val_score\n","from sklearn.metrics import roc_curve"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T04:52:37.450604Z","iopub.status.busy":"2024-03-26T04:52:37.449822Z","iopub.status.idle":"2024-03-26T04:52:37.476155Z","shell.execute_reply":"2024-03-26T04:52:37.474987Z","shell.execute_reply.started":"2024-03-26T04:52:37.450564Z"},"trusted":true},"outputs":[],"source":["nltk.download('all')"]},{"cell_type":"markdown","metadata":{},"source":["# Basic Function <a class=\"anchor\"  id=\"s2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-26T04:51:58.233631Z","iopub.status.idle":"2024-03-26T04:51:58.234006Z","shell.execute_reply":"2024-03-26T04:51:58.233840Z","shell.execute_reply.started":"2024-03-26T04:51:58.233824Z"},"trusted":true},"outputs":[],"source":["class BasicTextCleaning:\n","    def __init__(self):\n","        # define some necessary elements\n","        self.stopwords = set(stopwords.words('english'))\n","        self.words_corpus = set(words.words())\n","        self.stemmer = PorterStemmer()\n","        self.lemmatizer = WordNetLemmatizer()\n","\n","        # dictionary of methods can be used\n","        self.methods = {'lowercase': str.lower,\n","                        'accent_removal': self.accent_removal,\n","                        'strip': str.strip,\n","                        'nice_display': self.nice_display,\n","                        'tokenization': nltk.word_tokenize,\n","                        'stemming': self.stemming,\n","                        'lemmatization': self.lemmatization,\n","                        'punctuation_removal': self.punctuation_removal,\n","                        'stopwords_removal': self.stopwords_removal,\n","                        'contractions_expand': self.contractions_expand,\n","                        'nonsense_removal': self.nonsense_removal,\n","                        'number_removal': self.number_removal}\n","\n","        self.punctuations = '[%s]' % re.escape(string.punctuation)\n","\n","    def text_cleaning(self, texts, methods=None):\n","        if not methods:\n","            methods = ['accent_removal', 'lowercase', 'nice_display', 'punctuation_removal',\n","                       'stopwords_removal', 'lemmatization', 'stemming']\n","        if isinstance(texts, str):\n","            texts = [texts]\n","        cleaned_texts = []\n","        for text in texts:\n","            for method in methods:\n","                if method not in self.methods.keys():\n","                    raise Warning('Invalid method \"{}\". Basic text cleaning methods available: {}'.format(method, \", \".join(self.methods.keys())))\n","                text = self.methods[method](text)\n","            cleaned_texts.append(text)\n","        return cleaned_texts\n","\n","    def strip_text(self, text):\n","        return text.strip()\n","\n","    def lowercase(self, text):\n","        return text.lower()\n","\n","    def contractions_expand(self, text):\n","        return contractions.fix(text)\n","\n","    def number_removal(self, text):\n","        text = re.sub(r\"[^a-zA-Z]\", \" \", text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text\n","\n","    def nice_display(self, text):\n","        text = re.sub(r\"([^\\w\\s([{\\'])(\\w)\", r\"\\1 \\2\", text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def accent_removal(self, text):\n","        text = unidecode(text)\n","        return text\n","\n","    def punctuation_removal(self, text):\n","        text = re.sub(self.punctuations, ' ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        return text.strip()\n","\n","    def stopwords_removal(self, text):\n","        return \" \".join([word for word in text.split() if word not in self.stopwords])\n","\n","    def stemming(self, text):\n","        return \" \".join([self.stemmer.stem(word) for word in text.split()])\n","\n","    def lemmatization(self, text):\n","        return \" \".join([self.lemmatizer.lemmatize(word) for word in text.split()])\n","\n","    def tokenization(self, text):\n","        return nltk.word_tokenize(text)\n","\n","    def nonsense_removal(self, text):\n","        return \" \".join([word for word in text.split() if wordnet.synsets(word)])"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T04:52:57.150939Z","iopub.status.busy":"2024-03-26T04:52:57.150389Z","iopub.status.idle":"2024-03-26T04:52:57.159791Z","shell.execute_reply":"2024-03-26T04:52:57.158853Z","shell.execute_reply.started":"2024-03-26T04:52:57.150905Z"},"trusted":true},"outputs":[],"source":["# data_save = pd.DataFrame(columns=['data','length_used', 'feature_extraction', 'feature_selection', 'model', 'accuracy', 'f1', 'recall', 'precision', 'roc_auc', 'notes'])\n","# data_save.to_csv('results.csv', index=False)\n","def save_and_print(data, length_used, feature_extraction, feature_selection, model, accuracy, f1, recall, precision, roc_auc, notes=None):\n","    print('Accuracy:', accuracy)\n","    print('F1:', f1)\n","    print('Recall:', recall)\n","    print('Precision:', precision)\n","    print('ROC AUC:', roc_auc)\n","    data_save = pd.read_csv('/kaggle/working/results.csv')\n","    new_row = {'data': data, 'length_used': length_used, \n","               'feature_extraction': feature_extraction, \n","               'feature_selection': feature_selection, \n","               'model': model, 'accuracy': accuracy, \n","               'f1': f1, 'recall': recall, \n","               'precision': precision, 'roc_auc': roc_auc, \n","               'notes': notes}\n","    data_save.loc[len(data_save)] = new_row\n","    data_save.to_csv('/kaggle/working/results.csv', index=False)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialize <a class=\"anchor\"  id=\"s3\"></a>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T04:53:03.552439Z","iopub.status.busy":"2024-03-26T04:53:03.551999Z","iopub.status.idle":"2024-03-26T04:53:03.821903Z","shell.execute_reply":"2024-03-26T04:53:03.820712Z","shell.execute_reply.started":"2024-03-26T04:53:03.552405Z"},"trusted":true},"outputs":[],"source":["data = pd.read_csv('/kaggle/input/fake-review-dataset/data_input.csv')\n","data['text'] = data['text'].fillna('')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T14:31:19.206730Z","iopub.status.busy":"2024-03-14T14:31:19.205966Z","iopub.status.idle":"2024-03-14T14:31:19.212867Z","shell.execute_reply":"2024-03-14T14:31:19.212011Z","shell.execute_reply.started":"2024-03-14T14:31:19.206688Z"},"trusted":true},"outputs":[],"source":["# delete output data in kaggle\n","file_path = \"/kaggle/working/results.csv\"\n","if os.path.exists(file_path):\n","    os.remove(\"/kaggle/working/results.csv\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T05:05:16.962910Z","iopub.status.busy":"2024-03-26T05:05:16.962505Z","iopub.status.idle":"2024-03-26T05:05:16.978983Z","shell.execute_reply":"2024-03-26T05:05:16.977425Z","shell.execute_reply.started":"2024-03-26T05:05:16.962879Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'/kaggle/working/results.csv'"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# copy results file from input to output in kaggle for updating\n","src_path = r\"/kaggle/input/fake-review-dataset/results.csv\"\n","dst_path = r\"/kaggle/working/\"\n","shutil.copy(src_path, dst_path)"]},{"cell_type":"markdown","metadata":{},"source":["# CountVectorizer <a class=\"anchor\"  id=\"s4\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["[sklearn.feature_extraction.text.CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html): Convert a collection of text documents to a matrix of token counts.\n","- binary: 1 vs 0\n","- ngram_range: (1,1), (1,2),(1,3),(2,2),(2,3),(3,3)\n","- min_df: 0 or 0.001"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-14T15:57:24.351739Z","iopub.status.busy":"2024-03-14T15:57:24.351338Z","iopub.status.idle":"2024-03-14T15:57:24.361242Z","shell.execute_reply":"2024-03-14T15:57:24.359867Z","shell.execute_reply.started":"2024-03-14T15:57:24.351708Z"},"trusted":true},"outputs":[],"source":["length_used = None #modify\n","if length_used == None:\n","    X = data[['text']]\n","    y = data[['label']]\n","if length_used == 'MinMaxScaler':\n","    X = data[['text', 'length_minmax']]\n","    y = data['label']\n","if length_used == 'StandardScaler':\n","    X = data[['text', 'length_std']]\n","    y = data['label']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = CountVectorizer(binary=False, min_df=0, ngram_range=(1,1)) #modify\n","f_selection = PCA(n_components=500, random_state=42) #modify"]},{"cell_type":"markdown","metadata":{},"source":["## Logistic"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Logistic Regression\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","    \n","    # length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    \n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    model = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    accuracy.append(accuracy_score(y_test, y_pred))\n","    f1.append(f1_score(y_test, y_pred))\n","    recall.append(recall_score(y_test, y_pred))\n","    precision.append(precision_score(y_test, y_pred))\n","    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","    \n","save_and_print(data='fake_reviews_dataset',\n","               length_used=None,\n","               feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n","               feature_selection='PCA(n_components=500, random_state=42)',\n","               model=\"LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\",\n","               accuracy=np.mean(accuracy).round(5),\n","               f1=np.mean(f1).round(5),\n","               recall=np.mean(recall).round(5),\n","               precision=np.mean(precision).round(5),\n","               roc_auc=np.mean(roc_auc).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["## KNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# KNeighborsClassifier(n_neighbors=1/3/5,metric='euclidean'/ 'manhattan'/ 'minkowski'/'cosine')\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = {}, {}, {}, {}, {}\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","    \n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","    \n","    # length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    for n in [1,3,5]:\n","        for metric in ['euclidean', 'manhattan', 'cosine']:\n","            model = KNeighborsClassifier(n_neighbors=n, metric=metric)\n","            model.fit(X_train, y_train)\n","            y_pred = model.predict(X_test)\n","\n","            if n not in accuracy.keys():\n","                accuracy[n] = {}\n","                f1[n] = {}\n","                recall[n] = {}\n","                precision[n] = {}\n","                roc_auc[n] = {}\n","            if metric not in accuracy[n].keys():\n","                accuracy[n][metric] = []\n","                f1[n][metric] = []\n","                recall[n][metric] = []\n","                precision[n][metric] = []\n","                roc_auc[n][metric] = []\n","\n","            accuracy[n][metric].append(accuracy_score(y_test, y_pred))\n","            f1[n][metric].append(f1_score(y_test, y_pred))\n","            recall[n][metric].append(recall_score(y_test, y_pred))\n","            precision[n][metric].append(precision_score(y_test, y_pred))\n","            roc_auc[n][metric].append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","            \n","for n in [1,3,5]:\n","    for metric in ['euclidean', 'manhattan', 'cosine']:\n","        save_and_print(data='fake_reviews_dataset',\n","                       length_used=None,\n","                       feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n","                       feature_selection='PCA(n_components=500, random_state=42)',\n","                       model=f\"KNeighborsClassifier(n_neighbors={n}, metric='{metric}')\",\n","                       accuracy=np.mean(accuracy[n][metric]).round(5),\n","                       f1=np.mean(f1[n][metric]).round(5),\n","                       recall=np.mean(recall[n][metric]).round(5),\n","                       precision=np.mean(precision[n][metric]).round(5),\n","                       roc_auc=np.mean(roc_auc[n][metric]).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["## "]},{"cell_type":"markdown","metadata":{},"source":["## SVC"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:59:03.144373Z","iopub.status.idle":"2024-03-12T07:59:03.144794Z","shell.execute_reply":"2024-03-12T07:59:03.144615Z","shell.execute_reply.started":"2024-03-12T07:59:03.144599Z"},"trusted":true},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","    \n","    # length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    model = SVC(probability=True, class_weight='balanced', random_state=42)\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    accuracy.append(accuracy_score(y_test, y_pred))\n","    f1.append(f1_score(y_test, y_pred))\n","    recall.append(recall_score(y_test, y_pred))\n","    precision.append(precision_score(y_test, y_pred))\n","    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","\n","save_and_print(data='fake_reviews_dataset',\n","                length_used=None,\n","                feature_extraction='CountVectorizer(ngram_range=(1,1))',\n","                feature_selection='PCA(n_components=500, random_state=42)',\n","                model=\"SVC(probability=True, class_weight='balanced', random_state=42)\",\n","                accuracy=np.mean(accuracy).round(5),\n","                f1=np.mean(f1).round(5),\n","                recall=np.mean(recall).round(5),\n","                precision=np.mean(precision).round(5),\n","                roc_auc=np.mean(roc_auc).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["## Gausian NB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Gaussian NB()\n","\n","kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","\n","# length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    model = GaussianNB()\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    accuracy.append(accuracy_score(y_test, y_pred))\n","    f1.append(f1_score(y_test, y_pred))\n","    recall.append(recall_score(y_test, y_pred))\n","    precision.append(precision_score(y_test, y_pred))\n","    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","\n","save_and_print(data='fake_reviews_dataset',\n","                length_used=None,\n","                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n","                feature_selection='PCA(n_components=500, random_state=42)',\n","                model=\"GaussianNB()\",\n","                accuracy=np.mean(accuracy).round(5),\n","                f1=np.mean(f1).round(5),\n","                recall=np.mean(recall).round(5),\n","                precision=np.mean(precision).round(5),\n","                roc_auc=np.mean(roc_auc).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["## Multinomial NB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","    \n","    # length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    model = MultinomialNB()\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    accuracy.append(accuracy_score(y_test, y_pred))\n","    f1.append(f1_score(y_test, y_pred))\n","    recall.append(recall_score(y_test, y_pred))\n","    precision.append(precision_score(y_test, y_pred))\n","    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","\n","save_and_print(data='fake_reviews_dataset',\n","                length_used=None,\n","                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n","                feature_selection='PCA(n_components=500, random_state=42)',\n","                model=\"MultinomialNB()\",\n","                accuracy=np.mean(accuracy).round(5),\n","                f1=np.mean(f1).round(5),\n","                recall=np.mean(recall).round(5),\n","                precision=np.mean(precision).round(5),\n","                roc_auc=np.mean(roc_auc).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["## Bernoulli NB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kf = KFold(n_splits=5, shuffle=True, random_state=42)\n","accuracy, f1, recall, precision, roc_auc = [], [], [], [], []\n","\n","for train_index, test_index in kf.split(X):\n","    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n","    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","\n","    # length = None\n","    X_train = vectorizer.fit_transform(X_train)\n","    X_test = vectorizer.transform(X_test)\n","    \n","    # length = 'MinMaxScaler' or 'StandardScaler'\n","#     X_train_text = vectorizer.fit_transform(X_train['text'])\n","#     X_test_text = vectorizer.transform(X_test['text'])\n","\n","#     X_train = np.hstack((X_train_text.toarray(), X_train['length_minmax'].values.reshape(-1, 1)))\n","#     X_test = np.hstack((X_test_text.toarray(), X_test['length_minmax'].values.reshape(-1, 1)))\n","\n","    X_train = f_selection.fit_transform(X_train.toarray())\n","    X_test = f_selection.transform(X_test.toarray())\n","\n","    model = BernoulliNB()\n","    model.fit(X_train, y_train)\n","    y_pred = model.predict(X_test)\n","\n","    accuracy.append(accuracy_score(y_test, y_pred))\n","    f1.append(f1_score(y_test, y_pred))\n","    recall.append(recall_score(y_test, y_pred))\n","    precision.append(precision_score(y_test, y_pred))\n","    roc_auc.append(roc_auc_score(y_test, model.predict_proba(X_test)[:, 1]))\n","\n","save_and_print(data='fake_reviews_dataset',\n","                length_used=None,\n","                feature_extraction='CountVectorizer(binary=False, min_df=0, ngram_range=(1,1))',\n","                feature_selection='PCA(n_components=500, random_state=42)',\n","                model=\"BernoulliNB()\",\n","                accuracy=np.mean(accuracy).round(5),\n","                f1=np.mean(f1).round(5),\n","                recall=np.mean(recall).round(5),\n","                precision=np.mean(precision).round(5),\n","                roc_auc=np.mean(roc_auc).round(5))"]},{"cell_type":"markdown","metadata":{},"source":["# Results <a class=\"anchor\"  id=\"sn\"></a>"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-03-26T05:05:24.674390Z","iopub.status.busy":"2024-03-26T05:05:24.674014Z","iopub.status.idle":"2024-03-26T05:05:24.710168Z","shell.execute_reply":"2024-03-26T05:05:24.708965Z","shell.execute_reply.started":"2024-03-26T05:05:24.674363Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>data</th>\n","      <th>length_used</th>\n","      <th>feature_extraction</th>\n","      <th>feature_selection</th>\n","      <th>model</th>\n","      <th>accuracy</th>\n","      <th>f1</th>\n","      <th>recall</th>\n","      <th>precision</th>\n","      <th>roc_auc</th>\n","      <th>notes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>LogisticRegression(max_iter=1000, class_weight...</td>\n","      <td>0.82855</td>\n","      <td>0.82873</td>\n","      <td>0.82988</td>\n","      <td>0.82759</td>\n","      <td>0.91569</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=1, metric='eu...</td>\n","      <td>0.66299</td>\n","      <td>0.69726</td>\n","      <td>0.77629</td>\n","      <td>0.63289</td>\n","      <td>0.66301</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=1, metric='ma...</td>\n","      <td>0.65913</td>\n","      <td>0.69170</td>\n","      <td>0.76482</td>\n","      <td>0.63139</td>\n","      <td>0.65915</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=1, metric='mi...</td>\n","      <td>0.66299</td>\n","      <td>0.69726</td>\n","      <td>0.77629</td>\n","      <td>0.63289</td>\n","      <td>0.66301</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=False, min_df=0, ngram_...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=1, metric='co...</td>\n","      <td>0.67711</td>\n","      <td>0.72248</td>\n","      <td>0.84062</td>\n","      <td>0.63349</td>\n","      <td>0.67711</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>270</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=3, metric='ma...</td>\n","      <td>0.71364</td>\n","      <td>0.67571</td>\n","      <td>0.59876</td>\n","      <td>0.77727</td>\n","      <td>0.74053</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>271</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=3, metric='co...</td>\n","      <td>0.73798</td>\n","      <td>0.71670</td>\n","      <td>0.66286</td>\n","      <td>0.78009</td>\n","      <td>0.75273</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>272</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=5, metric='eu...</td>\n","      <td>0.73716</td>\n","      <td>0.70838</td>\n","      <td>0.63835</td>\n","      <td>0.79620</td>\n","      <td>0.75573</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>273</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=5, metric='ma...</td>\n","      <td>0.70499</td>\n","      <td>0.65627</td>\n","      <td>0.56464</td>\n","      <td>0.78494</td>\n","      <td>0.74067</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>274</th>\n","      <td>fake_reviews_dataset</td>\n","      <td>NaN</td>\n","      <td>CountVectorizer(binary=True, min_df=0.001, ngr...</td>\n","      <td>PCA(n_components=500, random_state=42)</td>\n","      <td>KNeighborsClassifier(n_neighbors=5, metric='co...</td>\n","      <td>0.70303</td>\n","      <td>0.69182</td>\n","      <td>0.66802</td>\n","      <td>0.71853</td>\n","      <td>0.75490</td>\n","      <td>NaN</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>275 rows × 11 columns</p>\n","</div>"],"text/plain":["                     data length_used  \\\n","0    fake_reviews_dataset         NaN   \n","1    fake_reviews_dataset         NaN   \n","2    fake_reviews_dataset         NaN   \n","3    fake_reviews_dataset         NaN   \n","4    fake_reviews_dataset         NaN   \n","..                    ...         ...   \n","270  fake_reviews_dataset         NaN   \n","271  fake_reviews_dataset         NaN   \n","272  fake_reviews_dataset         NaN   \n","273  fake_reviews_dataset         NaN   \n","274  fake_reviews_dataset         NaN   \n","\n","                                    feature_extraction  \\\n","0    CountVectorizer(binary=False, min_df=0, ngram_...   \n","1    CountVectorizer(binary=False, min_df=0, ngram_...   \n","2    CountVectorizer(binary=False, min_df=0, ngram_...   \n","3    CountVectorizer(binary=False, min_df=0, ngram_...   \n","4    CountVectorizer(binary=False, min_df=0, ngram_...   \n","..                                                 ...   \n","270  CountVectorizer(binary=True, min_df=0.001, ngr...   \n","271  CountVectorizer(binary=True, min_df=0.001, ngr...   \n","272  CountVectorizer(binary=True, min_df=0.001, ngr...   \n","273  CountVectorizer(binary=True, min_df=0.001, ngr...   \n","274  CountVectorizer(binary=True, min_df=0.001, ngr...   \n","\n","                          feature_selection  \\\n","0    PCA(n_components=500, random_state=42)   \n","1    PCA(n_components=500, random_state=42)   \n","2    PCA(n_components=500, random_state=42)   \n","3    PCA(n_components=500, random_state=42)   \n","4    PCA(n_components=500, random_state=42)   \n","..                                      ...   \n","270  PCA(n_components=500, random_state=42)   \n","271  PCA(n_components=500, random_state=42)   \n","272  PCA(n_components=500, random_state=42)   \n","273  PCA(n_components=500, random_state=42)   \n","274  PCA(n_components=500, random_state=42)   \n","\n","                                                 model  accuracy       f1  \\\n","0    LogisticRegression(max_iter=1000, class_weight...   0.82855  0.82873   \n","1    KNeighborsClassifier(n_neighbors=1, metric='eu...   0.66299  0.69726   \n","2    KNeighborsClassifier(n_neighbors=1, metric='ma...   0.65913  0.69170   \n","3    KNeighborsClassifier(n_neighbors=1, metric='mi...   0.66299  0.69726   \n","4    KNeighborsClassifier(n_neighbors=1, metric='co...   0.67711  0.72248   \n","..                                                 ...       ...      ...   \n","270  KNeighborsClassifier(n_neighbors=3, metric='ma...   0.71364  0.67571   \n","271  KNeighborsClassifier(n_neighbors=3, metric='co...   0.73798  0.71670   \n","272  KNeighborsClassifier(n_neighbors=5, metric='eu...   0.73716  0.70838   \n","273  KNeighborsClassifier(n_neighbors=5, metric='ma...   0.70499  0.65627   \n","274  KNeighborsClassifier(n_neighbors=5, metric='co...   0.70303  0.69182   \n","\n","      recall  precision  roc_auc  notes  \n","0    0.82988    0.82759  0.91569    NaN  \n","1    0.77629    0.63289  0.66301    NaN  \n","2    0.76482    0.63139  0.65915    NaN  \n","3    0.77629    0.63289  0.66301    NaN  \n","4    0.84062    0.63349  0.67711    NaN  \n","..       ...        ...      ...    ...  \n","270  0.59876    0.77727  0.74053    NaN  \n","271  0.66286    0.78009  0.75273    NaN  \n","272  0.63835    0.79620  0.75573    NaN  \n","273  0.56464    0.78494  0.74067    NaN  \n","274  0.66802    0.71853  0.75490    NaN  \n","\n","[275 rows x 11 columns]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["pd.read_csv('/kaggle/working/results.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-03-12T07:59:03.148675Z","iopub.status.idle":"2024-03-12T07:59:03.149037Z","shell.execute_reply":"2024-03-12T07:59:03.148867Z","shell.execute_reply.started":"2024-03-12T07:59:03.148852Z"},"trusted":true},"outputs":[],"source":["# # Drop sth\n","# data_save = pd.read_csv('/kaggle/working/results.csv')\n","# data_save = data_save.drop(0)\n","# data_save.to_csv('/kaggle/working/results.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8052353,"datasetId":4579460,"sourceId":7943411,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"}},"nbformat":4,"nbformat_minor":4}
