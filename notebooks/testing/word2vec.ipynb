{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local import\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "    sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# for preprocessing\n",
    "from src.preprocessing import BasicTextCleaning\n",
    "from src.exploration import check_balance, distribution_barplot, distribution_otherbased\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import ngrams\n",
    "\n",
    "# for modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# for timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of OFS:  (40432, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  \n",
       "0      Love this!  Well made, sturdy, and very comfor...  \n",
       "1      love it, a great upgrade from the original.  I...  \n",
       "2      This pillow saved my back. I love the look and...  \n",
       "3      Missing information on how to use it, but it i...  \n",
       "4      Very nice set. Good quality. We have had the s...  \n",
       "...                                                  ...  \n",
       "40427  I had read some reviews saying that this bra r...  \n",
       "40428  I wasn't sure exactly what it would be. It is ...  \n",
       "40429  You can wear the hood by itself, wear it with ...  \n",
       "40430  I liked nothing about this dress. The only rea...  \n",
       "40431  I work in the wedding industry and have to wor...  \n",
       "\n",
       "[40432 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osf = pd.read_csv(\"../../data/fake_reviews_dataset.csv\")\n",
    "print(\"Shape of OFS: \", osf.shape)\n",
    "# osf.head()\n",
    "osf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = BasicTextCleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    osf_cleaned = pd.read_csv(\"../../data/cleaned/osf_cleaned.csv\")\n",
    "    osf_cleaned = osf_cleaned.replace(np.nan, '')\n",
    "except:\n",
    "    osf_cleaned = pd.DataFrame()\n",
    "    osf_cleaned['length'] = osf['text_'].apply(lambda x: len(x))\n",
    "    osf_cleaned['texts'] = cleaner.text_cleaning(osf['text_'])\n",
    "\n",
    "    ordinal = OrdinalEncoder(categories=[['OR', 'CG']], dtype=int)\n",
    "    osf_cleaned['labels'] = ordinal.fit_transform(osf[['label']])\n",
    "    osf_cleaned.to_csv(\"../../data/cleaned/osf_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "class AvgWord2Vec:\n",
    "    def __init__(self, vector_size=300, min_count=1, sg=1, ngram_range=(1, 1), window=5, epochs=5, seed=42,\n",
    "                 quiet=True):\n",
    "        self.w2v = Word2Vec(vector_size=vector_size, min_count=min_count, sg=sg,\n",
    "                            window=window, workers=4, seed=seed, epochs=epochs)\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.window = window\n",
    "        self.seed = seed\n",
    "        self.vsize = vector_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.ngrams = np.arange(ngram_range[0], ngram_range[1]+1, 1)\n",
    "        self.raw = None\n",
    "        self.corpus = None\n",
    "        self.vocabulary_ = None\n",
    "        self.quiet = quiet\n",
    "\n",
    "    def _create_ngrams(self, n, X):\n",
    "        phrases = []\n",
    "        for sent in X:\n",
    "            words = sent.split()\n",
    "            if len(words) >= n:  # Check if words list is not empty\n",
    "                ngram_list = list(ngrams(words, n))\n",
    "                phrases.append([\" \".join(word) for word in ngram_list])\n",
    "            else:\n",
    "                phrases.append([])\n",
    "        return phrases\n",
    "    \n",
    "    def _create_corpus(self, X, update_train=False):\n",
    "        ngrams_phrases = {}\n",
    "        for n in self.ngrams:\n",
    "            phrases = self._create_ngrams(n, X)\n",
    "            ngrams_phrases[f\"{n}\"] = phrases\n",
    "        data = []\n",
    "        corpus = []\n",
    "        for n in ngrams_phrases.values():\n",
    "            if len(data)==0:\n",
    "                data = n\n",
    "            data = [data[i] + n[i] for i in range(len(data))]\n",
    "            corpus.extend(n)\n",
    "        if update_train:\n",
    "            self.corpus = corpus\n",
    "        return data, corpus\n",
    "    \n",
    "    def _avg_sentence(self, sentences):\n",
    "        w2v_model = self.w2v\n",
    "        avg_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence)!=0:\n",
    "                avg_sentence = np.mean([w2v_model.wv.get_vector(word) for word in sentence\n",
    "                                        if word in w2v_model.wv.key_to_index], axis=0)\n",
    "            else:\n",
    "                avg_sentence = np.zeros(w2v_model.vector_size)\n",
    "            avg_sentences.append(avg_sentence)\n",
    "        return np.array(avg_sentences)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=4, seed=self.seed, epochs=self.epochs)\n",
    "        self.raw = list(X)\n",
    "        \n",
    "        start = time.time()\n",
    "        corpus = self._create_corpus(update_train=True, X=X)[1]\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        \n",
    "        start = time.time()\n",
    "        self.w2v.build_vocab(corpus)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Build vocab: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.train(corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Training : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        self.vocabulary_ = self.w2v.wv.key_to_index\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=4, seed=self.seed, epochs=self.epochs)\n",
    "        self.raw = list(X)\n",
    "\n",
    "        start = time.time()\n",
    "        data, corpus = self._create_corpus(update_train=True, X=X)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.build_vocab(corpus)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Build vocab: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.train(corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Training : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents = self._avg_sentence(data)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Average : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents_sprs = scipy.sparse.csr_matrix(avg_sents)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Sparse : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        return avg_sents_sprs\n",
    "        \n",
    "    def transform(self, X):\n",
    "        start = time.time()\n",
    "        data = self._create_corpus(update_train=False, X=X)[0]\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents = self._avg_sentence(data)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Average : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents_sprs = scipy.sparse.csr_matrix(avg_sents)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Sparse : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        return avg_sents_sprs\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        columns = np.array([f'component_{i+1}' for i in range(self.vsize)])\n",
    "        return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(sentences, w2v_model):\n",
    "    avg_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            avg_sentence = np.mean([w2v_model.wv.get_vector(word) for word in sentence if word in w2v_model.wv.key_to_index], axis=0)\n",
    "        else:\n",
    "            avg_sentence = np.zeros(w2v_model.vector_size)\n",
    "        avg_sentences.append(avg_sentence)\n",
    "    return np.array(avg_sentences)\n",
    "\n",
    "def text_extractor(X_train, X_test, extractor):\n",
    "    X_train = extractor.fit_transform(X_train).toarray()\n",
    "    X_test = extractor.transform(X_test).toarray()\n",
    "    try:\n",
    "        X_train = pd.DataFrame(X_train, columns=extractor.get_feature_names_out())\n",
    "        X_test = pd.DataFrame(X_test, columns=extractor.get_feature_names_out())\n",
    "    except:\n",
    "        X_train = pd.DataFrame(X_train, columns=[f\"component_{i+1}\" for i in range(X_train.shape[1])])\n",
    "        X_test = pd.DataFrame(X_test, columns=[f\"component_{i+1}\" for i in range(X_test.shape[1])])  \n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_selection(X_train, X_test, selector):\n",
    "    X_train = selector.fit_transform(X_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train, columns=selector.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=selector.get_feature_names_out())\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(model, X_train, y_train, X_test, probability=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    if probability:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        return y_pred, y_pred_proba\n",
    "    return y_pred\n",
    "\n",
    "def evaluation(y_true, y_pred, y_pred_prob, scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc']):\n",
    "    scores = {'accuracy': accuracy_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'roc_auc': roc_auc_score}\n",
    "    \n",
    "    result = {}\n",
    "    for method in scoring:\n",
    "        if method == 'roc_auc':\n",
    "            result[method] = scores[method](y_true, y_pred_prob.T[1])\n",
    "        else:\n",
    "            result[method] = scores[method](y_true, y_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def cross_validation(data, extractor=None, model=None, selector=None, length_scaler=None,\n",
    "                     scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5,\n",
    "                     avg_output=True, quiet=True, data_list=None):\n",
    "    kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    indices_folds = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "    scores = {method: [] for method in scoring}\n",
    "    round_num = 1\n",
    "    fold = 0\n",
    "\n",
    "    for train_indices, test_indices in indices_folds:\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if not quiet:\n",
    "            print(f\"round {round_num}:\")\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if extractor is not None:\n",
    "            start = time.time()\n",
    "            X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        else:\n",
    "            X_train, X_test = data_list[fold][0], data_list[fold][1]\n",
    "            fold += 1\n",
    "\n",
    "        # start = time.time()\n",
    "        # X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "        # durations = time.time() - start\n",
    "        # if not quiet:\n",
    "        #     print(f\"round {round}:\")\n",
    "        #     print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        if length_scaler is not None:\n",
    "            start = time.time()\n",
    "            X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "            X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tLength scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        \n",
    "        if selector is not None:\n",
    "            start = time.time()\n",
    "            X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "        durations = time.time() - start\n",
    "        if not quiet:\n",
    "            print(f'\\tModelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "        for method in scoring:\n",
    "            scores[method].append(result[method])\n",
    "        durations = time.time() - start\n",
    "        if not quiet:\n",
    "            print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s', end=\"\\n\\n\")\n",
    "\n",
    "        round_num += 1\n",
    "        \n",
    "    if avg_output:\n",
    "        avg_scores = {key: np.mean(values) for key, values in scores.items()}\n",
    "\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def multi_models_cross_validation(data, extractor, models=None, selector=None, length_scaler=None,\n",
    "#                                   scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5, quiet=True):\n",
    "#     kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "#     indices = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "#     output = {model: {score: [] for score in scoring} for model in models.keys()}\n",
    "#     round = 1\n",
    "\n",
    "#     for train_indices, test_indices in indices:\n",
    "#         if not quiet:\n",
    "#           print(f\"round {round}:\")\n",
    "\n",
    "#         train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "#         y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "#         start = time.time()\n",
    "#         X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         start = time.time()\n",
    "#         if length_scaler is not None:\n",
    "#             X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "#             X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tLength scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         start = time.time()\n",
    "#         if selector is not None:\n",
    "#             X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         for key, model in models.items():\n",
    "#             start = time.time()\n",
    "#             y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "#             durations = time.time() - start\n",
    "#             if not quiet:\n",
    "#                 print(f'\\t{key} - Modelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#             start = time.time()\n",
    "#             result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "#             for method in scoring:\n",
    "#                 output[key][method].append(result[method])\n",
    "#             durations = time.time() - start\n",
    "#             if not quiet:\n",
    "#                 print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "#         if not quiet:\n",
    "#             print()\n",
    "#         round += 1\n",
    "\n",
    "#     for model_name, model_scores in output.items():\n",
    "#         output[model_name] = {key: np.mean(values) for key, values in model_scores.items()}\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multi_cross_validation(data, extractor, models=None, selector=None, length_scalers=None,\n",
    "                           scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5, quiet=True,\n",
    "                           data_list=None):\n",
    "    kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    indices = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "    output = {model: {score: [] for score in scoring} for model in models.keys()}\n",
    "    output = {length: {model: {score: [] for score in scoring} for model in models.keys()} for length in length_scalers}\n",
    "    round_num = 1\n",
    "    fold = 0\n",
    "\n",
    "    for train_indices, test_indices in indices:\n",
    "        if not quiet:\n",
    "            print(f\"round {round_num}:\")\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if extractor is not None:\n",
    "            start = time.time()\n",
    "            text_train, text_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        else:\n",
    "            text_train, text_test = data_list[fold][0], data_list[fold][1]\n",
    "            fold += 1\n",
    "        \n",
    "        for length_scaler_name, length_scaler in length_scalers.items():\n",
    "            X_train, X_test = text_train, text_test\n",
    "            if length_scaler is not None:\n",
    "                start = time.time()\n",
    "                X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "                X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tLength scaled - {length_scaler_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "            if selector is not None:\n",
    "                start = time.time()\n",
    "                X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                start = time.time()\n",
    "                y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\t{model_name} - Modelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "                start = time.time()\n",
    "                result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "                for method in scoring:\n",
    "                    output[length_scaler_name][model_name][method].append(result[method])\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        if not quiet:\n",
    "            print()\n",
    "        round_num += 1\n",
    "\n",
    "    for length, result in output.items():\n",
    "        for model_name, model_scores in result.items():\n",
    "            output[length][model_name] = {key: np.mean(values) for key, values in model_scores.items()}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor = AvgWord2Vec(window=10, vector_size=300, seed=42, sg=1, ngram_range=(1, 1), epochs=15)\n",
    "# model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "# # model = SVC(probability=True, class_weight='balanced')\n",
    "# # extractor = TfidfVectorizer(min_df=0.001, ngram_range=(1, 1))\n",
    "# # model = SVC()\n",
    "\n",
    "# cross_validation(data=osf_cleaned, length_scaler=None,\n",
    "#                  model=model, extractor=extractor, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_test, extractor, length_scaler=None, selector=None, quiet=True):\n",
    "    start = time.time()\n",
    "    X_train_final, X_test_final = text_extractor(X_train=X_train['texts'], X_test=X_train['texts'],\n",
    "                                                 extractor=extractor)\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Text extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    start = time.time()\n",
    "    if length_scaler is not None:\n",
    "        X_train_final['length'] = length_scaler.fit_transform(X_train[['length']])\n",
    "        X_test_final['length'] = length_scaler.transform(X_test[['length']])\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Length scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    start = time.time()\n",
    "    if selector is not None:\n",
    "        X_train_final, X_test_final = feature_selection(X_train_final, X_test_final, selector)\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Dimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>love well made sturdi comfort love pretti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>love great upgrad origin mine coupl year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>pillow save back love look feel pillow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>miss inform use great product price</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>nice set good qualiti set two month</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>1694</td>\n",
       "      <td>read review say bra ran small order two band c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>1304</td>\n",
       "      <td>sure exactli would littl larg small size think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>1987</td>\n",
       "      <td>wear hood wear hood wear jacket without hood 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>1301</td>\n",
       "      <td>like noth dress reason gave 4 star order size ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>1768</td>\n",
       "      <td>work wed industri work long day foot outsid he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       length                                              texts  labels\n",
       "0          75          love well made sturdi comfort love pretti       1\n",
       "1          80           love great upgrad origin mine coupl year       1\n",
       "2          67             pillow save back love look feel pillow       1\n",
       "3          81                miss inform use great product price       1\n",
       "4          85                nice set good qualiti set two month       1\n",
       "...       ...                                                ...     ...\n",
       "40427    1694  read review say bra ran small order two band c...       0\n",
       "40428    1304  sure exactli would littl larg small size think...       1\n",
       "40429    1987  wear hood wear hood wear jacket without hood 3...       0\n",
       "40430    1301  like noth dress reason gave 4 star order size ...       1\n",
       "40431    1768  work wed industri work long day foot outsid he...       0\n",
       "\n",
       "[40432 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = osf_cleaned.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_file = False\n",
    "\n",
    "if saved_file:\n",
    "    kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    indices = kfolds.split(data)\n",
    "    fold = 1\n",
    "    for train_indices, test_indices in indices:\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "        text_train, text_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'],\n",
    "                                            extractor=AvgWord2Vec(vector_size=600, window=10, ngram_range=(1, 1), sg=1, epochs=15))\n",
    "        text_train.to_csv(f\"../../data/processed_skipgram/uni/fold{fold}/train.csv\")\n",
    "        text_test.to_csv(f\"../../data/processed_skipgram/uni/fold{fold}/test.csv\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>text_extraction</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.865651</td>\n",
       "      <td>0.866163</td>\n",
       "      <td>0.928591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.859022</td>\n",
       "      <td>0.860758</td>\n",
       "      <td>0.938256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.869880</td>\n",
       "      <td>0.871144</td>\n",
       "      <td>0.946182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>0.863540</td>\n",
       "      <td>0.941246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.802508</td>\n",
       "      <td>0.800856</td>\n",
       "      <td>0.883884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.543703</td>\n",
       "      <td>0.674749</td>\n",
       "      <td>0.600135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.505070</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.559175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        length_used  ngrams text_extraction               model  accuracy  \\\n",
       "63              NaN  (1, 1)        Word2Vec  LogisticRegression  0.865651   \n",
       "64              NaN  (1, 1)        Word2Vec            LightGBM  0.859022   \n",
       "65              NaN  (1, 1)        Word2Vec            Catboost  0.869880   \n",
       "66              NaN  (1, 1)        Word2Vec             XGBoost  0.862955   \n",
       "67              NaN  (1, 1)        Word2Vec            AdaBoost  0.802508   \n",
       "..              ...     ...             ...                 ...       ...   \n",
       "121  StandardScaler  (1, 3)        Word2Vec            Catboost  0.517659   \n",
       "122  StandardScaler  (1, 3)        Word2Vec             XGBoost  0.516695   \n",
       "123  StandardScaler  (1, 3)        Word2Vec            AdaBoost  0.500148   \n",
       "124  StandardScaler  (1, 3)        Word2Vec                 KNN  0.543703   \n",
       "125  StandardScaler  (1, 3)        Word2Vec                 SVM  0.505070   \n",
       "\n",
       "           f1   roc_auc  \n",
       "63   0.866163  0.928591  \n",
       "64   0.860758  0.938256  \n",
       "65   0.871144  0.946182  \n",
       "66   0.863540  0.941246  \n",
       "67   0.800856  0.883884  \n",
       "..        ...       ...  \n",
       "121  0.672257  0.477459  \n",
       "122  0.671254  0.509397  \n",
       "123  0.665530  0.550193  \n",
       "124  0.674749  0.600135  \n",
       "125  0.665777  0.559175  \n",
       "\n",
       "[63 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_file = pd.read_csv(r\"..\\..\\output\\csv\\w2v_model_results.csv\")\n",
    "new_file = saved_file.loc[(saved_file[\"w2v_type\"]==\"skipgram\")].drop(columns=[\"vector_size\", \"epochs\", \"precision\", \"recall\"]).rename(columns={\"w2v_type\": \"text_extraction\"})\n",
    "new_file[\"text_extraction\"] = [\"Word2Vec\"]*63\n",
    "new_file = new_file[[\"length_used\", \"ngrams\", \"text_extraction\", \"model\", \"accuracy\", \"f1\", \"roc_auc\"]]\n",
    "# new_file.reset_index(drop=True).to_csv(\"../../output/csv/w2v_results_final.csv\", index=False)\n",
    "new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 0m23s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m16s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LightGBM: Done in 1m17s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m11s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m2s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: Catboost: Done in 10m48s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m25s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m28s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m34s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m27s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m32s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: XGBoost: Done in 2m28s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 4m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 4m1s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 3m57s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m testcases[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     46\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 47\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mosf_cleaned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mlength_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# scores = multi_cross_validation(data=osf_cleaned,\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#                                 extractor=None,\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#                                 models=testcases['model'],\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m#                                 length_scalers=testcases['length_used'], quiet=False, data_list=data_list)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_used\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scaler_name)\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(data, extractor, model, selector, length_scaler, scoring, cv, avg_output, quiet, data_list)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDimensionality reduction: Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 56\u001b[0m y_pred, y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mmodelling\u001b[1;34m(model, X_train, y_train, X_test, probability)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodelling\u001b[39m(model, X_train, y_train, X_test, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probability:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:169\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    166\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:589\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:656\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 656\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': {(1, 1): \"uni\", (1, 2): \"unibi\", (1, 3): \"unibitri\"},\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "x = {'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "     'KNN': KNeighborsClassifier(),\n",
    "     'SVM': SVC(probability=True, max_iter=1000)}\n",
    "\n",
    "for n in [(1, 1)]:\n",
    "    # start = time.time()\n",
    "    data_list = []\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        train = pd.read_csv(f\"../../data/processed_skipgram/{testcases['ngrams'][n]}/fold{fold}/train.csv\", index_col=0)\n",
    "        test = pd.read_csv(f\"../../data/processed_skipgram/{testcases['ngrams'][n]}/fold{fold}/test.csv\", index_col=0)\n",
    "        data_list.append([train, test])\n",
    "    \n",
    "    for scaler_name, scaler in {'MinMaxScaler': MinMaxScaler()}.items():\n",
    "        for model_name, model in testcases[\"model\"].items():\n",
    "            start = time.time()\n",
    "            scores = cross_validation(data=osf_cleaned,\n",
    "                                      length_scaler=scaler,\n",
    "                                      model=model, quiet=False, data_list=data_list)\n",
    "            # scores = multi_cross_validation(data=osf_cleaned,\n",
    "            #                                 extractor=None,\n",
    "            #                                 models=testcases['model'],\n",
    "            #                                 length_scalers=testcases['length_used'], quiet=False, data_list=data_list)\n",
    "            output[\"length_used\"].append(scaler_name)\n",
    "            output['w2v_type'].append('skipgram')\n",
    "            output['vector_size'].append(600)\n",
    "            output['epochs'].append(15)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in scores.items():\n",
    "                output[method].append(score)\n",
    "            # for scaler_name, scaler_resuls in scores.items():\n",
    "            #     output['length_used'].append(scaler_name)\n",
    "            #     for model_name, model_scores in scaler_resuls.items():\n",
    "            #         output['w2v_type'].append('skipgram')\n",
    "            #         output['vector_size'].append(600)\n",
    "            #         output['epochs'].append(15)\n",
    "            #         output['ngrams'].append(n)\n",
    "            #         output['model'].append(model_name)\n",
    "            #         for method, score in model_scores.items():\n",
    "            #             output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df.to_csv(\"../../output/csv/w2v_model_results_sg_uni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>w2v_type</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.882276</td>\n",
       "      <td>0.895280</td>\n",
       "      <td>0.869782</td>\n",
       "      <td>0.940606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.869905</td>\n",
       "      <td>0.875007</td>\n",
       "      <td>0.910862</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.951442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.887070</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.904816</td>\n",
       "      <td>0.873968</td>\n",
       "      <td>0.957431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.866665</td>\n",
       "      <td>0.870922</td>\n",
       "      <td>0.899529</td>\n",
       "      <td>0.844371</td>\n",
       "      <td>0.946819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.818881</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.877120</td>\n",
       "      <td>0.785922</td>\n",
       "      <td>0.906793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.847794</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>0.835094</td>\n",
       "      <td>0.914293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.499851</td>\n",
       "      <td>0.666196</td>\n",
       "      <td>0.998284</td>\n",
       "      <td>0.499931</td>\n",
       "      <td>0.850376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.883137</td>\n",
       "      <td>0.883712</td>\n",
       "      <td>0.888535</td>\n",
       "      <td>0.879079</td>\n",
       "      <td>0.943551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.823804</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>0.944722</td>\n",
       "      <td>0.761323</td>\n",
       "      <td>0.946015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.856931</td>\n",
       "      <td>0.945169</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.830506</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>0.935710</td>\n",
       "      <td>0.773617</td>\n",
       "      <td>0.942627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.829631</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.780040</td>\n",
       "      <td>0.907543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.861199</td>\n",
       "      <td>0.859714</td>\n",
       "      <td>0.850763</td>\n",
       "      <td>0.868913</td>\n",
       "      <td>0.924831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.500297</td>\n",
       "      <td>0.666380</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>0.500150</td>\n",
       "      <td>0.840014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.884167</td>\n",
       "      <td>0.888289</td>\n",
       "      <td>0.880162</td>\n",
       "      <td>0.943573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.827761</td>\n",
       "      <td>0.845753</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.766520</td>\n",
       "      <td>0.947083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.856931</td>\n",
       "      <td>0.945169</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.830506</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>0.935710</td>\n",
       "      <td>0.773617</td>\n",
       "      <td>0.942627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.829631</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.780040</td>\n",
       "      <td>0.907543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>0.851702</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.904487</td>\n",
       "      <td>0.928885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.576943</td>\n",
       "      <td>0.699483</td>\n",
       "      <td>0.982196</td>\n",
       "      <td>0.543975</td>\n",
       "      <td>0.887883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499283</td>\n",
       "      <td>0.664933</td>\n",
       "      <td>0.993769</td>\n",
       "      <td>0.499638</td>\n",
       "      <td>0.550229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.524040</td>\n",
       "      <td>0.674650</td>\n",
       "      <td>0.985917</td>\n",
       "      <td>0.513078</td>\n",
       "      <td>0.817206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.533512</td>\n",
       "      <td>0.679166</td>\n",
       "      <td>0.985914</td>\n",
       "      <td>0.518448</td>\n",
       "      <td>0.853315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.530495</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.973736</td>\n",
       "      <td>0.516624</td>\n",
       "      <td>0.780557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.498788</td>\n",
       "      <td>0.664840</td>\n",
       "      <td>0.994326</td>\n",
       "      <td>0.499398</td>\n",
       "      <td>0.597742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.565690</td>\n",
       "      <td>0.687858</td>\n",
       "      <td>0.954497</td>\n",
       "      <td>0.538553</td>\n",
       "      <td>0.727047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.492827</td>\n",
       "      <td>0.659270</td>\n",
       "      <td>0.981470</td>\n",
       "      <td>0.496359</td>\n",
       "      <td>0.597131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.664921</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>0.499650</td>\n",
       "      <td>0.546067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.517412</td>\n",
       "      <td>0.671849</td>\n",
       "      <td>0.988150</td>\n",
       "      <td>0.508967</td>\n",
       "      <td>0.522746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.989475</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.986952</td>\n",
       "      <td>0.508602</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.994711</td>\n",
       "      <td>0.500077</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.560916</td>\n",
       "      <td>0.684653</td>\n",
       "      <td>0.951332</td>\n",
       "      <td>0.535519</td>\n",
       "      <td>0.707975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.490527</td>\n",
       "      <td>0.656725</td>\n",
       "      <td>0.974961</td>\n",
       "      <td>0.495144</td>\n",
       "      <td>0.585203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.664921</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>0.499650</td>\n",
       "      <td>0.543472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.516645</td>\n",
       "      <td>0.671641</td>\n",
       "      <td>0.988787</td>\n",
       "      <td>0.508559</td>\n",
       "      <td>0.504940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.989475</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.986952</td>\n",
       "      <td>0.508602</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.994711</td>\n",
       "      <td>0.500077</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.543703</td>\n",
       "      <td>0.674749</td>\n",
       "      <td>0.946426</td>\n",
       "      <td>0.524456</td>\n",
       "      <td>0.600135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.505070</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.986024</td>\n",
       "      <td>0.502601</td>\n",
       "      <td>0.559175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_used  w2v_type  vector_size  epochs  ngrams               model  \\\n",
       "0              NaN  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "1              NaN  skipgram          600      15  (1, 2)            LightGBM   \n",
       "2              NaN  skipgram          600      15  (1, 2)            Catboost   \n",
       "3              NaN  skipgram          600      15  (1, 2)             XGBoost   \n",
       "4              NaN  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "5              NaN  skipgram          600      15  (1, 2)                 KNN   \n",
       "6              NaN  skipgram          600      15  (1, 2)                 SVM   \n",
       "7     MinMaxScaler  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "8     MinMaxScaler  skipgram          600      15  (1, 2)            LightGBM   \n",
       "9     MinMaxScaler  skipgram          600      15  (1, 2)            Catboost   \n",
       "10    MinMaxScaler  skipgram          600      15  (1, 2)             XGBoost   \n",
       "11    MinMaxScaler  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "12    MinMaxScaler  skipgram          600      15  (1, 2)                 KNN   \n",
       "13    MinMaxScaler  skipgram          600      15  (1, 2)                 SVM   \n",
       "14  StandardScaler  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "15  StandardScaler  skipgram          600      15  (1, 2)            LightGBM   \n",
       "16  StandardScaler  skipgram          600      15  (1, 2)            Catboost   \n",
       "17  StandardScaler  skipgram          600      15  (1, 2)             XGBoost   \n",
       "18  StandardScaler  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "19  StandardScaler  skipgram          600      15  (1, 2)                 KNN   \n",
       "20  StandardScaler  skipgram          600      15  (1, 2)                 SVM   \n",
       "21             NaN  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "22             NaN  skipgram          600      15  (1, 3)            LightGBM   \n",
       "23             NaN  skipgram          600      15  (1, 3)            Catboost   \n",
       "24             NaN  skipgram          600      15  (1, 3)             XGBoost   \n",
       "25             NaN  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "26             NaN  skipgram          600      15  (1, 3)                 KNN   \n",
       "27             NaN  skipgram          600      15  (1, 3)                 SVM   \n",
       "28    MinMaxScaler  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "29    MinMaxScaler  skipgram          600      15  (1, 3)            LightGBM   \n",
       "30    MinMaxScaler  skipgram          600      15  (1, 3)            Catboost   \n",
       "31    MinMaxScaler  skipgram          600      15  (1, 3)             XGBoost   \n",
       "32    MinMaxScaler  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "33    MinMaxScaler  skipgram          600      15  (1, 3)                 KNN   \n",
       "34    MinMaxScaler  skipgram          600      15  (1, 3)                 SVM   \n",
       "35  StandardScaler  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "36  StandardScaler  skipgram          600      15  (1, 3)            LightGBM   \n",
       "37  StandardScaler  skipgram          600      15  (1, 3)            Catboost   \n",
       "38  StandardScaler  skipgram          600      15  (1, 3)             XGBoost   \n",
       "39  StandardScaler  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "40  StandardScaler  skipgram          600      15  (1, 3)                 KNN   \n",
       "41  StandardScaler  skipgram          600      15  (1, 3)                 SVM   \n",
       "\n",
       "    accuracy        f1    recall  precision   roc_auc  \n",
       "0   0.880590  0.882276  0.895280   0.869782  0.940606  \n",
       "1   0.869905  0.875007  0.910862   0.842000  0.951442  \n",
       "2   0.887070  0.889021  0.904816   0.873968  0.957431  \n",
       "3   0.866665  0.870922  0.899529   0.844371  0.946819  \n",
       "4   0.818881  0.828889  0.877120   0.785922  0.906793  \n",
       "5   0.847794  0.850615  0.866806   0.835094  0.914293  \n",
       "6   0.499851  0.666196  0.998284   0.499931  0.850376  \n",
       "7   0.883137  0.883712  0.888535   0.879079  0.943551  \n",
       "8   0.823804  0.843013  0.944722   0.761323  0.946015  \n",
       "9   0.842130  0.856931  0.945169   0.783927  0.952500  \n",
       "10  0.830506  0.846812  0.935710   0.773617  0.942627  \n",
       "11  0.817916  0.829631  0.886268   0.780040  0.907543  \n",
       "12  0.861199  0.859714  0.850763   0.868913  0.924831  \n",
       "13  0.500297  0.666380  0.998227   0.500150  0.840014  \n",
       "14  0.883681  0.884167  0.888289   0.880162  0.943573  \n",
       "15  0.827761  0.845753  0.943536   0.766520  0.947083  \n",
       "16  0.842130  0.856931  0.945169   0.783927  0.952500  \n",
       "17  0.830506  0.846812  0.935710   0.773617  0.942627  \n",
       "18  0.817916  0.829631  0.886268   0.780040  0.907543  \n",
       "19  0.859863  0.851702  0.804766   0.904487  0.928885  \n",
       "20  0.576943  0.699483  0.982196   0.543975  0.887883  \n",
       "21  0.499283  0.664933  0.993769   0.499638  0.550229  \n",
       "22  0.524040  0.674650  0.985917   0.513078  0.817206  \n",
       "23  0.533512  0.679166  0.985914   0.518448  0.853315  \n",
       "24  0.530495  0.674827  0.973736   0.516624  0.780557  \n",
       "25  0.498788  0.664840  0.994326   0.499398  0.597742  \n",
       "26  0.565690  0.687858  0.954497   0.538553  0.727047  \n",
       "27  0.492827  0.659270  0.981470   0.496359  0.597131  \n",
       "28  0.499307  0.664921  0.993669   0.499650  0.546067  \n",
       "29  0.517412  0.671849  0.988150   0.508967  0.522746  \n",
       "30  0.517659  0.672257  0.989475   0.509083  0.477459  \n",
       "31  0.516695  0.671254  0.986952   0.508602  0.509397  \n",
       "32  0.500148  0.665530  0.994711   0.500077  0.550193  \n",
       "33  0.560916  0.684653  0.951332   0.535519  0.707975  \n",
       "34  0.490527  0.656725  0.974961   0.495144  0.585203  \n",
       "35  0.499307  0.664921  0.993669   0.499650  0.543472  \n",
       "36  0.516645  0.671641  0.988787   0.508559  0.504940  \n",
       "37  0.517659  0.672257  0.989475   0.509083  0.477459  \n",
       "38  0.516695  0.671254  0.986952   0.508602  0.509397  \n",
       "39  0.500148  0.665530  0.994711   0.500077  0.550193  \n",
       "40  0.543703  0.674749  0.946426   0.524456  0.600135  \n",
       "41  0.505070  0.665777  0.986024   0.502601  0.559175  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df\n",
    "# saved_file = pd.read_csv(\"../../output/csv/w2v_model_results.csv\")\n",
    "# saved_file\n",
    "# new_df = pd.concat([saved_file, output_df], axis=0, ignore_index=True)\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>w2v_type</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860457</td>\n",
       "      <td>0.861401</td>\n",
       "      <td>0.867456</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.923285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.861001</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.882299</td>\n",
       "      <td>0.846196</td>\n",
       "      <td>0.939263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.869732</td>\n",
       "      <td>0.871865</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.857626</td>\n",
       "      <td>0.944711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.863375</td>\n",
       "      <td>0.864967</td>\n",
       "      <td>0.875369</td>\n",
       "      <td>0.854818</td>\n",
       "      <td>0.941157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.822641</td>\n",
       "      <td>0.822810</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>0.821935</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.828502</td>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.836860</td>\n",
       "      <td>0.823118</td>\n",
       "      <td>0.889266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.696355</td>\n",
       "      <td>0.720611</td>\n",
       "      <td>0.785233</td>\n",
       "      <td>0.670095</td>\n",
       "      <td>0.795997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860902</td>\n",
       "      <td>0.861681</td>\n",
       "      <td>0.866714</td>\n",
       "      <td>0.856712</td>\n",
       "      <td>0.923469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.884151</td>\n",
       "      <td>0.885006</td>\n",
       "      <td>0.891749</td>\n",
       "      <td>0.878371</td>\n",
       "      <td>0.957102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.892635</td>\n",
       "      <td>0.892974</td>\n",
       "      <td>0.896006</td>\n",
       "      <td>0.889983</td>\n",
       "      <td>0.961555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.886171</td>\n",
       "      <td>0.886559</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.958313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.841141</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.840059</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>0.916428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.828923</td>\n",
       "      <td>0.830180</td>\n",
       "      <td>0.836367</td>\n",
       "      <td>0.824116</td>\n",
       "      <td>0.889746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.695241</td>\n",
       "      <td>0.733775</td>\n",
       "      <td>0.837723</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>0.817718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860902</td>\n",
       "      <td>0.861668</td>\n",
       "      <td>0.866616</td>\n",
       "      <td>0.856782</td>\n",
       "      <td>0.923470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.885338</td>\n",
       "      <td>0.886294</td>\n",
       "      <td>0.893867</td>\n",
       "      <td>0.878857</td>\n",
       "      <td>0.957507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.892635</td>\n",
       "      <td>0.892974</td>\n",
       "      <td>0.896006</td>\n",
       "      <td>0.889983</td>\n",
       "      <td>0.961555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.886171</td>\n",
       "      <td>0.886559</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.958313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.841141</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.840059</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>0.916428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.839433</td>\n",
       "      <td>0.827104</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.905929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.773844</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>0.837620</td>\n",
       "      <td>0.746080</td>\n",
       "      <td>0.875583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.867184</td>\n",
       "      <td>0.867731</td>\n",
       "      <td>0.871618</td>\n",
       "      <td>0.863908</td>\n",
       "      <td>0.928794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.858924</td>\n",
       "      <td>0.865220</td>\n",
       "      <td>0.905936</td>\n",
       "      <td>0.828028</td>\n",
       "      <td>0.942348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.867506</td>\n",
       "      <td>0.872440</td>\n",
       "      <td>0.906487</td>\n",
       "      <td>0.840893</td>\n",
       "      <td>0.947507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.861817</td>\n",
       "      <td>0.866893</td>\n",
       "      <td>0.900266</td>\n",
       "      <td>0.835933</td>\n",
       "      <td>0.942467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.810422</td>\n",
       "      <td>0.818943</td>\n",
       "      <td>0.857320</td>\n",
       "      <td>0.784520</td>\n",
       "      <td>0.896601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.814652</td>\n",
       "      <td>0.825759</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>0.778975</td>\n",
       "      <td>0.888866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506975</td>\n",
       "      <td>0.667402</td>\n",
       "      <td>0.989181</td>\n",
       "      <td>0.503705</td>\n",
       "      <td>0.748289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.866640</td>\n",
       "      <td>0.867173</td>\n",
       "      <td>0.870966</td>\n",
       "      <td>0.863433</td>\n",
       "      <td>0.929053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.861545</td>\n",
       "      <td>0.869756</td>\n",
       "      <td>0.924818</td>\n",
       "      <td>0.820915</td>\n",
       "      <td>0.950871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.834681</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.865577</td>\n",
       "      <td>0.872665</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.828917</td>\n",
       "      <td>0.951320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.814256</td>\n",
       "      <td>0.823598</td>\n",
       "      <td>0.867254</td>\n",
       "      <td>0.784603</td>\n",
       "      <td>0.901865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.816136</td>\n",
       "      <td>0.826791</td>\n",
       "      <td>0.877822</td>\n",
       "      <td>0.781396</td>\n",
       "      <td>0.890316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.509473</td>\n",
       "      <td>0.668106</td>\n",
       "      <td>0.987426</td>\n",
       "      <td>0.504911</td>\n",
       "      <td>0.786280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.866566</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.870817</td>\n",
       "      <td>0.863413</td>\n",
       "      <td>0.929054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.862559</td>\n",
       "      <td>0.870660</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.822090</td>\n",
       "      <td>0.950693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.834681</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.865577</td>\n",
       "      <td>0.872665</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.828917</td>\n",
       "      <td>0.951320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.814256</td>\n",
       "      <td>0.823598</td>\n",
       "      <td>0.867254</td>\n",
       "      <td>0.784603</td>\n",
       "      <td>0.901865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.834809</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.854452</td>\n",
       "      <td>0.822196</td>\n",
       "      <td>0.906746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.583845</td>\n",
       "      <td>0.692302</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>0.549998</td>\n",
       "      <td>0.816544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.839268</td>\n",
       "      <td>0.916574</td>\n",
       "      <td>0.774026</td>\n",
       "      <td>0.914417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.802433</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.926459</td>\n",
       "      <td>0.742356</td>\n",
       "      <td>0.921827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.816606</td>\n",
       "      <td>0.834611</td>\n",
       "      <td>0.925580</td>\n",
       "      <td>0.760036</td>\n",
       "      <td>0.927827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.791353</td>\n",
       "      <td>0.814465</td>\n",
       "      <td>0.915673</td>\n",
       "      <td>0.733607</td>\n",
       "      <td>0.908968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.748787</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.900095</td>\n",
       "      <td>0.693407</td>\n",
       "      <td>0.871499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.773175</td>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.916893</td>\n",
       "      <td>0.712178</td>\n",
       "      <td>0.878652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.666947</td>\n",
       "      <td>0.958654</td>\n",
       "      <td>0.511862</td>\n",
       "      <td>0.767138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.845160</td>\n",
       "      <td>0.913563</td>\n",
       "      <td>0.786331</td>\n",
       "      <td>0.916070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_used w2v_type  vector_size  epochs  ngrams               model  \\\n",
       "0              NaN     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "1              NaN     cbow          300      30  (1, 1)            LightGBM   \n",
       "2              NaN     cbow          300      30  (1, 1)            Catboost   \n",
       "3              NaN     cbow          300      30  (1, 1)             XGBoost   \n",
       "4              NaN     cbow          300      30  (1, 1)            AdaBoost   \n",
       "5              NaN     cbow          300      30  (1, 1)                 KNN   \n",
       "6              NaN     cbow          300      30  (1, 1)                 SVM   \n",
       "7     MinMaxScaler     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "8     MinMaxScaler     cbow          300      30  (1, 1)            LightGBM   \n",
       "9     MinMaxScaler     cbow          300      30  (1, 1)            Catboost   \n",
       "10    MinMaxScaler     cbow          300      30  (1, 1)             XGBoost   \n",
       "11    MinMaxScaler     cbow          300      30  (1, 1)            AdaBoost   \n",
       "12    MinMaxScaler     cbow          300      30  (1, 1)                 KNN   \n",
       "13    MinMaxScaler     cbow          300      30  (1, 1)                 SVM   \n",
       "14  StandardScaler     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "15  StandardScaler     cbow          300      30  (1, 1)            LightGBM   \n",
       "16  StandardScaler     cbow          300      30  (1, 1)            Catboost   \n",
       "17  StandardScaler     cbow          300      30  (1, 1)             XGBoost   \n",
       "18  StandardScaler     cbow          300      30  (1, 1)            AdaBoost   \n",
       "19  StandardScaler     cbow          300      30  (1, 1)                 KNN   \n",
       "20  StandardScaler     cbow          300      30  (1, 1)                 SVM   \n",
       "21             NaN     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "22             NaN     cbow          300      30  (1, 2)            LightGBM   \n",
       "23             NaN     cbow          300      30  (1, 2)            Catboost   \n",
       "24             NaN     cbow          300      30  (1, 2)             XGBoost   \n",
       "25             NaN     cbow          300      30  (1, 2)            AdaBoost   \n",
       "26             NaN     cbow          300      30  (1, 2)                 KNN   \n",
       "27             NaN     cbow          300      30  (1, 2)                 SVM   \n",
       "28    MinMaxScaler     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "29    MinMaxScaler     cbow          300      30  (1, 2)            LightGBM   \n",
       "30    MinMaxScaler     cbow          300      30  (1, 2)            Catboost   \n",
       "31    MinMaxScaler     cbow          300      30  (1, 2)             XGBoost   \n",
       "32    MinMaxScaler     cbow          300      30  (1, 2)            AdaBoost   \n",
       "33    MinMaxScaler     cbow          300      30  (1, 2)                 KNN   \n",
       "34    MinMaxScaler     cbow          300      30  (1, 2)                 SVM   \n",
       "35  StandardScaler     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "36  StandardScaler     cbow          300      30  (1, 2)            LightGBM   \n",
       "37  StandardScaler     cbow          300      30  (1, 2)            Catboost   \n",
       "38  StandardScaler     cbow          300      30  (1, 2)             XGBoost   \n",
       "39  StandardScaler     cbow          300      30  (1, 2)            AdaBoost   \n",
       "40  StandardScaler     cbow          300      30  (1, 2)                 KNN   \n",
       "41  StandardScaler     cbow          300      30  (1, 2)                 SVM   \n",
       "42             NaN     cbow          300      30  (1, 3)  LogisticRegression   \n",
       "43             NaN     cbow          300      30  (1, 3)            LightGBM   \n",
       "44             NaN     cbow          300      30  (1, 3)            Catboost   \n",
       "45             NaN     cbow          300      30  (1, 3)             XGBoost   \n",
       "46             NaN     cbow          300      30  (1, 3)            AdaBoost   \n",
       "47             NaN     cbow          300      30  (1, 3)                 KNN   \n",
       "48             NaN     cbow          300      30  (1, 3)                 SVM   \n",
       "49    MinMaxScaler     cbow          300      30  (1, 3)  LogisticRegression   \n",
       "\n",
       "    accuracy        f1    recall  precision   roc_auc  \n",
       "0   0.860457  0.861401  0.867456   0.855434  0.923285  \n",
       "1   0.861001  0.863863  0.882299   0.846196  0.939263  \n",
       "2   0.869732  0.871865  0.886598   0.857626  0.944711  \n",
       "3   0.863375  0.864967  0.875369   0.854818  0.941157  \n",
       "4   0.822641  0.822810  0.823706   0.821935  0.901408  \n",
       "5   0.828502  0.829918  0.836860   0.823118  0.889266  \n",
       "6   0.696355  0.720611  0.785233   0.670095  0.795997  \n",
       "7   0.860902  0.861681  0.866714   0.856712  0.923469  \n",
       "8   0.884151  0.885006  0.891749   0.878371  0.957102  \n",
       "9   0.892635  0.892974  0.896006   0.889983  0.961555  \n",
       "10  0.886130  0.886171  0.886559   0.885806  0.958313  \n",
       "11  0.841141  0.840971  0.840059   0.841892  0.916428  \n",
       "12  0.828923  0.830180  0.836367   0.824116  0.889746  \n",
       "13  0.695241  0.733775  0.837723   0.654123  0.817718  \n",
       "14  0.860902  0.861668  0.866616   0.856782  0.923470  \n",
       "15  0.885338  0.886294  0.893867   0.878857  0.957507  \n",
       "16  0.892635  0.892974  0.896006   0.889983  0.961555  \n",
       "17  0.886130  0.886171  0.886559   0.885806  0.958313  \n",
       "18  0.841141  0.840971  0.840059   0.841892  0.916428  \n",
       "19  0.841808  0.839433  0.827104   0.852174  0.905929  \n",
       "20  0.773844  0.787692  0.837620   0.746080  0.875583  \n",
       "21  0.867184  0.867731  0.871618   0.863908  0.928794  \n",
       "22  0.858924  0.865220  0.905936   0.828028  0.942348  \n",
       "23  0.867506  0.872440  0.906487   0.840893  0.947507  \n",
       "24  0.861817  0.866893  0.900266   0.835933  0.942467  \n",
       "25  0.810422  0.818943  0.857320   0.784520  0.896601  \n",
       "26  0.814652  0.825759  0.878564   0.778975  0.888866  \n",
       "27  0.506975  0.667402  0.989181   0.503705  0.748289  \n",
       "28  0.866640  0.867173  0.870966   0.863433  0.929053  \n",
       "29  0.861545  0.869756  0.924818   0.820915  0.950871  \n",
       "30  0.872774  0.879610  0.929672   0.834681  0.956661  \n",
       "31  0.865577  0.872665  0.921300   0.828917  0.951320  \n",
       "32  0.814256  0.823598  0.867254   0.784603  0.901865  \n",
       "33  0.816136  0.826791  0.877822   0.781396  0.890316  \n",
       "34  0.509473  0.668106  0.987426   0.504911  0.786280  \n",
       "35  0.866566  0.867089  0.870817   0.863413  0.929054  \n",
       "36  0.862559  0.870660  0.925374   0.822090  0.950693  \n",
       "37  0.872774  0.879610  0.929672   0.834681  0.956661  \n",
       "38  0.865577  0.872665  0.921300   0.828917  0.951320  \n",
       "39  0.814256  0.823598  0.867254   0.784603  0.901865  \n",
       "40  0.834809  0.837975  0.854452   0.822196  0.906746  \n",
       "41  0.583845  0.692302  0.935614   0.549998  0.816544  \n",
       "42  0.824471  0.839268  0.916574   0.774026  0.914417  \n",
       "43  0.802433  0.824200  0.926459   0.742356  0.921827  \n",
       "44  0.816606  0.834611  0.925580   0.760036  0.927827  \n",
       "45  0.791353  0.814465  0.915673   0.733607  0.908968  \n",
       "46  0.748787  0.782432  0.900095   0.693407  0.871499  \n",
       "47  0.773175  0.801650  0.916893   0.712178  0.878652  \n",
       "48  0.520973  0.666947  0.958654   0.511862  0.767138  \n",
       "49  0.832633  0.845160  0.913563   0.786331  0.916070  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"C:\\Users\\minhn\\Downloads\\w2v_model_kaggle (3).csv\"\n",
    "# \"C:\\Users\\minhn\\Downloads\\w2v_model_ggcolab_2 (1).csv\"\n",
    "outsources_df = pd.read_csv(r\"C:\\Users\\minhn\\Downloads\\w2v_model_ggcolab_2 (1).csv\")\n",
    "outsources_df\n",
    "saved_file = pd.read_csv(\"../../output/csv/w2v_model_results.csv\")\n",
    "saved_file\n",
    "new_df = pd.concat([outsources_df, saved_file], axis=0, ignore_index=True)\n",
    "new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"../../output/csv/w2v_model_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output_df.to_csv(\"../../output/csv/w2v_model_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     34\u001b[0m extractor \u001b[38;5;241m=\u001b[39m AvgWord2Vec(vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39mn, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mosf_cleaned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestcases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlength_scalers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestcases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength_used\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_name, scaler_resuls \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     40\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_used\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scaler_name)\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mmulti_cross_validation\u001b[1;34m(data, extractor, models, selector, length_scalers, scoring, cv, quiet)\u001b[0m\n\u001b[0;32m     16\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[train_indices, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m.\u001b[39miloc[test_indices, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 19\u001b[0m text_train, text_test \u001b[38;5;241m=\u001b[39m \u001b[43mtext_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtext_extractor\u001b[1;34m(X_train, X_test, extractor)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_extractor\u001b[39m(X_train, X_test, extractor):\n\u001b[1;32m---> 12\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     13\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mtransform(X_test)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     14\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train, columns\u001b[38;5;241m=\u001b[39mextractor\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "Cell \u001b[1;32mIn[14], line 103\u001b[0m, in \u001b[0;36mAvgWord2Vec.fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuild vocab: Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    102\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquiet:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'GradientBoost': GradientBoostingClassifier(random_state=42),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "for n in [(1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = AvgWord2Vec(vector_size=600, window=10, ngram_range=n, sg=1, epochs=15)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=testcases['model'],\n",
    "                                    length_scalers=testcases['length_used'], quiet=False)\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        output['length_used'].append(scaler_name)\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['w2v_type'].append('skipgram')\n",
    "            output['vector_size'].append(600)\n",
    "            output['epochs'].append(15)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tText extraction: Done in 4m14s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 1m26s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m33s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m21s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m40s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m27s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m35s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - StandardScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m29s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m37s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 3m51s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m59s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m28s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m19s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m31s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m24s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m38s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - StandardScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m57s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m36s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 3m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 4m11s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 1m10s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "for n in [(1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = AvgWord2Vec(vector_size=300, window=10, ngram_range=n, sg=0, epochs=30)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=testcases['model'],\n",
    "                                    length_scalers=testcases['length_used'], quiet=False)\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        output['length_used'].append(scaler_name)\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['w2v_type'].append('cbow')\n",
    "            output['vector_size'].append(300)\n",
    "            output['epochs'].append(30)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - cbow - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m10s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 1m19s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 1m19s\n",
      "\n",
      "round 1:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m4s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m11s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 2) - model: LogisticRegression: Done in 2m4s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 2) - model: LogisticRegression: Done in 2m4s\n",
      "\n",
      "round 1:\n",
      "\tText extraction: Done in 0m4s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m7s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m5s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m20s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m6s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m16s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m8s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m19s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 3) - model: LogisticRegression: Done in 2m39s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 3) - model: LogisticRegression: Done in 2m39s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'text_extraction': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "                       'KNN': KNeighborsClassifier()}}\n",
    "model = {\"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)}\n",
    "\n",
    "for n in [(1, 1), (1, 2), (1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = TfidfVectorizer(ngram_range=n, min_df=0.001)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=model,\n",
    "                                    length_scalers={\"None\": None,\n",
    "                                                    \"MinMaxScaler\": MinMaxScaler()}, quiet=False,\n",
    "                                    scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['length_used'].append(scaler_name)\n",
    "            output['text_extraction'].append('Hashing')\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df[\"text_extraction\"] = [\"tfidf\"]*6\n",
    "# output_df.to_csv(\"../../output/csv/tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
