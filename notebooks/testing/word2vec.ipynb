{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local import\n",
    "import sys\n",
    "if \"../../\" not in sys.path:\n",
    "    sys.path.append(\"../../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "# for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# for preprocessing\n",
    "from src.preprocessing import BasicTextCleaning\n",
    "from src.exploration import check_balance, distribution_barplot, distribution_otherbased\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk import ngrams\n",
    "\n",
    "# for modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# for evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# for timing\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of OFS:  (40432, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>rating</th>\n",
       "      <th>label</th>\n",
       "      <th>text_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Love this!  Well made, sturdy, and very comfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>love it, a great upgrade from the original.  I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>This pillow saved my back. I love the look and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Missing information on how to use it, but it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Home_and_Kitchen_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>Very nice set. Good quality. We have had the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I had read some reviews saying that this bra r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I wasn't sure exactly what it would be. It is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>You can wear the hood by itself, wear it with ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CG</td>\n",
       "      <td>I liked nothing about this dress. The only rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>Clothing_Shoes_and_Jewelry_5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OR</td>\n",
       "      <td>I work in the wedding industry and have to wor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           category  rating label  \\\n",
       "0                Home_and_Kitchen_5     5.0    CG   \n",
       "1                Home_and_Kitchen_5     5.0    CG   \n",
       "2                Home_and_Kitchen_5     5.0    CG   \n",
       "3                Home_and_Kitchen_5     1.0    CG   \n",
       "4                Home_and_Kitchen_5     5.0    CG   \n",
       "...                             ...     ...   ...   \n",
       "40427  Clothing_Shoes_and_Jewelry_5     4.0    OR   \n",
       "40428  Clothing_Shoes_and_Jewelry_5     5.0    CG   \n",
       "40429  Clothing_Shoes_and_Jewelry_5     2.0    OR   \n",
       "40430  Clothing_Shoes_and_Jewelry_5     1.0    CG   \n",
       "40431  Clothing_Shoes_and_Jewelry_5     5.0    OR   \n",
       "\n",
       "                                                   text_  \n",
       "0      Love this!  Well made, sturdy, and very comfor...  \n",
       "1      love it, a great upgrade from the original.  I...  \n",
       "2      This pillow saved my back. I love the look and...  \n",
       "3      Missing information on how to use it, but it i...  \n",
       "4      Very nice set. Good quality. We have had the s...  \n",
       "...                                                  ...  \n",
       "40427  I had read some reviews saying that this bra r...  \n",
       "40428  I wasn't sure exactly what it would be. It is ...  \n",
       "40429  You can wear the hood by itself, wear it with ...  \n",
       "40430  I liked nothing about this dress. The only rea...  \n",
       "40431  I work in the wedding industry and have to wor...  \n",
       "\n",
       "[40432 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "osf = pd.read_csv(\"../../data/fake_reviews_dataset.csv\")\n",
    "print(\"Shape of OFS: \", osf.shape)\n",
    "# osf.head()\n",
    "osf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_balance(data=osf, target=\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 4))\n",
    "distribution_barplot(data=osf, feature=\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "distribution_otherbased(data=osf, dist_feature=\"label\", base_feature=\"rating\", normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = BasicTextCleaning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    osf_cleaned = pd.read_csv(\"../../data/cleaned/osf_cleaned.csv\")\n",
    "    osf_cleaned = osf_cleaned.replace(np.nan, '')\n",
    "except:\n",
    "    osf_cleaned = pd.DataFrame()\n",
    "    osf_cleaned['length'] = osf['text_'].apply(lambda x: len(x))\n",
    "    osf_cleaned['texts'] = cleaner.text_cleaning(osf['text_'])\n",
    "\n",
    "    ordinal = OrdinalEncoder(categories=[['OR', 'CG']], dtype=int)\n",
    "    osf_cleaned['labels'] = ordinal.fit_transform(osf[['label']])\n",
    "    osf_cleaned.to_csv(\"../../data/cleaned/osf_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "class AvgWord2Vec:\n",
    "    def __init__(self, vector_size=300, min_count=1, sg=1, ngram_range=(1, 1), window=5, epochs=5, seed=42,\n",
    "                 quiet=True):\n",
    "        self.w2v = Word2Vec(vector_size=vector_size, min_count=min_count, sg=sg,\n",
    "                            window=window, workers=4, seed=seed, epochs=epochs)\n",
    "        self.min_count = min_count\n",
    "        self.sg = sg\n",
    "        self.window = window\n",
    "        self.seed = seed\n",
    "        self.vsize = vector_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "        self.ngrams = np.arange(ngram_range[0], ngram_range[1]+1, 1)\n",
    "        self.raw = None\n",
    "        self.corpus = None\n",
    "        self.vocabulary_ = None\n",
    "        self.quiet = quiet\n",
    "\n",
    "    def _create_ngrams(self, n, X):\n",
    "        phrases = []\n",
    "        for sent in X:\n",
    "            words = sent.split()\n",
    "            if len(words) >= n:  # Check if words list is not empty\n",
    "                ngram_list = list(ngrams(words, n))\n",
    "                phrases.append([\" \".join(word) for word in ngram_list])\n",
    "            else:\n",
    "                phrases.append([])\n",
    "        return phrases\n",
    "    \n",
    "    def _create_corpus(self, X, update_train=False):\n",
    "        ngrams_phrases = {}\n",
    "        for n in self.ngrams:\n",
    "            phrases = self._create_ngrams(n, X)\n",
    "            ngrams_phrases[f\"{n}\"] = phrases\n",
    "        data = []\n",
    "        corpus = []\n",
    "        for n in ngrams_phrases.values():\n",
    "            if len(data)==0:\n",
    "                data = n\n",
    "            data = [data[i] + n[i] for i in range(len(data))]\n",
    "            corpus.extend(n)\n",
    "        if update_train:\n",
    "            self.corpus = corpus\n",
    "        return data, corpus\n",
    "    \n",
    "    def _avg_sentence(self, sentences):\n",
    "        w2v_model = self.w2v\n",
    "        avg_sentences = []\n",
    "        for sentence in sentences:\n",
    "            if len(sentence)!=0:\n",
    "                avg_sentence = np.mean([w2v_model.wv.get_vector(word) for word in sentence\n",
    "                                        if word in w2v_model.wv.key_to_index], axis=0)\n",
    "            else:\n",
    "                avg_sentence = np.zeros(w2v_model.vector_size)\n",
    "            avg_sentences.append(avg_sentence)\n",
    "        return np.array(avg_sentences)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=4, seed=self.seed, epochs=self.epochs)\n",
    "        self.raw = list(X)\n",
    "        \n",
    "        start = time.time()\n",
    "        corpus = self._create_corpus(update_train=True, X=X)[1]\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        \n",
    "        start = time.time()\n",
    "        self.w2v.build_vocab(corpus)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Build vocab: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.train(corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Training : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        self.vocabulary_ = self.w2v.wv.key_to_index\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        self.w2v = Word2Vec(vector_size=self.vsize, min_count=self.min_count, sg=self.sg,\n",
    "                            window=self.window, workers=4, seed=self.seed, epochs=self.epochs)\n",
    "        self.raw = list(X)\n",
    "\n",
    "        start = time.time()\n",
    "        data, corpus = self._create_corpus(update_train=True, X=X)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.build_vocab(corpus)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Build vocab: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        self.w2v.train(corpus, total_examples=self.w2v.corpus_count, epochs=self.w2v.epochs)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Training : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents = self._avg_sentence(data)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Average : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents_sprs = scipy.sparse.csr_matrix(avg_sents)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Sparse : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        return avg_sents_sprs\n",
    "        \n",
    "    def transform(self, X):\n",
    "        start = time.time()\n",
    "        data = self._create_corpus(update_train=False, X=X)[0]\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Create corpus: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents = self._avg_sentence(data)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Average : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        avg_sents_sprs = scipy.sparse.csr_matrix(avg_sents)\n",
    "        durations = time.time() - start\n",
    "        if not self.quiet:\n",
    "            print(f'Sparse : Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        return avg_sents_sprs\n",
    "    \n",
    "    def get_feature_names_out(self):\n",
    "        columns = np.array([f'component_{i+1}' for i in range(self.vsize)])\n",
    "        return columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word2vec(sentences, w2v_model):\n",
    "    avg_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence:\n",
    "            avg_sentence = np.mean([w2v_model.wv.get_vector(word) for word in sentence if word in w2v_model.wv.key_to_index], axis=0)\n",
    "        else:\n",
    "            avg_sentence = np.zeros(w2v_model.vector_size)\n",
    "        avg_sentences.append(avg_sentence)\n",
    "    return np.array(avg_sentences)\n",
    "\n",
    "def text_extractor(X_train, X_test, extractor):\n",
    "    X_train = extractor.fit_transform(X_train).toarray()\n",
    "    X_test = extractor.transform(X_test).toarray()\n",
    "    try:\n",
    "        X_train = pd.DataFrame(X_train, columns=extractor.get_feature_names_out())\n",
    "        X_test = pd.DataFrame(X_test, columns=extractor.get_feature_names_out())\n",
    "    except:\n",
    "        X_train = pd.DataFrame(X_train, columns=[f\"component_{i+1}\" for i in range(X_train.shape[1])])\n",
    "        X_test = pd.DataFrame(X_test, columns=[f\"component_{i+1}\" for i in range(X_test.shape[1])])  \n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "def feature_selection(X_train, X_test, selector):\n",
    "    X_train = selector.fit_transform(X_train)\n",
    "    X_test = selector.transform(X_test)\n",
    "    X_train = pd.DataFrame(X_train, columns=selector.get_feature_names_out())\n",
    "    X_test = pd.DataFrame(X_test, columns=selector.get_feature_names_out())\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelling(model, X_train, y_train, X_test, probability=True):\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    if probability:\n",
    "        y_pred_proba = model.predict_proba(X_test)\n",
    "        return y_pred, y_pred_proba\n",
    "    return y_pred\n",
    "\n",
    "def evaluation(y_true, y_pred, y_pred_prob, scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc']):\n",
    "    scores = {'accuracy': accuracy_score,\n",
    "              'f1': f1_score,\n",
    "              'recall': recall_score,\n",
    "              'precision': precision_score,\n",
    "              'roc_auc': roc_auc_score}\n",
    "    \n",
    "    result = {}\n",
    "    for method in scoring:\n",
    "        if method == 'roc_auc':\n",
    "            result[method] = scores[method](y_true, y_pred_prob.T[1])\n",
    "        else:\n",
    "            result[method] = scores[method](y_true, y_pred)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def cross_validation(data, extractor=None, model=None, selector=None, length_scaler=None,\n",
    "                     scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5,\n",
    "                     avg_output=True, quiet=True, data_list=None):\n",
    "    kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    indices_folds = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "    scores = {method: [] for method in scoring}\n",
    "    round_num = 1\n",
    "    fold = 0\n",
    "\n",
    "    for train_indices, test_indices in indices_folds:\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if not quiet:\n",
    "            print(f\"round {round_num}:\")\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if extractor is not None:\n",
    "            start = time.time()\n",
    "            X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        else:\n",
    "            X_train, X_test = data_list[fold][0], data_list[fold][1]\n",
    "            fold += 1\n",
    "\n",
    "        # start = time.time()\n",
    "        # X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "        # durations = time.time() - start\n",
    "        # if not quiet:\n",
    "        #     print(f\"round {round}:\")\n",
    "        #     print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        if length_scaler is not None:\n",
    "            start = time.time()\n",
    "            X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "            X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tLength scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        \n",
    "        if selector is not None:\n",
    "            start = time.time()\n",
    "            X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "        durations = time.time() - start\n",
    "        if not quiet:\n",
    "            print(f'\\tModelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "        start = time.time()\n",
    "        result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "        for method in scoring:\n",
    "            scores[method].append(result[method])\n",
    "        durations = time.time() - start\n",
    "        if not quiet:\n",
    "            print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s', end=\"\\n\\n\")\n",
    "\n",
    "        round_num += 1\n",
    "        \n",
    "    if avg_output:\n",
    "        avg_scores = {key: np.mean(values) for key, values in scores.items()}\n",
    "\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# def multi_models_cross_validation(data, extractor, models=None, selector=None, length_scaler=None,\n",
    "#                                   scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5, quiet=True):\n",
    "#     kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "#     indices = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "#     output = {model: {score: [] for score in scoring} for model in models.keys()}\n",
    "#     round = 1\n",
    "\n",
    "#     for train_indices, test_indices in indices:\n",
    "#         if not quiet:\n",
    "#           print(f\"round {round}:\")\n",
    "\n",
    "#         train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "#         y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "#         start = time.time()\n",
    "#         X_train, X_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         start = time.time()\n",
    "#         if length_scaler is not None:\n",
    "#             X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "#             X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tLength scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         start = time.time()\n",
    "#         if selector is not None:\n",
    "#             X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "#         durations = time.time() - start\n",
    "#         if not quiet:\n",
    "#             print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#         for key, model in models.items():\n",
    "#             start = time.time()\n",
    "#             y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "#             durations = time.time() - start\n",
    "#             if not quiet:\n",
    "#                 print(f'\\t{key} - Modelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "#             start = time.time()\n",
    "#             result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "#             for method in scoring:\n",
    "#                 output[key][method].append(result[method])\n",
    "#             durations = time.time() - start\n",
    "#             if not quiet:\n",
    "#                 print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "#         if not quiet:\n",
    "#             print()\n",
    "#         round += 1\n",
    "\n",
    "#     for model_name, model_scores in output.items():\n",
    "#         output[model_name] = {key: np.mean(values) for key, values in model_scores.items()}\n",
    "\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def multi_cross_validation(data, extractor, models=None, selector=None, length_scalers=None,\n",
    "                           scoring=['accuracy', 'f1', 'recall', 'precision', 'roc_auc'], cv=5, quiet=True,\n",
    "                           data_list=None):\n",
    "    kfolds = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    indices = kfolds.split(data.iloc[:, :-1], data.iloc[:, -1])\n",
    "    output = {model: {score: [] for score in scoring} for model in models.keys()}\n",
    "    output = {length: {model: {score: [] for score in scoring} for model in models.keys()} for length in length_scalers}\n",
    "    round_num = 1\n",
    "    fold = 0\n",
    "\n",
    "    for train_indices, test_indices in indices:\n",
    "        if not quiet:\n",
    "            print(f\"round {round_num}:\")\n",
    "\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "\n",
    "        if extractor is not None:\n",
    "            start = time.time()\n",
    "            text_train, text_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'], extractor=extractor)\n",
    "            durations = time.time() - start\n",
    "            if not quiet:\n",
    "                print(f'\\tText extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        else:\n",
    "            text_train, text_test = data_list[fold][0], data_list[fold][1]\n",
    "            fold += 1\n",
    "        \n",
    "        for length_scaler_name, length_scaler in length_scalers.items():\n",
    "            X_train, X_test = text_train, text_test\n",
    "            if length_scaler is not None:\n",
    "                start = time.time()\n",
    "                X_train['length'] = length_scaler.fit_transform(train_set[['length']])\n",
    "                X_test['length'] = length_scaler.transform(test_set[['length']])\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tLength scaled - {length_scaler_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "            if selector is not None:\n",
    "                start = time.time()\n",
    "                X_train, X_test = feature_selection(X_train, X_test, selector)\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tDimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "            for model_name, model in models.items():\n",
    "                start = time.time()\n",
    "                y_pred, y_pred_prob = modelling(model, X_train, y_train, X_test)\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\t{model_name} - Modelling: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "                start = time.time()\n",
    "                result = evaluation(y_true=y_test, y_pred=y_pred, y_pred_prob=y_pred_prob, scoring=scoring)\n",
    "                for method in scoring:\n",
    "                    output[length_scaler_name][model_name][method].append(result[method])\n",
    "                durations = time.time() - start\n",
    "                if not quiet:\n",
    "                    print(f'\\tEvaluation: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        if not quiet:\n",
    "            print()\n",
    "        round_num += 1\n",
    "\n",
    "    for length, result in output.items():\n",
    "        for model_name, model_scores in result.items():\n",
    "            output[length][model_name] = {key: np.mean(values) for key, values in model_scores.items()}\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extractor = AvgWord2Vec(window=10, vector_size=300, seed=42, sg=1, ngram_range=(1, 1), epochs=15)\n",
    "# model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "# # model = SVC(probability=True, class_weight='balanced')\n",
    "# # extractor = TfidfVectorizer(min_df=0.001, ngram_range=(1, 1))\n",
    "# # model = SVC()\n",
    "\n",
    "# cross_validation(data=osf_cleaned, length_scaler=None,\n",
    "#                  model=model, extractor=extractor, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(X_train, X_test, extractor, length_scaler=None, selector=None, quiet=True):\n",
    "    start = time.time()\n",
    "    X_train_final, X_test_final = text_extractor(X_train=X_train['texts'], X_test=X_train['texts'],\n",
    "                                                 extractor=extractor)\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Text extraction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    start = time.time()\n",
    "    if length_scaler is not None:\n",
    "        X_train_final['length'] = length_scaler.fit_transform(X_train[['length']])\n",
    "        X_test_final['length'] = length_scaler.transform(X_test[['length']])\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Length scale: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    start = time.time()\n",
    "    if selector is not None:\n",
    "        X_train_final, X_test_final = feature_selection(X_train_final, X_test_final, selector)\n",
    "    durations = time.time() - start\n",
    "    if not quiet:\n",
    "        print(f'Dimensionality reduction: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "\n",
    "    return X_train_final, X_test_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>texts</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>love well made sturdi comfort love pretti</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>love great upgrad origin mine coupl year</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "      <td>pillow save back love look feel pillow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>81</td>\n",
       "      <td>miss inform use great product price</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85</td>\n",
       "      <td>nice set good qualiti set two month</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40427</th>\n",
       "      <td>1694</td>\n",
       "      <td>read review say bra ran small order two band c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40428</th>\n",
       "      <td>1304</td>\n",
       "      <td>sure exactli would littl larg small size think...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40429</th>\n",
       "      <td>1987</td>\n",
       "      <td>wear hood wear hood wear jacket without hood 3...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40430</th>\n",
       "      <td>1301</td>\n",
       "      <td>like noth dress reason gave 4 star order size ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40431</th>\n",
       "      <td>1768</td>\n",
       "      <td>work wed industri work long day foot outsid he...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40432 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       length                                              texts  labels\n",
       "0          75          love well made sturdi comfort love pretti       1\n",
       "1          80           love great upgrad origin mine coupl year       1\n",
       "2          67             pillow save back love look feel pillow       1\n",
       "3          81                miss inform use great product price       1\n",
       "4          85                nice set good qualiti set two month       1\n",
       "...       ...                                                ...     ...\n",
       "40427    1694  read review say bra ran small order two band c...       0\n",
       "40428    1304  sure exactli would littl larg small size think...       1\n",
       "40429    1987  wear hood wear hood wear jacket without hood 3...       0\n",
       "40430    1301  like noth dress reason gave 4 star order size ...       1\n",
       "40431    1768  work wed industri work long day foot outsid he...       0\n",
       "\n",
       "[40432 rows x 3 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = osf_cleaned.copy()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_file = False\n",
    "\n",
    "if saved_file:\n",
    "    kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    indices = kfolds.split(data)\n",
    "    fold = 1\n",
    "    for train_indices, test_indices in indices:\n",
    "        train_set, test_set = data.iloc[train_indices, :-1], data.iloc[test_indices, :-1]\n",
    "        y_train, y_test = data.iloc[train_indices, -1], data.iloc[test_indices, -1]\n",
    "        text_train, text_test = text_extractor(X_train=train_set['texts'], X_test=test_set['texts'],\n",
    "                                            extractor=AvgWord2Vec(vector_size=600, window=10, ngram_range=(1, 1), sg=1, epochs=15))\n",
    "        text_train.to_csv(f\"../../data/processed_skipgram/uni/fold{fold}/train.csv\")\n",
    "        text_test.to_csv(f\"../../data/processed_skipgram/uni/fold{fold}/test.csv\")\n",
    "        fold += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>text_extraction</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.865651</td>\n",
       "      <td>0.866163</td>\n",
       "      <td>0.928591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.859022</td>\n",
       "      <td>0.860758</td>\n",
       "      <td>0.938256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.869880</td>\n",
       "      <td>0.871144</td>\n",
       "      <td>0.946182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.862955</td>\n",
       "      <td>0.863540</td>\n",
       "      <td>0.941246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>NaN</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.802508</td>\n",
       "      <td>0.800856</td>\n",
       "      <td>0.883884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.543703</td>\n",
       "      <td>0.674749</td>\n",
       "      <td>0.600135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Word2Vec</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.505070</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.559175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        length_used  ngrams text_extraction               model  accuracy  \\\n",
       "63              NaN  (1, 1)        Word2Vec  LogisticRegression  0.865651   \n",
       "64              NaN  (1, 1)        Word2Vec            LightGBM  0.859022   \n",
       "65              NaN  (1, 1)        Word2Vec            Catboost  0.869880   \n",
       "66              NaN  (1, 1)        Word2Vec             XGBoost  0.862955   \n",
       "67              NaN  (1, 1)        Word2Vec            AdaBoost  0.802508   \n",
       "..              ...     ...             ...                 ...       ...   \n",
       "121  StandardScaler  (1, 3)        Word2Vec            Catboost  0.517659   \n",
       "122  StandardScaler  (1, 3)        Word2Vec             XGBoost  0.516695   \n",
       "123  StandardScaler  (1, 3)        Word2Vec            AdaBoost  0.500148   \n",
       "124  StandardScaler  (1, 3)        Word2Vec                 KNN  0.543703   \n",
       "125  StandardScaler  (1, 3)        Word2Vec                 SVM  0.505070   \n",
       "\n",
       "           f1   roc_auc  \n",
       "63   0.866163  0.928591  \n",
       "64   0.860758  0.938256  \n",
       "65   0.871144  0.946182  \n",
       "66   0.863540  0.941246  \n",
       "67   0.800856  0.883884  \n",
       "..        ...       ...  \n",
       "121  0.672257  0.477459  \n",
       "122  0.671254  0.509397  \n",
       "123  0.665530  0.550193  \n",
       "124  0.674749  0.600135  \n",
       "125  0.665777  0.559175  \n",
       "\n",
       "[63 rows x 7 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_file = pd.read_csv(r\"..\\..\\output\\csv\\w2v_model_results.csv\")\n",
    "new_file = saved_file.loc[(saved_file[\"w2v_type\"]==\"skipgram\")].drop(columns=[\"vector_size\", \"epochs\", \"precision\", \"recall\"]).rename(columns={\"w2v_type\": \"text_extraction\"})\n",
    "new_file[\"text_extraction\"] = [\"Word2Vec\"]*63\n",
    "new_file = new_file[[\"length_used\", \"ngrams\", \"text_extraction\", \"model\", \"accuracy\", \"f1\", \"roc_auc\"]]\n",
    "# new_file.reset_index(drop=True).to_csv(\"../../output/csv/w2v_results_final.csv\", index=False)\n",
    "new_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Testing\n",
    "### Skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 0m23s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m16s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LightGBM: Done in 1m17s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m11s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 2m2s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: Catboost: Done in 10m48s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m25s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m28s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m34s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m27s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 0m32s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: XGBoost: Done in 2m28s\n",
      "\n",
      "round 1:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 4m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 4m1s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tLength scale: Done in 0m0s\n",
      "\tModelling: Done in 3m57s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tLength scale: Done in 0m0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m testcases[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     46\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 47\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mosf_cleaned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mlength_scaler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# scores = multi_cross_validation(data=osf_cleaned,\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;66;03m#                                 extractor=None,\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#                                 models=testcases['model'],\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;66;03m#                                 length_scalers=testcases['length_used'], quiet=False, data_list=data_list)\u001b[39;00m\n\u001b[0;32m     54\u001b[0m     output[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlength_used\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scaler_name)\n",
      "Cell \u001b[1;32mIn[9], line 56\u001b[0m, in \u001b[0;36mcross_validation\u001b[1;34m(data, extractor, model, selector, length_scaler, scoring, cv, avg_output, quiet, data_list)\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mDimensionality reduction: Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     55\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 56\u001b[0m y_pred, y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodelling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m, in \u001b[0;36mmodelling\u001b[1;34m(model, X_train, y_train, X_test, probability)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodelling\u001b[39m(model, X_train, y_train, X_test, probability\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m probability:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:169\u001b[0m, in \u001b[0;36mBaseWeightBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    166\u001b[0m sample_weight[zero_weight_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;66;03m# Boosting step\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m sample_weight, estimator_weight, estimator_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Early termination\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:589\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_boost_real(iboost, X, y, sample_weight, random_state)\n\u001b[0;32m    588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# elif self.algorithm == \"SAMME\":\u001b[39;00m\n\u001b[1;32m--> 589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_boost_discrete\u001b[49m\u001b[43m(\u001b[49m\u001b[43miboost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:656\u001b[0m, in \u001b[0;36mAdaBoostClassifier._boost_discrete\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    653\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Implement a single boost using the SAMME discrete algorithm.\"\"\"\u001b[39;00m\n\u001b[0;32m    654\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m--> 656\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    658\u001b[0m y_predict \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iboost \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1009\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    978\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    980\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    981\u001b[0m \n\u001b[0;32m    982\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1010\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1014\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1015\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': {(1, 1): \"uni\", (1, 2): \"unibi\", (1, 3): \"unibitri\"},\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "x = {'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "     'KNN': KNeighborsClassifier(),\n",
    "     'SVM': SVC(probability=True, max_iter=1000)}\n",
    "\n",
    "for n in [(1, 1)]:\n",
    "    # start = time.time()\n",
    "    data_list = []\n",
    "\n",
    "    for fold in range(1, 6):\n",
    "        train = pd.read_csv(f\"../../data/processed_skipgram/{testcases['ngrams'][n]}/fold{fold}/train.csv\", index_col=0)\n",
    "        test = pd.read_csv(f\"../../data/processed_skipgram/{testcases['ngrams'][n]}/fold{fold}/test.csv\", index_col=0)\n",
    "        data_list.append([train, test])\n",
    "    \n",
    "    for scaler_name, scaler in {'MinMaxScaler': MinMaxScaler()}.items():\n",
    "        for model_name, model in testcases[\"model\"].items():\n",
    "            start = time.time()\n",
    "            scores = cross_validation(data=osf_cleaned,\n",
    "                                      length_scaler=scaler,\n",
    "                                      model=model, quiet=False, data_list=data_list)\n",
    "            # scores = multi_cross_validation(data=osf_cleaned,\n",
    "            #                                 extractor=None,\n",
    "            #                                 models=testcases['model'],\n",
    "            #                                 length_scalers=testcases['length_used'], quiet=False, data_list=data_list)\n",
    "            output[\"length_used\"].append(scaler_name)\n",
    "            output['w2v_type'].append('skipgram')\n",
    "            output['vector_size'].append(600)\n",
    "            output['epochs'].append(15)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in scores.items():\n",
    "                output[method].append(score)\n",
    "            # for scaler_name, scaler_resuls in scores.items():\n",
    "            #     output['length_used'].append(scaler_name)\n",
    "            #     for model_name, model_scores in scaler_resuls.items():\n",
    "            #         output['w2v_type'].append('skipgram')\n",
    "            #         output['vector_size'].append(600)\n",
    "            #         output['epochs'].append(15)\n",
    "            #         output['ngrams'].append(n)\n",
    "            #         output['model'].append(model_name)\n",
    "            #         for method, score in model_scores.items():\n",
    "            #             output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df.to_csv(\"../../output/csv/w2v_model_results_sg_uni.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>w2v_type</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.880590</td>\n",
       "      <td>0.882276</td>\n",
       "      <td>0.895280</td>\n",
       "      <td>0.869782</td>\n",
       "      <td>0.940606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.869905</td>\n",
       "      <td>0.875007</td>\n",
       "      <td>0.910862</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.951442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.887070</td>\n",
       "      <td>0.889021</td>\n",
       "      <td>0.904816</td>\n",
       "      <td>0.873968</td>\n",
       "      <td>0.957431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.866665</td>\n",
       "      <td>0.870922</td>\n",
       "      <td>0.899529</td>\n",
       "      <td>0.844371</td>\n",
       "      <td>0.946819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.818881</td>\n",
       "      <td>0.828889</td>\n",
       "      <td>0.877120</td>\n",
       "      <td>0.785922</td>\n",
       "      <td>0.906793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.847794</td>\n",
       "      <td>0.850615</td>\n",
       "      <td>0.866806</td>\n",
       "      <td>0.835094</td>\n",
       "      <td>0.914293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.499851</td>\n",
       "      <td>0.666196</td>\n",
       "      <td>0.998284</td>\n",
       "      <td>0.499931</td>\n",
       "      <td>0.850376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.883137</td>\n",
       "      <td>0.883712</td>\n",
       "      <td>0.888535</td>\n",
       "      <td>0.879079</td>\n",
       "      <td>0.943551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.823804</td>\n",
       "      <td>0.843013</td>\n",
       "      <td>0.944722</td>\n",
       "      <td>0.761323</td>\n",
       "      <td>0.946015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.856931</td>\n",
       "      <td>0.945169</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.830506</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>0.935710</td>\n",
       "      <td>0.773617</td>\n",
       "      <td>0.942627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.829631</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.780040</td>\n",
       "      <td>0.907543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.861199</td>\n",
       "      <td>0.859714</td>\n",
       "      <td>0.850763</td>\n",
       "      <td>0.868913</td>\n",
       "      <td>0.924831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.500297</td>\n",
       "      <td>0.666380</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>0.500150</td>\n",
       "      <td>0.840014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.883681</td>\n",
       "      <td>0.884167</td>\n",
       "      <td>0.888289</td>\n",
       "      <td>0.880162</td>\n",
       "      <td>0.943573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.827761</td>\n",
       "      <td>0.845753</td>\n",
       "      <td>0.943536</td>\n",
       "      <td>0.766520</td>\n",
       "      <td>0.947083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.842130</td>\n",
       "      <td>0.856931</td>\n",
       "      <td>0.945169</td>\n",
       "      <td>0.783927</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.830506</td>\n",
       "      <td>0.846812</td>\n",
       "      <td>0.935710</td>\n",
       "      <td>0.773617</td>\n",
       "      <td>0.942627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.817916</td>\n",
       "      <td>0.829631</td>\n",
       "      <td>0.886268</td>\n",
       "      <td>0.780040</td>\n",
       "      <td>0.907543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.859863</td>\n",
       "      <td>0.851702</td>\n",
       "      <td>0.804766</td>\n",
       "      <td>0.904487</td>\n",
       "      <td>0.928885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.576943</td>\n",
       "      <td>0.699483</td>\n",
       "      <td>0.982196</td>\n",
       "      <td>0.543975</td>\n",
       "      <td>0.887883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499283</td>\n",
       "      <td>0.664933</td>\n",
       "      <td>0.993769</td>\n",
       "      <td>0.499638</td>\n",
       "      <td>0.550229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.524040</td>\n",
       "      <td>0.674650</td>\n",
       "      <td>0.985917</td>\n",
       "      <td>0.513078</td>\n",
       "      <td>0.817206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.533512</td>\n",
       "      <td>0.679166</td>\n",
       "      <td>0.985914</td>\n",
       "      <td>0.518448</td>\n",
       "      <td>0.853315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.530495</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.973736</td>\n",
       "      <td>0.516624</td>\n",
       "      <td>0.780557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.498788</td>\n",
       "      <td>0.664840</td>\n",
       "      <td>0.994326</td>\n",
       "      <td>0.499398</td>\n",
       "      <td>0.597742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.565690</td>\n",
       "      <td>0.687858</td>\n",
       "      <td>0.954497</td>\n",
       "      <td>0.538553</td>\n",
       "      <td>0.727047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.492827</td>\n",
       "      <td>0.659270</td>\n",
       "      <td>0.981470</td>\n",
       "      <td>0.496359</td>\n",
       "      <td>0.597131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.664921</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>0.499650</td>\n",
       "      <td>0.546067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.517412</td>\n",
       "      <td>0.671849</td>\n",
       "      <td>0.988150</td>\n",
       "      <td>0.508967</td>\n",
       "      <td>0.522746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.989475</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.986952</td>\n",
       "      <td>0.508602</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.994711</td>\n",
       "      <td>0.500077</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.560916</td>\n",
       "      <td>0.684653</td>\n",
       "      <td>0.951332</td>\n",
       "      <td>0.535519</td>\n",
       "      <td>0.707975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.490527</td>\n",
       "      <td>0.656725</td>\n",
       "      <td>0.974961</td>\n",
       "      <td>0.495144</td>\n",
       "      <td>0.585203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.499307</td>\n",
       "      <td>0.664921</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>0.499650</td>\n",
       "      <td>0.543472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.516645</td>\n",
       "      <td>0.671641</td>\n",
       "      <td>0.988787</td>\n",
       "      <td>0.508559</td>\n",
       "      <td>0.504940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.517659</td>\n",
       "      <td>0.672257</td>\n",
       "      <td>0.989475</td>\n",
       "      <td>0.509083</td>\n",
       "      <td>0.477459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.516695</td>\n",
       "      <td>0.671254</td>\n",
       "      <td>0.986952</td>\n",
       "      <td>0.508602</td>\n",
       "      <td>0.509397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.500148</td>\n",
       "      <td>0.665530</td>\n",
       "      <td>0.994711</td>\n",
       "      <td>0.500077</td>\n",
       "      <td>0.550193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.543703</td>\n",
       "      <td>0.674749</td>\n",
       "      <td>0.946426</td>\n",
       "      <td>0.524456</td>\n",
       "      <td>0.600135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>skipgram</td>\n",
       "      <td>600</td>\n",
       "      <td>15</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.505070</td>\n",
       "      <td>0.665777</td>\n",
       "      <td>0.986024</td>\n",
       "      <td>0.502601</td>\n",
       "      <td>0.559175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_used  w2v_type  vector_size  epochs  ngrams               model  \\\n",
       "0              NaN  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "1              NaN  skipgram          600      15  (1, 2)            LightGBM   \n",
       "2              NaN  skipgram          600      15  (1, 2)            Catboost   \n",
       "3              NaN  skipgram          600      15  (1, 2)             XGBoost   \n",
       "4              NaN  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "5              NaN  skipgram          600      15  (1, 2)                 KNN   \n",
       "6              NaN  skipgram          600      15  (1, 2)                 SVM   \n",
       "7     MinMaxScaler  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "8     MinMaxScaler  skipgram          600      15  (1, 2)            LightGBM   \n",
       "9     MinMaxScaler  skipgram          600      15  (1, 2)            Catboost   \n",
       "10    MinMaxScaler  skipgram          600      15  (1, 2)             XGBoost   \n",
       "11    MinMaxScaler  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "12    MinMaxScaler  skipgram          600      15  (1, 2)                 KNN   \n",
       "13    MinMaxScaler  skipgram          600      15  (1, 2)                 SVM   \n",
       "14  StandardScaler  skipgram          600      15  (1, 2)  LogisticRegression   \n",
       "15  StandardScaler  skipgram          600      15  (1, 2)            LightGBM   \n",
       "16  StandardScaler  skipgram          600      15  (1, 2)            Catboost   \n",
       "17  StandardScaler  skipgram          600      15  (1, 2)             XGBoost   \n",
       "18  StandardScaler  skipgram          600      15  (1, 2)            AdaBoost   \n",
       "19  StandardScaler  skipgram          600      15  (1, 2)                 KNN   \n",
       "20  StandardScaler  skipgram          600      15  (1, 2)                 SVM   \n",
       "21             NaN  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "22             NaN  skipgram          600      15  (1, 3)            LightGBM   \n",
       "23             NaN  skipgram          600      15  (1, 3)            Catboost   \n",
       "24             NaN  skipgram          600      15  (1, 3)             XGBoost   \n",
       "25             NaN  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "26             NaN  skipgram          600      15  (1, 3)                 KNN   \n",
       "27             NaN  skipgram          600      15  (1, 3)                 SVM   \n",
       "28    MinMaxScaler  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "29    MinMaxScaler  skipgram          600      15  (1, 3)            LightGBM   \n",
       "30    MinMaxScaler  skipgram          600      15  (1, 3)            Catboost   \n",
       "31    MinMaxScaler  skipgram          600      15  (1, 3)             XGBoost   \n",
       "32    MinMaxScaler  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "33    MinMaxScaler  skipgram          600      15  (1, 3)                 KNN   \n",
       "34    MinMaxScaler  skipgram          600      15  (1, 3)                 SVM   \n",
       "35  StandardScaler  skipgram          600      15  (1, 3)  LogisticRegression   \n",
       "36  StandardScaler  skipgram          600      15  (1, 3)            LightGBM   \n",
       "37  StandardScaler  skipgram          600      15  (1, 3)            Catboost   \n",
       "38  StandardScaler  skipgram          600      15  (1, 3)             XGBoost   \n",
       "39  StandardScaler  skipgram          600      15  (1, 3)            AdaBoost   \n",
       "40  StandardScaler  skipgram          600      15  (1, 3)                 KNN   \n",
       "41  StandardScaler  skipgram          600      15  (1, 3)                 SVM   \n",
       "\n",
       "    accuracy        f1    recall  precision   roc_auc  \n",
       "0   0.880590  0.882276  0.895280   0.869782  0.940606  \n",
       "1   0.869905  0.875007  0.910862   0.842000  0.951442  \n",
       "2   0.887070  0.889021  0.904816   0.873968  0.957431  \n",
       "3   0.866665  0.870922  0.899529   0.844371  0.946819  \n",
       "4   0.818881  0.828889  0.877120   0.785922  0.906793  \n",
       "5   0.847794  0.850615  0.866806   0.835094  0.914293  \n",
       "6   0.499851  0.666196  0.998284   0.499931  0.850376  \n",
       "7   0.883137  0.883712  0.888535   0.879079  0.943551  \n",
       "8   0.823804  0.843013  0.944722   0.761323  0.946015  \n",
       "9   0.842130  0.856931  0.945169   0.783927  0.952500  \n",
       "10  0.830506  0.846812  0.935710   0.773617  0.942627  \n",
       "11  0.817916  0.829631  0.886268   0.780040  0.907543  \n",
       "12  0.861199  0.859714  0.850763   0.868913  0.924831  \n",
       "13  0.500297  0.666380  0.998227   0.500150  0.840014  \n",
       "14  0.883681  0.884167  0.888289   0.880162  0.943573  \n",
       "15  0.827761  0.845753  0.943536   0.766520  0.947083  \n",
       "16  0.842130  0.856931  0.945169   0.783927  0.952500  \n",
       "17  0.830506  0.846812  0.935710   0.773617  0.942627  \n",
       "18  0.817916  0.829631  0.886268   0.780040  0.907543  \n",
       "19  0.859863  0.851702  0.804766   0.904487  0.928885  \n",
       "20  0.576943  0.699483  0.982196   0.543975  0.887883  \n",
       "21  0.499283  0.664933  0.993769   0.499638  0.550229  \n",
       "22  0.524040  0.674650  0.985917   0.513078  0.817206  \n",
       "23  0.533512  0.679166  0.985914   0.518448  0.853315  \n",
       "24  0.530495  0.674827  0.973736   0.516624  0.780557  \n",
       "25  0.498788  0.664840  0.994326   0.499398  0.597742  \n",
       "26  0.565690  0.687858  0.954497   0.538553  0.727047  \n",
       "27  0.492827  0.659270  0.981470   0.496359  0.597131  \n",
       "28  0.499307  0.664921  0.993669   0.499650  0.546067  \n",
       "29  0.517412  0.671849  0.988150   0.508967  0.522746  \n",
       "30  0.517659  0.672257  0.989475   0.509083  0.477459  \n",
       "31  0.516695  0.671254  0.986952   0.508602  0.509397  \n",
       "32  0.500148  0.665530  0.994711   0.500077  0.550193  \n",
       "33  0.560916  0.684653  0.951332   0.535519  0.707975  \n",
       "34  0.490527  0.656725  0.974961   0.495144  0.585203  \n",
       "35  0.499307  0.664921  0.993669   0.499650  0.543472  \n",
       "36  0.516645  0.671641  0.988787   0.508559  0.504940  \n",
       "37  0.517659  0.672257  0.989475   0.509083  0.477459  \n",
       "38  0.516695  0.671254  0.986952   0.508602  0.509397  \n",
       "39  0.500148  0.665530  0.994711   0.500077  0.550193  \n",
       "40  0.543703  0.674749  0.946426   0.524456  0.600135  \n",
       "41  0.505070  0.665777  0.986024   0.502601  0.559175  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df\n",
    "# saved_file = pd.read_csv(\"../../output/csv/w2v_model_results.csv\")\n",
    "# saved_file\n",
    "# new_df = pd.concat([saved_file, output_df], axis=0, ignore_index=True)\n",
    "# new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length_used</th>\n",
       "      <th>w2v_type</th>\n",
       "      <th>vector_size</th>\n",
       "      <th>epochs</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860457</td>\n",
       "      <td>0.861401</td>\n",
       "      <td>0.867456</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.923285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.861001</td>\n",
       "      <td>0.863863</td>\n",
       "      <td>0.882299</td>\n",
       "      <td>0.846196</td>\n",
       "      <td>0.939263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.869732</td>\n",
       "      <td>0.871865</td>\n",
       "      <td>0.886598</td>\n",
       "      <td>0.857626</td>\n",
       "      <td>0.944711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.863375</td>\n",
       "      <td>0.864967</td>\n",
       "      <td>0.875369</td>\n",
       "      <td>0.854818</td>\n",
       "      <td>0.941157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.822641</td>\n",
       "      <td>0.822810</td>\n",
       "      <td>0.823706</td>\n",
       "      <td>0.821935</td>\n",
       "      <td>0.901408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.828502</td>\n",
       "      <td>0.829918</td>\n",
       "      <td>0.836860</td>\n",
       "      <td>0.823118</td>\n",
       "      <td>0.889266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.696355</td>\n",
       "      <td>0.720611</td>\n",
       "      <td>0.785233</td>\n",
       "      <td>0.670095</td>\n",
       "      <td>0.795997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860902</td>\n",
       "      <td>0.861681</td>\n",
       "      <td>0.866714</td>\n",
       "      <td>0.856712</td>\n",
       "      <td>0.923469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.884151</td>\n",
       "      <td>0.885006</td>\n",
       "      <td>0.891749</td>\n",
       "      <td>0.878371</td>\n",
       "      <td>0.957102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.892635</td>\n",
       "      <td>0.892974</td>\n",
       "      <td>0.896006</td>\n",
       "      <td>0.889983</td>\n",
       "      <td>0.961555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.886171</td>\n",
       "      <td>0.886559</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.958313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.841141</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.840059</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>0.916428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.828923</td>\n",
       "      <td>0.830180</td>\n",
       "      <td>0.836367</td>\n",
       "      <td>0.824116</td>\n",
       "      <td>0.889746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.695241</td>\n",
       "      <td>0.733775</td>\n",
       "      <td>0.837723</td>\n",
       "      <td>0.654123</td>\n",
       "      <td>0.817718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.860902</td>\n",
       "      <td>0.861668</td>\n",
       "      <td>0.866616</td>\n",
       "      <td>0.856782</td>\n",
       "      <td>0.923470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.885338</td>\n",
       "      <td>0.886294</td>\n",
       "      <td>0.893867</td>\n",
       "      <td>0.878857</td>\n",
       "      <td>0.957507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.892635</td>\n",
       "      <td>0.892974</td>\n",
       "      <td>0.896006</td>\n",
       "      <td>0.889983</td>\n",
       "      <td>0.961555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.886130</td>\n",
       "      <td>0.886171</td>\n",
       "      <td>0.886559</td>\n",
       "      <td>0.885806</td>\n",
       "      <td>0.958313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.841141</td>\n",
       "      <td>0.840971</td>\n",
       "      <td>0.840059</td>\n",
       "      <td>0.841892</td>\n",
       "      <td>0.916428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.841808</td>\n",
       "      <td>0.839433</td>\n",
       "      <td>0.827104</td>\n",
       "      <td>0.852174</td>\n",
       "      <td>0.905929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 1)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.773844</td>\n",
       "      <td>0.787692</td>\n",
       "      <td>0.837620</td>\n",
       "      <td>0.746080</td>\n",
       "      <td>0.875583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.867184</td>\n",
       "      <td>0.867731</td>\n",
       "      <td>0.871618</td>\n",
       "      <td>0.863908</td>\n",
       "      <td>0.928794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.858924</td>\n",
       "      <td>0.865220</td>\n",
       "      <td>0.905936</td>\n",
       "      <td>0.828028</td>\n",
       "      <td>0.942348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.867506</td>\n",
       "      <td>0.872440</td>\n",
       "      <td>0.906487</td>\n",
       "      <td>0.840893</td>\n",
       "      <td>0.947507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.861817</td>\n",
       "      <td>0.866893</td>\n",
       "      <td>0.900266</td>\n",
       "      <td>0.835933</td>\n",
       "      <td>0.942467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.810422</td>\n",
       "      <td>0.818943</td>\n",
       "      <td>0.857320</td>\n",
       "      <td>0.784520</td>\n",
       "      <td>0.896601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.814652</td>\n",
       "      <td>0.825759</td>\n",
       "      <td>0.878564</td>\n",
       "      <td>0.778975</td>\n",
       "      <td>0.888866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.506975</td>\n",
       "      <td>0.667402</td>\n",
       "      <td>0.989181</td>\n",
       "      <td>0.503705</td>\n",
       "      <td>0.748289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.866640</td>\n",
       "      <td>0.867173</td>\n",
       "      <td>0.870966</td>\n",
       "      <td>0.863433</td>\n",
       "      <td>0.929053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.861545</td>\n",
       "      <td>0.869756</td>\n",
       "      <td>0.924818</td>\n",
       "      <td>0.820915</td>\n",
       "      <td>0.950871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.834681</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.865577</td>\n",
       "      <td>0.872665</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.828917</td>\n",
       "      <td>0.951320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.814256</td>\n",
       "      <td>0.823598</td>\n",
       "      <td>0.867254</td>\n",
       "      <td>0.784603</td>\n",
       "      <td>0.901865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.816136</td>\n",
       "      <td>0.826791</td>\n",
       "      <td>0.877822</td>\n",
       "      <td>0.781396</td>\n",
       "      <td>0.890316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.509473</td>\n",
       "      <td>0.668106</td>\n",
       "      <td>0.987426</td>\n",
       "      <td>0.504911</td>\n",
       "      <td>0.786280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.866566</td>\n",
       "      <td>0.867089</td>\n",
       "      <td>0.870817</td>\n",
       "      <td>0.863413</td>\n",
       "      <td>0.929054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.862559</td>\n",
       "      <td>0.870660</td>\n",
       "      <td>0.925374</td>\n",
       "      <td>0.822090</td>\n",
       "      <td>0.950693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.879610</td>\n",
       "      <td>0.929672</td>\n",
       "      <td>0.834681</td>\n",
       "      <td>0.956661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.865577</td>\n",
       "      <td>0.872665</td>\n",
       "      <td>0.921300</td>\n",
       "      <td>0.828917</td>\n",
       "      <td>0.951320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.814256</td>\n",
       "      <td>0.823598</td>\n",
       "      <td>0.867254</td>\n",
       "      <td>0.784603</td>\n",
       "      <td>0.901865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.834809</td>\n",
       "      <td>0.837975</td>\n",
       "      <td>0.854452</td>\n",
       "      <td>0.822196</td>\n",
       "      <td>0.906746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>StandardScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 2)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.583845</td>\n",
       "      <td>0.692302</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>0.549998</td>\n",
       "      <td>0.816544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.824471</td>\n",
       "      <td>0.839268</td>\n",
       "      <td>0.916574</td>\n",
       "      <td>0.774026</td>\n",
       "      <td>0.914417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.802433</td>\n",
       "      <td>0.824200</td>\n",
       "      <td>0.926459</td>\n",
       "      <td>0.742356</td>\n",
       "      <td>0.921827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>Catboost</td>\n",
       "      <td>0.816606</td>\n",
       "      <td>0.834611</td>\n",
       "      <td>0.925580</td>\n",
       "      <td>0.760036</td>\n",
       "      <td>0.927827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.791353</td>\n",
       "      <td>0.814465</td>\n",
       "      <td>0.915673</td>\n",
       "      <td>0.733607</td>\n",
       "      <td>0.908968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.748787</td>\n",
       "      <td>0.782432</td>\n",
       "      <td>0.900095</td>\n",
       "      <td>0.693407</td>\n",
       "      <td>0.871499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>KNN</td>\n",
       "      <td>0.773175</td>\n",
       "      <td>0.801650</td>\n",
       "      <td>0.916893</td>\n",
       "      <td>0.712178</td>\n",
       "      <td>0.878652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>NaN</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.520973</td>\n",
       "      <td>0.666947</td>\n",
       "      <td>0.958654</td>\n",
       "      <td>0.511862</td>\n",
       "      <td>0.767138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>MinMaxScaler</td>\n",
       "      <td>cbow</td>\n",
       "      <td>300</td>\n",
       "      <td>30</td>\n",
       "      <td>(1, 3)</td>\n",
       "      <td>LogisticRegression</td>\n",
       "      <td>0.832633</td>\n",
       "      <td>0.845160</td>\n",
       "      <td>0.913563</td>\n",
       "      <td>0.786331</td>\n",
       "      <td>0.916070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       length_used w2v_type  vector_size  epochs  ngrams               model  \\\n",
       "0              NaN     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "1              NaN     cbow          300      30  (1, 1)            LightGBM   \n",
       "2              NaN     cbow          300      30  (1, 1)            Catboost   \n",
       "3              NaN     cbow          300      30  (1, 1)             XGBoost   \n",
       "4              NaN     cbow          300      30  (1, 1)            AdaBoost   \n",
       "5              NaN     cbow          300      30  (1, 1)                 KNN   \n",
       "6              NaN     cbow          300      30  (1, 1)                 SVM   \n",
       "7     MinMaxScaler     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "8     MinMaxScaler     cbow          300      30  (1, 1)            LightGBM   \n",
       "9     MinMaxScaler     cbow          300      30  (1, 1)            Catboost   \n",
       "10    MinMaxScaler     cbow          300      30  (1, 1)             XGBoost   \n",
       "11    MinMaxScaler     cbow          300      30  (1, 1)            AdaBoost   \n",
       "12    MinMaxScaler     cbow          300      30  (1, 1)                 KNN   \n",
       "13    MinMaxScaler     cbow          300      30  (1, 1)                 SVM   \n",
       "14  StandardScaler     cbow          300      30  (1, 1)  LogisticRegression   \n",
       "15  StandardScaler     cbow          300      30  (1, 1)            LightGBM   \n",
       "16  StandardScaler     cbow          300      30  (1, 1)            Catboost   \n",
       "17  StandardScaler     cbow          300      30  (1, 1)             XGBoost   \n",
       "18  StandardScaler     cbow          300      30  (1, 1)            AdaBoost   \n",
       "19  StandardScaler     cbow          300      30  (1, 1)                 KNN   \n",
       "20  StandardScaler     cbow          300      30  (1, 1)                 SVM   \n",
       "21             NaN     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "22             NaN     cbow          300      30  (1, 2)            LightGBM   \n",
       "23             NaN     cbow          300      30  (1, 2)            Catboost   \n",
       "24             NaN     cbow          300      30  (1, 2)             XGBoost   \n",
       "25             NaN     cbow          300      30  (1, 2)            AdaBoost   \n",
       "26             NaN     cbow          300      30  (1, 2)                 KNN   \n",
       "27             NaN     cbow          300      30  (1, 2)                 SVM   \n",
       "28    MinMaxScaler     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "29    MinMaxScaler     cbow          300      30  (1, 2)            LightGBM   \n",
       "30    MinMaxScaler     cbow          300      30  (1, 2)            Catboost   \n",
       "31    MinMaxScaler     cbow          300      30  (1, 2)             XGBoost   \n",
       "32    MinMaxScaler     cbow          300      30  (1, 2)            AdaBoost   \n",
       "33    MinMaxScaler     cbow          300      30  (1, 2)                 KNN   \n",
       "34    MinMaxScaler     cbow          300      30  (1, 2)                 SVM   \n",
       "35  StandardScaler     cbow          300      30  (1, 2)  LogisticRegression   \n",
       "36  StandardScaler     cbow          300      30  (1, 2)            LightGBM   \n",
       "37  StandardScaler     cbow          300      30  (1, 2)            Catboost   \n",
       "38  StandardScaler     cbow          300      30  (1, 2)             XGBoost   \n",
       "39  StandardScaler     cbow          300      30  (1, 2)            AdaBoost   \n",
       "40  StandardScaler     cbow          300      30  (1, 2)                 KNN   \n",
       "41  StandardScaler     cbow          300      30  (1, 2)                 SVM   \n",
       "42             NaN     cbow          300      30  (1, 3)  LogisticRegression   \n",
       "43             NaN     cbow          300      30  (1, 3)            LightGBM   \n",
       "44             NaN     cbow          300      30  (1, 3)            Catboost   \n",
       "45             NaN     cbow          300      30  (1, 3)             XGBoost   \n",
       "46             NaN     cbow          300      30  (1, 3)            AdaBoost   \n",
       "47             NaN     cbow          300      30  (1, 3)                 KNN   \n",
       "48             NaN     cbow          300      30  (1, 3)                 SVM   \n",
       "49    MinMaxScaler     cbow          300      30  (1, 3)  LogisticRegression   \n",
       "\n",
       "    accuracy        f1    recall  precision   roc_auc  \n",
       "0   0.860457  0.861401  0.867456   0.855434  0.923285  \n",
       "1   0.861001  0.863863  0.882299   0.846196  0.939263  \n",
       "2   0.869732  0.871865  0.886598   0.857626  0.944711  \n",
       "3   0.863375  0.864967  0.875369   0.854818  0.941157  \n",
       "4   0.822641  0.822810  0.823706   0.821935  0.901408  \n",
       "5   0.828502  0.829918  0.836860   0.823118  0.889266  \n",
       "6   0.696355  0.720611  0.785233   0.670095  0.795997  \n",
       "7   0.860902  0.861681  0.866714   0.856712  0.923469  \n",
       "8   0.884151  0.885006  0.891749   0.878371  0.957102  \n",
       "9   0.892635  0.892974  0.896006   0.889983  0.961555  \n",
       "10  0.886130  0.886171  0.886559   0.885806  0.958313  \n",
       "11  0.841141  0.840971  0.840059   0.841892  0.916428  \n",
       "12  0.828923  0.830180  0.836367   0.824116  0.889746  \n",
       "13  0.695241  0.733775  0.837723   0.654123  0.817718  \n",
       "14  0.860902  0.861668  0.866616   0.856782  0.923470  \n",
       "15  0.885338  0.886294  0.893867   0.878857  0.957507  \n",
       "16  0.892635  0.892974  0.896006   0.889983  0.961555  \n",
       "17  0.886130  0.886171  0.886559   0.885806  0.958313  \n",
       "18  0.841141  0.840971  0.840059   0.841892  0.916428  \n",
       "19  0.841808  0.839433  0.827104   0.852174  0.905929  \n",
       "20  0.773844  0.787692  0.837620   0.746080  0.875583  \n",
       "21  0.867184  0.867731  0.871618   0.863908  0.928794  \n",
       "22  0.858924  0.865220  0.905936   0.828028  0.942348  \n",
       "23  0.867506  0.872440  0.906487   0.840893  0.947507  \n",
       "24  0.861817  0.866893  0.900266   0.835933  0.942467  \n",
       "25  0.810422  0.818943  0.857320   0.784520  0.896601  \n",
       "26  0.814652  0.825759  0.878564   0.778975  0.888866  \n",
       "27  0.506975  0.667402  0.989181   0.503705  0.748289  \n",
       "28  0.866640  0.867173  0.870966   0.863433  0.929053  \n",
       "29  0.861545  0.869756  0.924818   0.820915  0.950871  \n",
       "30  0.872774  0.879610  0.929672   0.834681  0.956661  \n",
       "31  0.865577  0.872665  0.921300   0.828917  0.951320  \n",
       "32  0.814256  0.823598  0.867254   0.784603  0.901865  \n",
       "33  0.816136  0.826791  0.877822   0.781396  0.890316  \n",
       "34  0.509473  0.668106  0.987426   0.504911  0.786280  \n",
       "35  0.866566  0.867089  0.870817   0.863413  0.929054  \n",
       "36  0.862559  0.870660  0.925374   0.822090  0.950693  \n",
       "37  0.872774  0.879610  0.929672   0.834681  0.956661  \n",
       "38  0.865577  0.872665  0.921300   0.828917  0.951320  \n",
       "39  0.814256  0.823598  0.867254   0.784603  0.901865  \n",
       "40  0.834809  0.837975  0.854452   0.822196  0.906746  \n",
       "41  0.583845  0.692302  0.935614   0.549998  0.816544  \n",
       "42  0.824471  0.839268  0.916574   0.774026  0.914417  \n",
       "43  0.802433  0.824200  0.926459   0.742356  0.921827  \n",
       "44  0.816606  0.834611  0.925580   0.760036  0.927827  \n",
       "45  0.791353  0.814465  0.915673   0.733607  0.908968  \n",
       "46  0.748787  0.782432  0.900095   0.693407  0.871499  \n",
       "47  0.773175  0.801650  0.916893   0.712178  0.878652  \n",
       "48  0.520973  0.666947  0.958654   0.511862  0.767138  \n",
       "49  0.832633  0.845160  0.913563   0.786331  0.916070  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"C:\\Users\\minhn\\Downloads\\w2v_model_kaggle (3).csv\"\n",
    "# \"C:\\Users\\minhn\\Downloads\\w2v_model_ggcolab_2 (1).csv\"\n",
    "outsources_df = pd.read_csv(r\"C:\\Users\\minhn\\Downloads\\w2v_model_ggcolab_2 (1).csv\")\n",
    "outsources_df\n",
    "saved_file = pd.read_csv(\"../../output/csv/w2v_model_results.csv\")\n",
    "saved_file\n",
    "new_df = pd.concat([outsources_df, saved_file], axis=0, ignore_index=True)\n",
    "new_df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df.to_csv(\"../../output/csv/w2v_model_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# output_df.to_csv(\"../../output/csv/w2v_model_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     34\u001b[0m extractor \u001b[38;5;241m=\u001b[39m AvgWord2Vec(vector_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m600\u001b[39m, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39mn, sg\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mosf_cleaned\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestcases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mlength_scalers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtestcases\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlength_used\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m scaler_name, scaler_resuls \u001b[38;5;129;01min\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     40\u001b[0m     output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlength_used\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(scaler_name)\n",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m, in \u001b[0;36mmulti_cross_validation\u001b[1;34m(data, extractor, models, selector, length_scalers, scoring, cv, quiet)\u001b[0m\n\u001b[0;32m     16\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[train_indices, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], data\u001b[38;5;241m.\u001b[39miloc[test_indices, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     18\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 19\u001b[0m text_train, text_test \u001b[38;5;241m=\u001b[39m \u001b[43mtext_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_set\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtexts\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n",
      "Cell \u001b[1;32mIn[7], line 12\u001b[0m, in \u001b[0;36mtext_extractor\u001b[1;34m(X_train, X_test, extractor)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtext_extractor\u001b[39m(X_train, X_test, extractor):\n\u001b[1;32m---> 12\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     13\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m extractor\u001b[38;5;241m.\u001b[39mtransform(X_test)\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m     14\u001b[0m     X_train \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X_train, columns\u001b[38;5;241m=\u001b[39mextractor\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "Cell \u001b[1;32mIn[14], line 103\u001b[0m, in \u001b[0;36mAvgWord2Vec.fit_transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBuild vocab: Done in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m(durations\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    102\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorpus_count\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    104\u001b[0m durations \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquiet:\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1073\u001b[0m, in \u001b[0;36mWord2Vec.train\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1070\u001b[0m     callback\u001b[38;5;241m.\u001b[39mon_epoch_begin(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1072\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m corpus_iterable \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1073\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcorpus_iterable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqueue_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1078\u001b[0m     trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_epoch_corpusfile(\n\u001b[0;32m   1079\u001b[0m         corpus_file, cur_epoch\u001b[38;5;241m=\u001b[39mcur_epoch, total_examples\u001b[38;5;241m=\u001b[39mtotal_examples, total_words\u001b[38;5;241m=\u001b[39mtotal_words,\n\u001b[0;32m   1080\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1434\u001b[0m, in \u001b[0;36mWord2Vec._train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1431\u001b[0m     thread\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# make interrupting the process with ctrl+c easier\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m     thread\u001b[38;5;241m.\u001b[39mstart()\n\u001b[1;32m-> 1434\u001b[0m trained_word_count, raw_word_count, job_tally \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_epoch_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_queue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_examples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_words\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_delay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreport_delay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_corpus_file_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trained_word_count, raw_word_count, job_tally\n",
      "File \u001b[1;32md:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\gensim\\models\\word2vec.py:1289\u001b[0m, in \u001b[0;36mWord2Vec._log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1286\u001b[0m unfinished_worker_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworkers\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m unfinished_worker_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1289\u001b[0m     report \u001b[38;5;241m=\u001b[39m \u001b[43mprogress_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# blocks if workers too slow\u001b[39;00m\n\u001b[0;32m   1290\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m report \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# a thread reporting that it finished\u001b[39;00m\n\u001b[0;32m   1291\u001b[0m         unfinished_worker_count \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[1;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'GradientBoost': GradientBoostingClassifier(random_state=42),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "for n in [(1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = AvgWord2Vec(vector_size=600, window=10, ngram_range=n, sg=1, epochs=15)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=testcases['model'],\n",
    "                                    length_scalers=testcases['length_used'], quiet=False)\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        output['length_used'].append(scaler_name)\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['w2v_type'].append('skipgram')\n",
    "            output['vector_size'].append(600)\n",
    "            output['epochs'].append(15)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tText extraction: Done in 4m14s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 1m26s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m33s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m21s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m40s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m27s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m3s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m35s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - StandardScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m29s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m37s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 3m51s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m59s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m28s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m19s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m31s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m56s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m24s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 2m38s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - StandardScaler: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 0m57s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tXGBoost - Modelling: Done in 0m12s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAdaBoost - Modelling: Done in 1m36s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tKNN - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\NEU\\DSEB\\DSEB Thesis\\spam-review-detection\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:297: ConvergenceWarning: Solver terminated early (max_iter=1000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tSVM - Modelling: Done in 3m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 4m11s\n",
      "\tLength scaled - None: Done in 0m0s\n",
      "\tDimensionality reduction: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLightGBM - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tCatboost - Modelling: Done in 1m10s\n",
      "\tEvaluation: Done in 0m0s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'w2v_type': [],\n",
    "          'vector_size': [],\n",
    "          'epochs': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'recall': [],\n",
    "          'precision': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LogisticRegression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42),\n",
    "                       'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42, algorithm=\"SAMME\"),\n",
    "                       'KNN': KNeighborsClassifier(),\n",
    "                       'SVM': SVC(probability=True, max_iter=1000)}}\n",
    "\n",
    "for n in [(1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = AvgWord2Vec(vector_size=300, window=10, ngram_range=n, sg=0, epochs=30)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=testcases['model'],\n",
    "                                    length_scalers=testcases['length_used'], quiet=False)\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        output['length_used'].append(scaler_name)\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['w2v_type'].append('cbow')\n",
    "            output['vector_size'].append(300)\n",
    "            output['epochs'].append(30)\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - cbow - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "round 1:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m4s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m10s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m1s\n",
      "\tLogisticRegression - Modelling: Done in 0m5s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 1m19s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 1) - model: LogisticRegression: Done in 1m19s\n",
      "\n",
      "round 1:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m4s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m13s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m11s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m3s\n",
      "\tLogisticRegression - Modelling: Done in 0m9s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m14s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 2) - model: LogisticRegression: Done in 2m4s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 2) - model: LogisticRegression: Done in 2m4s\n",
      "\n",
      "round 1:\n",
      "\tText extraction: Done in 0m4s\n",
      "\tLogisticRegression - Modelling: Done in 0m6s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 2:\n",
      "\tText extraction: Done in 0m7s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m15s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 3:\n",
      "\tText extraction: Done in 0m5s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m20s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 4:\n",
      "\tText extraction: Done in 0m6s\n",
      "\tLogisticRegression - Modelling: Done in 0m8s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m16s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "round 5:\n",
      "\tText extraction: Done in 0m8s\n",
      "\tLogisticRegression - Modelling: Done in 0m7s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\tLength scaled - MinMaxScaler: Done in 0m0s\n",
      "\tLogisticRegression - Modelling: Done in 0m19s\n",
      "\tEvaluation: Done in 0m0s\n",
      "\n",
      "length: None - skipgram - ngram: (1, 3) - model: LogisticRegression: Done in 2m39s\n",
      "\n",
      "length: MinMaxScaler - skipgram - ngram: (1, 3) - model: LogisticRegression: Done in 2m39s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "output = {'length_used': [],\n",
    "          'text_extraction': [],\n",
    "          'ngrams': [],\n",
    "          'model': [],\n",
    "          'accuracy': [],\n",
    "          'f1': [],\n",
    "          'roc_auc': []}\n",
    "\n",
    "testcases = {'length_used': {'None': None,\n",
    "                             'MinMaxScaler': MinMaxScaler(),\n",
    "                             'StandardScaler': StandardScaler()},\n",
    "             'type': {'skipgram': {'vector_size': 600, 'epochs': 15},\n",
    "                      'cbow': {'vector_size': 300, 'epochs': 30}},\n",
    "             'ngrams': [(1, 1), (1, 2), (1, 3)],\n",
    "             'feature_extraction': 'Word2Vec(vector_size=300, window=10)',\n",
    "             'feature selection': {'None': None},\n",
    "             'model': {'LightGBM': LGBMClassifier(random_state=42, verbose=-1),\n",
    "                       'Catboost': CatBoostClassifier(random_state=42, verbose=False),\n",
    "                       'XGBoost': XGBClassifier(random_state=42),\n",
    "                       'AdaBoost': AdaBoostClassifier(random_state=42),\n",
    "                       'KNN': KNeighborsClassifier()}}\n",
    "model = {\"LogisticRegression\": LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)}\n",
    "\n",
    "for n in [(1, 1), (1, 2), (1, 3)]:\n",
    "    start = time.time()\n",
    "    extractor = TfidfVectorizer(ngram_range=n, min_df=0.001)\n",
    "    scores = multi_cross_validation(data=osf_cleaned,\n",
    "                                    extractor=extractor,\n",
    "                                    models=model,\n",
    "                                    length_scalers={\"None\": None,\n",
    "                                                    \"MinMaxScaler\": MinMaxScaler()}, quiet=False,\n",
    "                                    scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n",
    "    for scaler_name, scaler_resuls in scores.items():\n",
    "        for model_name, model_scores in scaler_resuls.items():\n",
    "            output['length_used'].append(scaler_name)\n",
    "            output['text_extraction'].append('Hashing')\n",
    "            output['ngrams'].append(n)\n",
    "            output['model'].append(model_name)\n",
    "            for method, score in model_scores.items():\n",
    "                output[method].append(score)\n",
    "            durations = time.time() - start\n",
    "            print(f'length: {scaler_name} - skipgram - ngram: {n} - model: {model_name}: Done in {int(durations//60)}m{int(durations%60)}s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_df = pd.DataFrame(output)\n",
    "# output_df[\"text_extraction\"] = [\"tfidf\"]*6\n",
    "# output_df.to_csv(\"../../output/csv/tfidf.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
